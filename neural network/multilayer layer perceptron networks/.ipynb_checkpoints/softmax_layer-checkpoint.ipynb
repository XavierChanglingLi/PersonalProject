{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChanglingLi**\n",
    "\n",
    "Fall 2020\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement single layer network to test softmax activation and cross-entropy loss\n",
    "\n",
    "You will first implement and test out the softmax activation and cross-entropy loss in a single layer net before embedding it in a more complex multi-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Load in preprocessed STL-10 data\n",
    "\n",
    "Use your automated preprocessing function to load in the STL-10 data in the following split:\n",
    "- 3500 training samples\n",
    "- 500 test samples\n",
    "- 500 validation samples\n",
    "- 500 samples for development\n",
    "\n",
    "Assign the LAST 15 images in the dev set to a variable called `test_imgs` and the LAST 15 classes in the dev set to `test_labels`. These are the variable names assumed for the test code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached numpy arrays the images and labels. Loading them...\n",
      "Images are: (5000, 32, 32, 3)\n",
      "Labels are: (5000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.load_stl10(3500,500,500,500)\n",
    "test_imgs = x_dev[-15:,:]\n",
    "test_labels = y_dev[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement the following functions\n",
    "\n",
    "In `softmax_layer.py`, implement the following methods in the base class `SoftmaxLayer`:\n",
    "\n",
    "- `fit`\n",
    "- `net_in`\n",
    "- `predict`\n",
    "- `one_hot`\n",
    "- `accuracy`\n",
    "- `activation` (softmax) $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}$ where $x_i$ are the \"net in\" values and there are $C$ output neurons (one per input class). $f(x_i)$ is the activation values of each output neuron $i$. Since this is softmax, it is the probability that a given input belongs to the class $i$ coded by the output neuron.\n",
    "- `loss` (cross-entropy) $L(x_m) = -\\frac{1}{B}\\sum_{b=1}^B{Log \\left (\\frac{e^{x_m}}{\\sum_{n=1}^C e^{x_n}}\\right )}$. $m$ is the correct class for the $b^{th}$ input. $x_m$ is the output neuron activation for the correct class, $x_n$ is the output neuron activation for all of the classes (in the sum). The batch size is $B$, so the loss is averaged over each mini-batch of inputs. The expression in the $Log$ is just the softmax.\n",
    "- `gradient` (for softmax/cross-entropy)\n",
    "\n",
    "You're welcome to work in any order, but I recommend starting with `fit` because as you work though it, you should recognize why we need most of the other methods. You can finish `fit` or branch off as you need the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Test key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax_layer import SoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some small Gaussian weights equal to the length of an image feature vector\n",
    "np.random.seed(0)\n",
    "randWts = np.random.normal(loc=0, scale=0.01, size=(x_dev.shape[1], 10))\n",
    "b = 1\n",
    "softmaxNet = SoftmaxLayer(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `onehot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your one hot vectors:\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_test1 = np.array([2, 2, 0, 1])\n",
    "c_test = 4\n",
    "y_one_hot = softmaxNet.one_hot(y_test1, c_test)\n",
    "print(f'Your one hot vectors:\\n{y_one_hot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your one hot vectors should look like:\n",
    "\n",
    "    [[0. 0. 1. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `loss`,  `net_in`, softmax `activation` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net in shape=(15, 10), min=0.7160773059462715, max=1.4072103751494882\n",
      "Should be\n",
      "net in shape=(15, 10), min=0.7160773059462714, max=1.4072103751494884\n",
      "\n",
      "net act shape=(15, 10), min=0.07322406412627332, max=0.1433135816597887\n",
      "Should be\n",
      "net act shape=(15, 10), min=0.0732240641262733, max=0.1433135816597887\n",
      "\n",
      "The loss (without regularization) is 2.28 and it should approx be 2.28\n",
      "The loss (with 0.5 regularization) is 3.03 and it should approx be 3.03\n"
     ]
    }
   ],
   "source": [
    "lossNoReg, lossReg = softmaxNet.test_loss(randWts, b, test_imgs, test_labels)\n",
    "print(f'The loss (without regularization) is {lossNoReg:.2f} and it should approx be 2.28')\n",
    "print(f'The loss (with 0.5 regularization) is {lossReg:.2f} and it should approx be 3.03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net in: (15, 10), 0.7160773059462715, 1.4072103751494882\n",
      "net in 1st few values of 1st input are: [1.05  0.845 1.13  1.228 0.969]\n",
      "net act 1st few values of 1st input are: [0.105 0.085 0.114 0.125 0.097]\n",
      "y one hot: (15, 10), sum is 15.0\n",
      "\n",
      "1st few Wt gradient values are [0.01  0.015 0.014 0.009]\n",
      "and should be                  [0.01  0.015 0.014 0.009] \n",
      "1st few Wt bias values are [ 0.101 -0.098  0.102 -0.026]\n",
      "and should be              [ 0.101 -0.098  0.102 -0.026]\n"
     ]
    }
   ],
   "source": [
    "grad_wts, grad_b = softmaxNet.test_gradient(randWts, b, test_imgs, test_labels, 10)\n",
    "print()\n",
    "print(f'1st few Wt gradient values are {grad_wts[:4,0]}\\nand should be                  [0.01  0.015 0.014 0.009] ')\n",
    "print(f'1st few Wt bias values are {grad_b[:4]}\\nand should be              [ 0.101 -0.098  0.102 -0.026]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 0 / 1200 . Training loss:  2.31972502977644 .\n",
      "Completed iter 100 / 1200 . Training loss:  2.2756005903398893 .\n",
      "Completed iter 200 / 1200 . Training loss:  2.2883716000863896 .\n",
      "Completed iter 300 / 1200 . Training loss:  2.2692842923790515 .\n",
      "Completed iter 400 / 1200 . Training loss:  2.258616928580862 .\n",
      "Completed iter 500 / 1200 . Training loss:  2.252728874746743 .\n",
      "Completed iter 600 / 1200 . Training loss:  2.2348873010886208 .\n",
      "Completed iter 700 / 1200 . Training loss:  2.212513783144309 .\n",
      "Completed iter 800 / 1200 . Training loss:  2.1929147772322395 .\n",
      "Completed iter 900 / 1200 . Training loss:  2.1813125576006884 .\n",
      "Completed iter 1000 / 1200 . Training loss:  2.187388817159243 .\n",
      "Completed iter 1100 / 1200 . Training loss:  2.195435956585718 .\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, n_epochs=600, mini_batch_sz=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the random mini-batch sampling process, you may get different specific numbers, but the loss should generally decrease over iterations. You should get something like this:\n",
    "\n",
    "    Starting to train network...There will be 600 epochs and 1200 iterations total, 2 iter/epoch.\n",
    "      Completed iter 0/1200. Training loss: 2.32.\n",
    "      Completed iter 100/1200. Training loss: 2.28.\n",
    "      Completed iter 200/1200. Training loss: 2.29.\n",
    "      Completed iter 300/1200. Training loss: 2.27.\n",
    "      Completed iter 400/1200. Training loss: 2.26.\n",
    "      Completed iter 500/1200. Training loss: 2.25.\n",
    "      Completed iter 600/1200. Training loss: 2.23.\n",
    "      Completed iter 700/1200. Training loss: 2.21.\n",
    "      Completed iter 800/1200. Training loss: 2.19.\n",
    "      Completed iter 900/1200. Training loss: 2.18.\n",
    "      Completed iter 1000/1200. Training loss: 2.19.\n",
    "      Completed iter 1100/1200. Training loss: 2.20.\n",
    "    Finished training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the loss\n",
    "\n",
    "It should look noisy, but decrease and look linear on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()\n",
    "    \n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What do you think the linear drop in loss over the epochs tells us about the state of the training process? How is current training going? What's the future potential like?\n",
    "\n",
    "**Question 2.** Below, write code to test regularization with training: check to make sure that the loss starts at around 154-158 for `reg=100`. Once you get this working, play around with the regularization parameter. You can drastically change the magnitude, but it should always remain nonnegative. How does regularization affect the training loss and **why**?\n",
    "\n",
    "**Question 3.** Play around with the batch size parameter. How does this affect the training loss and **why**? (*Think about the error gradient and how the weights change*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 0 / 1200 . Training loss:  154.06747080248388 .\n",
      "Completed iter 100 / 1200 . Training loss:  22.619434435188545 .\n",
      "Completed iter 200 / 1200 . Training loss:  5.019919731426475 .\n",
      "Completed iter 300 / 1200 . Training loss:  2.659490952517295 .\n",
      "Completed iter 400 / 1200 . Training loss:  2.343909123257356 .\n",
      "Completed iter 500 / 1200 . Training loss:  2.3004464949125762 .\n",
      "Completed iter 600 / 1200 . Training loss:  2.295423352878536 .\n",
      "Completed iter 700 / 1200 . Training loss:  2.2951983609549256 .\n",
      "Completed iter 800 / 1200 . Training loss:  2.2933358979671534 .\n",
      "Completed iter 900 / 1200 . Training loss:  2.294594581512908 .\n",
      "Completed iter 1000 / 1200 . Training loss:  2.295542508185893 .\n",
      "Completed iter 1100 / 1200 . Training loss:  2.2964494796487993 .\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, n_epochs=600, mini_batch_sz=250, reg=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** The linear drop in loss over epochs tells us that the training is on the track but might need to continue training as the loss just starts to flatten. It may also means that the training is finished as according to the loss printed out, the change is really small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** It will increases the training loss at the beignning if the regularization factor is big. This is because the regularization will affect how much a weight affect the training process and when the regularization is big, it will also increase the effect of some large weights values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** When the batch size is small, the training loss is unstable as changes in a wider range. This is because the update on weights are on a smaller part of the data and thus is unstable comparing to the weights updated on a larger size which averages values out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Train and optimize STL-10 dataset performance\n",
    "\n",
    "As you've surely noticed, hyperparameters can drastically affect learning! \n",
    "\n",
    "\n",
    "Implement a grid search for the best hyperparameters\n",
    "\n",
    "- learning rate,\n",
    "- regularization\n",
    "- batch size \n",
    "\n",
    "The grid search process should:\n",
    "\n",
    "1. Fit the model with specific values of hyperparameters that we're testing (using the training set).\n",
    "2. Compute the accuracy on the training set. \n",
    "3. Compute the accuracy on the validation set. \n",
    "4. Print out and record the best parameter combination as you go (that improves the **validation set accuracy**).\n",
    "5. Wipe the weights clean (reinitialize them) every time you try new parameters. It's easiest just to create a new net object on each run.\n",
    "\n",
    "#### Suggestions\n",
    "\n",
    "This can take quite a bit of simulation time! Here are some tips:\n",
    "- I suggest using a coarse-to-fine search strategy. First try varying parameters over many orders of magnitude. Use the \"new best\" print outs to refine the ranges that you test. Abort simulations prematurely if you feel there aren't productive (no reason to wait!). This can take however long or short that you want to dedicate. Remember, you are printing out the best parameter values on each run, so you can just proceed with those.\n",
    "- You should be able to achieve ~30% accuracy without too much effort (10% is chance performance).\n",
    "- High learning rates don't really make sense. You'll know if your value is \"high\" if numpy complains about numerical issues.\n",
    "- Your mini-batch sizes should be <= N and >= 1.\n",
    "- Time single network runs with a few different batch sizes you plan on trying in your big search. This will help you figure out a ballpark estimate how long grid search will take (*you can decide whether to go eat dinner, run it overnight, etc.*). If it will take an unreasonable amount of time, reduce the number of parameters you try in one search.\n",
    "\n",
    "**Important note:** Like usual, I am not grading based on your performance numbers or the number of hours your computer spends searching. I want to see that you successfully implemented the grid search to find progressively better hyperparameters on STL-10 and use the outcome to inform your ultimate training session that you use to evaluate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(lr_bound, batch_size_bound, regularization_bound):\n",
    "#the first number is the lower bound and the second number is the upper bound\n",
    "    lr_list = np.arange(lr_bound[0], lr_bound[1], (lr_bound[1]-lr_bound[0])/10)\n",
    "    batch_list = np.arange(batch_size_bound[0],batch_size_bound[1],int((batch_size_bound[1]-batch_size_bound[0])/10))\n",
    "    regularization_list = np.arange(regularization_bound[0],regularization_bound[1],(regularization_bound[1]-regularization_bound[0])/10)\n",
    "    his_accuracy_test = []\n",
    "    his_accuracy_valid = []\n",
    "    combination = dict()\n",
    "    for lr in lr_list:\n",
    "        for batch in batch_list:\n",
    "            for reg in regularization_list:\n",
    "                softmaxNet = SoftmaxLayer(10)\n",
    "                loss_history = softmaxNet.fit(x_test, y_test, n_epochs=200, lr=lr, mini_batch_sz=batch, reg=reg)\n",
    "                y_predict_test = softmaxNet.predict(x_test)\n",
    "                his_accuracy_test.append(softmaxNet.accuracy(y_test,y_predict_test))\n",
    "                y_predict_valid = softmaxNet.predict(x_val)\n",
    "                his_accuracy_valid.append(softmaxNet.accuracy(y_val,y_predict_valid))\n",
    "                combination[softmaxNet.accuracy(y_val,y_predict_valid)] = [lr, batch, reg]\n",
    "    print(his_accuracy_test)\n",
    "    print(his_accuracy_valid)\n",
    "    return combination.get(max(his_accuracy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 0 / 2000 . Training loss:  2.3147417965615777 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.310506811685994 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.3063893491585032 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.2731198894530418 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.3094581965474474 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.298545769587159 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2604187357083037 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.261160774703256 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.2666735335627832 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3185767516798377 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.2565784618074165 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.274276546328925 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.245544853710157 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.218861348548264 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.2090957601445886 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.2365754579908086 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.1747830368192185 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.2515385077539847 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.2168168903947056 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.2028680057150654 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.385855806637675 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.429599129492251 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4428841455769024 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.437900961793765 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4662299782095345 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.406928404791177 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.3853753415494836 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.4389459246182277 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.4716694577592544 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3948337959853983 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3398586434847224 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4006804628380203 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.406814641436503 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.37284825085 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.3794401096958206 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.3201549765357683 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3188858654441438 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3338412903880634 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.3884620574281956 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3481616722978007 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.621270913134708 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6156480888820672 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.6034129512563764 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.5933064966565045 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5575479479400096 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.5508008841471437 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.5610011245410984 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.5737997102301295 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.5138880585308927 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.54938170220085 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.5292264502468846 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.5296083534752842 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.5059731482310243 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.544906319109338 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5279191225021678 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.522634532023382 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.5121390579106535 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.499754976233474 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.460550601634636 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.4859522375846224 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7773414902204987 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7688706448087244 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.7750871557351378 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.7674748954437476 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.773154896387587 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7358301671736407 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.777159069638305 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.740719364740707 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.7363131137979257 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.692202307493175 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.670882667427311 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.6740962259836514 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.692340011120538 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.695247813060236 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.6500916677613873 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.687672302288115 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.661557019557986 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.669551260387704 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.6199751235745268 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.670871766585952 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.927413642802264 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.9071861428561907 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.921432220331555 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.9029696487173204 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.899527979468978 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8963316757836495 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.8619442845287812 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.821405036733825 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.8549393903446028 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.837188498313995 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.832028467836007 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.836205085063488 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.773741284365403 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.807568030916065 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.804559960305239 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.800515568802062 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.7898483106126273 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.7737620866953656 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.7549120124311264 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.75469106869069 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.073129862616364 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.0486028789387127 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0751136737448976 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0948917248936056 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0170276846386495 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9864785736315484 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0124867250667835 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9970685983212277 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.990946489483419 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.965760569107122 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9907903725263045 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9423799225137435 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9235000825837334 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9719592180616137 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.964039976107728 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.9133811880162233 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.9242967718523145 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8633435969484218 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9337881933976266 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8856101045962133 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.1902657979929816 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.198911527385057 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.1983622915907666 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.1827796135181403 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.2005055126414383 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1072791635696837 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 600 / 2000 . Training loss:  3.152306679127962 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1478253241022243 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1340509534612897 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1587603910325455 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1300805024405136 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0964344474963665 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.061337498976945 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0568383453815087 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0681188686864744 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.0306239487449242 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.0249492090273873 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.01657606146882 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0367673501953396 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9725103781148676 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.34007427172014 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.406645529950922 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.366562784491054 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.311560260151888 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.317402000641093 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.3460372797132822 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.2858164139064736 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3108961867047597 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.306885236775498 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.3097090937099005 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.2420133621666802 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.2575278212319208 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.2042875932557005 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.201320334694323 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.211212287752362 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.199606860056261 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.2289809490135384 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.1411548342166546 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.1699137195446014 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.128696866845387 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.532085032191245 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.495635998012191 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.5059320926353754 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.467859382071175 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4745331789892258 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.456332401411534 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.4557856480911533 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4585721393676505 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3946673480671627 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.3805609173852096 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.379808619556466 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.3689904564152773 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.377772317943456 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.3286377858853284 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.330033316507055 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.2895491706391597 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.306801714399421 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.2874161330145304 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.292676206481562 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.274391877199782 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.6670732049654755 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6934042330218633 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.661174753117267 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.635724059437199 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.642865001536864 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.598420322569896 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.5916090601381465 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.5844936065509723 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.551718362802805 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.507004598141585 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.498582179546677 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.49510939899098 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.4842760325266022 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.459060944074924 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.4295287287451544 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.4065981470936677 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.3751492502495424 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.378102626587391 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.3427823390821425 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.3713854923729185 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.307303292037686 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.298506224487832 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.286028840904537 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.3011326888805286 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.3101887412775994 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2798295695142765 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.2996350201272904 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.2541651342802473 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.2778628398861076 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.2538373364008377 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.4600577662417566 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4483726578537715 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4799474002398076 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.4337684225987393 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.417527124862282 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.4367411025409975 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.4210147898710748 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.409970312403327 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.3929794225079135 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.403142217344727 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.622791940694779 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.6308487567814254 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.601263689159536 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.6182816277953607 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.614730829350414 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.585677583223952 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.575084334495473 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.560345389781167 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.5783772150146005 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.566707039866933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.757822752178953 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7445008725781372 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.727712198744355 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.728385021354355 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.726610941573823 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.6924578164663067 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.7092701546848534 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.709476133799339 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.67467633886832 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.692209299175141 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9265413014923256 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.9317850709471998 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8988862494399776 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.9278211340039912 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.892197418719404 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8950501913346924 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.8965960801145716 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.8585619139022844 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.8439171914053416 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.8468375249014386 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.110049571379719 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 1000 . Training loss:  3.093067506626558 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.068726509132827 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0581430045665927 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.0549067953627738 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.030549106490346 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.036292707919163 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.0255470999599003 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.0129566025531256 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9863799252743357 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.216236856827984 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.2250628309629596 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1843068090265643 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1971109046472286 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.181553345030202 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.178990827463121 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.160094607410935 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.1424684416199278 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.149855064604243 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.106063912565468 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.385224555659188 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.3460991555182416 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.3265818341608946 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.3325134825954494 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.315072996495316 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.3043977588700404 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.269507267946625 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.26637923979707 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2799923089425302 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.2052197755300647 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.5429628279121688 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.551907485743971 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.533951501742867 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.523366538281019 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.5033924135065204 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.4689699659587254 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.4560832002264115 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4447760651525465 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.415766189869476 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.4347752717734474 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.69017988124609 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.671686541818888 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.6272428818986846 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.6175140202998843 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.597374710360514 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.5725133702038923 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.5695071497848163 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.527035022217327 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.5137494973549854 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.4976489672295212 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.29993591108145 .\n",
      "Completed iter 100 / 600 . Training loss:  2.2985079484821145 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2919791758267594 .\n",
      "Completed iter 300 / 600 . Training loss:  2.31333883464592 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2729770917356227 .\n",
      "Completed iter 500 / 600 . Training loss:  2.267671354387903 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.4520339126378436 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4554468747585183 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4327984371049625 .\n",
      "Completed iter 300 / 600 . Training loss:  2.4349591637587937 .\n",
      "Completed iter 400 / 600 . Training loss:  2.4274602970449397 .\n",
      "Completed iter 500 / 600 . Training loss:  2.436532608133289 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.611487947538703 .\n",
      "Completed iter 100 / 600 . Training loss:  2.6065847387005894 .\n",
      "Completed iter 200 / 600 . Training loss:  2.591374441574387 .\n",
      "Completed iter 300 / 600 . Training loss:  2.6019238357922942 .\n",
      "Completed iter 400 / 600 . Training loss:  2.610795076932681 .\n",
      "Completed iter 500 / 600 . Training loss:  2.564372699865825 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.761542056575022 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7524120242535974 .\n",
      "Completed iter 200 / 600 . Training loss:  2.744571817288678 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7247363607278983 .\n",
      "Completed iter 400 / 600 . Training loss:  2.7407290524457606 .\n",
      "Completed iter 500 / 600 . Training loss:  2.7267704855979775 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.93239175259302 .\n",
      "Completed iter 100 / 600 . Training loss:  2.8868150340174874 .\n",
      "Completed iter 200 / 600 . Training loss:  2.898918992737543 .\n",
      "Completed iter 300 / 600 . Training loss:  2.890231891028593 .\n",
      "Completed iter 400 / 600 . Training loss:  2.8811213301395426 .\n",
      "Completed iter 500 / 600 . Training loss:  2.8592699945338405 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.0781084171874635 .\n",
      "Completed iter 100 / 600 . Training loss:  3.0525942351173754 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0513910995732294 .\n",
      "Completed iter 300 / 600 . Training loss:  3.043749680504521 .\n",
      "Completed iter 400 / 600 . Training loss:  3.0266528950420053 .\n",
      "Completed iter 500 / 600 . Training loss:  3.017930366831979 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.237739105833857 .\n",
      "Completed iter 100 / 600 . Training loss:  3.2315141999317785 .\n",
      "Completed iter 200 / 600 . Training loss:  3.2093754909613024 .\n",
      "Completed iter 300 / 600 . Training loss:  3.207099229677688 .\n",
      "Completed iter 400 / 600 . Training loss:  3.181996893897453 .\n",
      "Completed iter 500 / 600 . Training loss:  3.176225200308056 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.437544785395101 .\n",
      "Completed iter 100 / 600 . Training loss:  3.413315760264516 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3953678667297913 .\n",
      "Completed iter 300 / 600 . Training loss:  3.399877181967553 .\n",
      "Completed iter 400 / 600 . Training loss:  3.3774983894965813 .\n",
      "Completed iter 500 / 600 . Training loss:  3.345558734295377 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.5556451320813993 .\n",
      "Completed iter 100 / 600 . Training loss:  3.5308402991155567 .\n",
      "Completed iter 200 / 600 . Training loss:  3.522166696887207 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5187359129686535 .\n",
      "Completed iter 400 / 600 . Training loss:  3.494086784688194 .\n",
      "Completed iter 500 / 600 . Training loss:  3.469153142996408 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.692701894346749 .\n",
      "Completed iter 100 / 600 . Training loss:  3.673572231932533 .\n",
      "Completed iter 200 / 600 . Training loss:  3.655036244648721 .\n",
      "Completed iter 300 / 600 . Training loss:  3.6231351072537006 .\n",
      "Completed iter 400 / 600 . Training loss:  3.6118911353976273 .\n",
      "Completed iter 500 / 600 . Training loss:  3.5866332324576033 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3203018036526526 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2943141080572436 .\n",
      "Completed iter 200 / 400 . Training loss:  2.286400096246141 .\n",
      "Completed iter 300 / 400 . Training loss:  2.28351291064287 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4819018383093563 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4719958528956347 .\n",
      "Completed iter 200 / 400 . Training loss:  2.462180589479067 .\n",
      "Completed iter 300 / 400 . Training loss:  2.457879982656359 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6106402636364607 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5940161043017236 .\n",
      "Completed iter 200 / 400 . Training loss:  2.597021513700342 .\n",
      "Completed iter 300 / 400 . Training loss:  2.583934720078982 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7903612545345275 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7918219102308943 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7829402937320644 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 300 / 400 . Training loss:  2.7627884872506394 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.91479013831047 .\n",
      "Completed iter 100 / 400 . Training loss:  2.90217733124552 .\n",
      "Completed iter 200 / 400 . Training loss:  2.915099968278015 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8828808829731773 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.1028947272719294 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0694857230971055 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0879986020421866 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0570234611340332 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2309798884553924 .\n",
      "Completed iter 100 / 400 . Training loss:  3.207511106736007 .\n",
      "Completed iter 200 / 400 . Training loss:  3.20006920776475 .\n",
      "Completed iter 300 / 400 . Training loss:  3.200579164273779 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3902915975274714 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3911573318760766 .\n",
      "Completed iter 200 / 400 . Training loss:  3.364254176947835 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3560069741406737 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5279025261175443 .\n",
      "Completed iter 100 / 400 . Training loss:  3.505169987942393 .\n",
      "Completed iter 200 / 400 . Training loss:  3.48074921404439 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4843157870593635 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6843883116615572 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6644854546854018 .\n",
      "Completed iter 200 / 400 . Training loss:  3.657345531585174 .\n",
      "Completed iter 300 / 400 . Training loss:  3.6335248745400306 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.286136610888324 .\n",
      "Completed iter 100 / 400 . Training loss:  2.278664273660419 .\n",
      "Completed iter 200 / 400 . Training loss:  2.276900119086686 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2852711728419255 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.460071292152603 .\n",
      "Completed iter 100 / 400 . Training loss:  2.46661960640099 .\n",
      "Completed iter 200 / 400 . Training loss:  2.45131266310811 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4440535537867913 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.5974713622818673 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5857002837361343 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5768995336469565 .\n",
      "Completed iter 300 / 400 . Training loss:  2.569365884751346 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.784078108343097 .\n",
      "Completed iter 100 / 400 . Training loss:  2.766842010355644 .\n",
      "Completed iter 200 / 400 . Training loss:  2.77135120930991 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7610616019254093 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.911875598786489 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9070059170025653 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9064942403365337 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8929605580550315 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0747666260205424 .\n",
      "Completed iter 100 / 400 . Training loss:  3.071796973078947 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0721260831747585 .\n",
      "Completed iter 300 / 400 . Training loss:  3.04027442172408 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2086764423829637 .\n",
      "Completed iter 100 / 400 . Training loss:  3.1982728388570423 .\n",
      "Completed iter 200 / 400 . Training loss:  3.197776551107464 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1839005481126628 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.370273414259124 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3529298625794404 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3304713212306067 .\n",
      "Completed iter 300 / 400 . Training loss:  3.319398197304485 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.526215209493065 .\n",
      "Completed iter 100 / 400 . Training loss:  3.498192484696217 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4973937502994135 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4668657378703567 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6954278432088103 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6881999469175133 .\n",
      "Completed iter 200 / 400 . Training loss:  3.655382835718175 .\n",
      "Completed iter 300 / 400 . Training loss:  3.6243311839580388 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3025141114835335 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3030191557536175 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4970281415136277 .\n",
      "Completed iter 100 / 200 . Training loss:  2.463297746191388 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6168342129458217 .\n",
      "Completed iter 100 / 200 . Training loss:  2.603621490944971 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7544647913633797 .\n",
      "Completed iter 100 / 200 . Training loss:  2.753443765615579 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9173679378922883 .\n",
      "Completed iter 100 / 200 . Training loss:  2.894767185827236 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0599510720552647 .\n",
      "Completed iter 100 / 200 . Training loss:  3.035777166322566 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2543289330413945 .\n",
      "Completed iter 100 / 200 . Training loss:  3.237644288684992 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3957461181196766 .\n",
      "Completed iter 100 / 200 . Training loss:  3.373739340401836 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5298694907090233 .\n",
      "Completed iter 100 / 200 . Training loss:  3.534836473750935 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.695701639433067 .\n",
      "Completed iter 100 / 200 . Training loss:  3.690563650082645 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.297450526899101 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2885777356941235 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.479912626734536 .\n",
      "Completed iter 100 / 200 . Training loss:  2.469250333773719 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6108320907739313 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6108176307032194 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.765219004602124 .\n",
      "Completed iter 100 / 200 . Training loss:  2.762327787030363 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9300381992232083 .\n",
      "Completed iter 100 / 200 . Training loss:  2.917002919993695 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0840766613671784 .\n",
      "Completed iter 100 / 200 . Training loss:  3.068058960523137 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.238583663633624 .\n",
      "Completed iter 100 / 200 . Training loss:  3.227470400182635 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4092229483629692 .\n",
      "Completed iter 100 / 200 . Training loss:  3.391082349329169 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5498731185107015 .\n",
      "Completed iter 100 / 200 . Training loss:  3.544920119762613 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.671546091846083 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6629883888572805 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.316839284337661 .\n",
      "Completed iter 100 / 200 . Training loss:  2.310691576362608 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4689845840636635 .\n",
      "Completed iter 100 / 200 . Training loss:  2.460309072428872 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6199361887259505 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6036296870766162 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.766771153553057 .\n",
      "Completed iter 100 / 200 . Training loss:  2.755802698909747 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9018820632734386 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9080904626008075 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.102100887908833 .\n",
      "Completed iter 100 / 200 . Training loss:  3.093388970688911 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.235941506783993 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 200 . Training loss:  3.2251477404234254 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3972616111079055 .\n",
      "Completed iter 100 / 200 . Training loss:  3.381537681651247 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.555651930462879 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5357135124917383 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.684018469766656 .\n",
      "Completed iter 100 / 200 . Training loss:  3.665416331865815 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3284044950169216 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3120951084187014 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.471686280575891 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4594711623343097 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6320116469321153 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6194813627718974 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.756798474421957 .\n",
      "Completed iter 100 / 200 . Training loss:  2.746174516983528 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9127076815601187 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9021181718597404 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0839973223527593 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0804877233999335 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.222965294707316 .\n",
      "Completed iter 100 / 200 . Training loss:  3.199566572019373 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.387850715942466 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3636298622598835 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.552317813852784 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5350608542393553 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.678848778920254 .\n",
      "Completed iter 100 / 200 . Training loss:  3.652080272307807 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.319040092135766 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3155589109527197 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4663995942310835 .\n",
      "Completed iter 100 / 200 . Training loss:  2.458047794237668 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6038405756267635 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6001883695372907 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.773055992986363 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7632069417761125 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9193055916764568 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9077189661943637 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.1140048609007547 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0906968933266663 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2380103110641536 .\n",
      "Completed iter 100 / 200 . Training loss:  3.22706764742525 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.383343727143078 .\n",
      "Completed iter 100 / 200 . Training loss:  3.363162896504681 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5207925335493346 .\n",
      "Completed iter 100 / 200 . Training loss:  3.504321445547269 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.687404692676223 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6566251782055534 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.309022137064813 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.282445054997245 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.3122379879291195 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.2675774620215545 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.2656247466315564 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.2738247233190667 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2621874463738236 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.2577771478097968 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.228073681762783 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.211347490493619 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.258920342697796 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.2178379338534135 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.2108526064151452 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2258346519458603 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.2095137806557186 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.160926400065487 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.2368906707494087 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.1968240998829747 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.2132293741753184 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.158119195682978 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4779453662764768 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4710068159738676 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4675530896993054 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.4485157679418013 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4665623350423926 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.457189826995053 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.4724507840406127 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.4533733423942112 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.391780733143234 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3705878237080573 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4188150891807614 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.3617536558888745 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.365110236421625 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.4097093375018463 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.395717521051313 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.372490163357251 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3540167497033377 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3980368891009602 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.3023879108034073 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.356434542041819 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.6257350499238736 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6285393085242887 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.6079193414972313 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.595189349249199 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.572238468711308 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.5495363322826505 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.585459535678724 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.5782050165285373 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.561185321238183 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.558538191094068 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.5282372339310117 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.512486945897365 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.523850326675611 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.490572714633089 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.538982966573111 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.5589543965702566 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.4966129213458035 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.4813301844358855 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.5163885318839294 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.43079703523649 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7325184110878977 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7131850034241722 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.757369626209302 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.7267035194458167 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.754326595465825 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7137255575431576 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.695235224624228 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.698303839648032 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6734673088555345 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.6904564207220645 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.6457246294385706 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.689772233356553 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.600162079645486 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1300 / 2000 . Training loss:  2.59546981959192 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.624122021215209 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6935836455013886 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.5945146144456612 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.5986517625034415 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.6148257501418573 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6355323458141187 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.9460707815261875 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.9581395266815633 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.9242928769141914 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.9164591794626658 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.916453770643968 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8998148520255427 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.868239568401669 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.8760162959915934 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.8553903883762595 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.86631375615307 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.8622981204506726 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8295644970601352 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.7884095451565596 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8260671815449996 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.798041080700623 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.7493310699339464 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.776729707093166 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.7586005640424265 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.717909096550689 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.760680441917777 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.060076348745895 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.047119170675754 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.055024127218306 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0261423993239296 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.991952766835591 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9859879613154003 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.9610833691055687 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9769147023588127 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.965173219104366 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.978278947699982 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9342623600735998 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9367843200627624 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9082187728615105 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8738968111958316 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.8779053965496635 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.854258609859095 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.9088837632393036 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.916047035426034 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.858658886182866 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.855910944314145 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.2445467728283406 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.19677210524683 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.2216034315431212 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.16431711288818 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.1800889556665357 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1558578089497464 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.1251197411351352 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1714653202770897 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1101171227877913 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.087169793290294 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1032827047151232 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0576749626508404 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.0635428593107017 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.096515326731148 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.022344568229042 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.0487109435608604 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.026610382373092 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.998418281643235 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9887613148076593 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.984818127172029 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.360831604242696 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.392755605759233 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3337111777907156 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.295763913662619 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.346292233217296 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.2885171521742156 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.2756147392952912 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.2607974005058824 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.2210794947534698 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.2421169220347674 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.2430297493913565 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.141551901626012 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.168927229924545 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.169240585364902 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.165653951031322 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.1779724940094587 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1321362148599015 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.065342995618301 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0824396348307213 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0575886800991094 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.530421495538377 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.5340780326847074 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.4983684632915044 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.5016122077963354 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4498038893201315 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4478627659612044 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.437132625488986 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4040596453518885 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.4190609268917376 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.3542780010913678 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.35512459012515 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.316446503939553 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.3114205007300566 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.3493506089657274 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.3014237448314807 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.2567720159477487 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.226444373340933 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.205775566282094 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.1692113293311373 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.198585299846696 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.668190909181722 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6603357544721025 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.6414419743634325 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.605828730597981 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.5694341386846955 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.521569707842993 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.554342445029947 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.502459255164679 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.444265983686483 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.4778500687496026 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.4440687223461417 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.4144694956297963 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.448716904962644 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.3897374336538095 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.367645033207097 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.4081005580914567 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.356589602861554 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.3125961877002177 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1800 / 2000 . Training loss:  3.302558887188447 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.275277391364649 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.3040787848211295 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.3012150003963905 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.315703551465216 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.277374244385086 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2751810700253 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2700360524983667 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.239704936609792 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.2757536299833308 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.2432582015555944 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.222760202268267 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.47754358001073 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4387062295349824 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4451288671228237 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.3863922557852613 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.4112390519165596 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.4234867629347057 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.4164753693046785 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.40505766594033 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.3903288917340566 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.366765516078291 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.6112573433313666 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.6169585916909304 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.5944068585360163 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.6013142432534084 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.591216155282831 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.583297152666802 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.5803662857576923 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.5652387606114755 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.5499551405236383 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.548941154035797 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7613191359138978 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7427262039179494 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7397660007190003 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7436914459849224 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.7210623092190467 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.720148380355968 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.7004375015201965 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.7160416082852143 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6380243378076664 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6756197652403237 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9053451650657505 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.884720952663324 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8930165484885833 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.8874620262726034 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.8569807667033014 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8519183910171386 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.8377766938939297 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.835463990862724 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.825689853289969 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.817228623845369 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.050513747517935 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0375257658561856 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.042088320104212 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0460321730214144 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.019242903407119 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9943800663021487 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.973836433262 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.983283185240823 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.988830066057543 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9371161500335456 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.24462706022084 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.238704129212622 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1949383228377375 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1911358895335784 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1795462964303036 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.18911194580446 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.1420528637684813 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.142478631511873 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.121925541444418 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.1498841000620943 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.3817852223006524 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.383705069919868 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.3355338654439546 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.327163220906322 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.3106073969220677 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.296981857627808 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.2753181770945927 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.26592579615337 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.234486115784032 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.231757243078796 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.5189339616877215 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.536276539113197 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.5025718165994073 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.468157669942681 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.4729134522381515 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.43424920265603 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.437552384314679 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3899875384439517 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.392639561208371 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.3633471756384434 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6807399861388457 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6947230762677266 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.6706813317562883 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.6366571145193247 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.60174062478659 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.5831782897716313 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.5568199381246366 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.5556234878584227 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.5033198224709805 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.5090520123856717 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3203313902910088 .\n",
      "Completed iter 100 / 600 . Training loss:  2.3122742324262755 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2955612619399814 .\n",
      "Completed iter 300 / 600 . Training loss:  2.2972374050238216 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2996733199187465 .\n",
      "Completed iter 500 / 600 . Training loss:  2.278330131865499 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.4427770970695653 .\n",
      "Completed iter 100 / 600 . Training loss:  2.417135294126472 .\n",
      "Completed iter 200 / 600 . Training loss:  2.407250931538148 .\n",
      "Completed iter 300 / 600 . Training loss:  2.4195154842856015 .\n",
      "Completed iter 400 / 600 . Training loss:  2.3907673649147205 .\n",
      "Completed iter 500 / 600 . Training loss:  2.4101045501384344 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.6312705968402383 .\n",
      "Completed iter 100 / 600 . Training loss:  2.5850785478609972 .\n",
      "Completed iter 200 / 600 . Training loss:  2.566607259523532 .\n",
      "Completed iter 300 / 600 . Training loss:  2.565403863608295 .\n",
      "Completed iter 400 / 600 . Training loss:  2.580262012586943 .\n",
      "Completed iter 500 / 600 . Training loss:  2.565185167562668 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.7725462466212534 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7648766753106293 .\n",
      "Completed iter 200 / 600 . Training loss:  2.7552258709444 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7611051876593415 .\n",
      "Completed iter 400 / 600 . Training loss:  2.7644308815893623 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 500 / 600 . Training loss:  2.7319876345156313 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.929144037120797 .\n",
      "Completed iter 100 / 600 . Training loss:  2.9119711494721985 .\n",
      "Completed iter 200 / 600 . Training loss:  2.9195791322011084 .\n",
      "Completed iter 300 / 600 . Training loss:  2.9013252707890613 .\n",
      "Completed iter 400 / 600 . Training loss:  2.893273448157642 .\n",
      "Completed iter 500 / 600 . Training loss:  2.90200538992286 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.0981113602328314 .\n",
      "Completed iter 100 / 600 . Training loss:  3.055758281875033 .\n",
      "Completed iter 200 / 600 . Training loss:  3.046018547291312 .\n",
      "Completed iter 300 / 600 . Training loss:  3.0568610938159066 .\n",
      "Completed iter 400 / 600 . Training loss:  3.0175230181291575 .\n",
      "Completed iter 500 / 600 . Training loss:  3.0138503224491204 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2565771234673373 .\n",
      "Completed iter 100 / 600 . Training loss:  3.229603728692286 .\n",
      "Completed iter 200 / 600 . Training loss:  3.2158336385134665 .\n",
      "Completed iter 300 / 600 . Training loss:  3.213644553536076 .\n",
      "Completed iter 400 / 600 . Training loss:  3.1982215187571232 .\n",
      "Completed iter 500 / 600 . Training loss:  3.170836092900909 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3875624938686455 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3661528434240684 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3487709662725385 .\n",
      "Completed iter 300 / 600 . Training loss:  3.3222398348612647 .\n",
      "Completed iter 400 / 600 . Training loss:  3.294112355467093 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2762631530566857 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.551180088722257 .\n",
      "Completed iter 100 / 600 . Training loss:  3.5265758853591826 .\n",
      "Completed iter 200 / 600 . Training loss:  3.5336213359687685 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5266517399015207 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4884857663840436 .\n",
      "Completed iter 500 / 600 . Training loss:  3.4533560426346024 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.696111499464637 .\n",
      "Completed iter 100 / 600 . Training loss:  3.684755513828602 .\n",
      "Completed iter 200 / 600 . Training loss:  3.6699267788513743 .\n",
      "Completed iter 300 / 600 . Training loss:  3.6374185757504014 .\n",
      "Completed iter 400 / 600 . Training loss:  3.616003471814809 .\n",
      "Completed iter 500 / 600 . Training loss:  3.5673438149995604 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.2948356951424294 .\n",
      "Completed iter 100 / 400 . Training loss:  2.317862703131658 .\n",
      "Completed iter 200 / 400 . Training loss:  2.313749656384187 .\n",
      "Completed iter 300 / 400 . Training loss:  2.301921111342344 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4864443267387144 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4495717979404223 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4937593768863993 .\n",
      "Completed iter 300 / 400 . Training loss:  2.455609781535616 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.61762877007892 .\n",
      "Completed iter 100 / 400 . Training loss:  2.618755193783452 .\n",
      "Completed iter 200 / 400 . Training loss:  2.607426430003139 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5933206198345236 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.756190722704683 .\n",
      "Completed iter 100 / 400 . Training loss:  2.743433025793651 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7513771783536938 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7253020640464527 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9369259151562126 .\n",
      "Completed iter 100 / 400 . Training loss:  2.934093869682873 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9312707362734023 .\n",
      "Completed iter 300 / 400 . Training loss:  2.907296754273192 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.075111342867086 .\n",
      "Completed iter 100 / 400 . Training loss:  3.072299811883804 .\n",
      "Completed iter 200 / 400 . Training loss:  3.05179142874276 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0436693152998027 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.237778595443033 .\n",
      "Completed iter 100 / 400 . Training loss:  3.2123422793845355 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2096578225879187 .\n",
      "Completed iter 300 / 400 . Training loss:  3.191620369618136 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.384875093789918 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3761401167404754 .\n",
      "Completed iter 200 / 400 . Training loss:  3.349007534863466 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3314434938667925 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5369978747979482 .\n",
      "Completed iter 100 / 400 . Training loss:  3.5113609839728794 .\n",
      "Completed iter 200 / 400 . Training loss:  3.492595294537363 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4824808553881486 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.680059230640678 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6554516916657467 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6342817198497124 .\n",
      "Completed iter 300 / 400 . Training loss:  3.619832937350449 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3220683262032384 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3150479718532653 .\n",
      "Completed iter 200 / 400 . Training loss:  2.318916867966741 .\n",
      "Completed iter 300 / 400 . Training loss:  2.3018494729851593 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.486231796237072 .\n",
      "Completed iter 100 / 400 . Training loss:  2.470071913685941 .\n",
      "Completed iter 200 / 400 . Training loss:  2.465566904766499 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4504946204547706 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.608440043544075 .\n",
      "Completed iter 100 / 400 . Training loss:  2.598821952927815 .\n",
      "Completed iter 200 / 400 . Training loss:  2.571943898050555 .\n",
      "Completed iter 300 / 400 . Training loss:  2.594085773377194 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7975352136547955 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7827207769865177 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7672881196510133 .\n",
      "Completed iter 300 / 400 . Training loss:  2.771826038074218 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.92271995522803 .\n",
      "Completed iter 100 / 400 . Training loss:  2.89714127241889 .\n",
      "Completed iter 200 / 400 . Training loss:  2.8777602557815167 .\n",
      "Completed iter 300 / 400 . Training loss:  2.876565558730164 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0808165033992845 .\n",
      "Completed iter 100 / 400 . Training loss:  3.048374800500118 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0552624558071875 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0327908939815305 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2238032228237716 .\n",
      "Completed iter 100 / 400 . Training loss:  3.2004938808689736 .\n",
      "Completed iter 200 / 400 . Training loss:  3.198640277410527 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1831880046567793 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.4034771859953725 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3748122706187513 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3545775798752153 .\n",
      "Completed iter 300 / 400 . Training loss:  3.33144544862044 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.531840499089276 .\n",
      "Completed iter 100 / 400 . Training loss:  3.5090405997617555 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4942593683637204 .\n",
      "Completed iter 300 / 400 . Training loss:  3.47041351696345 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.7015156560243057 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6784260322933613 .\n",
      "Completed iter 200 / 400 . Training loss:  3.681622324004257 .\n",
      "Completed iter 300 / 400 . Training loss:  3.6331366031930736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3206864776167437 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3036050786043534 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4850784990915207 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4625910180801203 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.622161882757313 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 200 . Training loss:  2.5971417070436287 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7878771465167302 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7656818449177436 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.953714175908503 .\n",
      "Completed iter 100 / 200 . Training loss:  2.925167877253338 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0757924154285106 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0527576341933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2255940945691397 .\n",
      "Completed iter 100 / 200 . Training loss:  3.203901175221293 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4057870534797217 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3770890557309836 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.572546117067062 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5414298145622123 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.710509397834309 .\n",
      "Completed iter 100 / 200 . Training loss:  3.686345375005871 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.306405887546793 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2955360704762193 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.451221809949087 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4423340422258475 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.608914741609734 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6061039868704796 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.760630018239776 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7365202623924807 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9211432007975997 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9063035180527614 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.039671495348085 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0307689795527217 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.202175894633073 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2070533128446055 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4169659952400853 .\n",
      "Completed iter 100 / 200 . Training loss:  3.4020938853486884 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5476133107875563 .\n",
      "Completed iter 100 / 200 . Training loss:  3.52543674798602 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7310957486556515 .\n",
      "Completed iter 100 / 200 . Training loss:  3.7093063061102916 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.296692410345086 .\n",
      "Completed iter 100 / 200 . Training loss:  2.281432669839529 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.464290955159415 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4552113292820086 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.61311727542361 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5962002316175345 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7671469727118185 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7663504999459976 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9354308816479104 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9115992683870053 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.090606853883209 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0781616798453535 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.240057029439625 .\n",
      "Completed iter 100 / 200 . Training loss:  3.220397858193499 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3948184776366594 .\n",
      "Completed iter 100 / 200 . Training loss:  3.370782483097808 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5444453750197895 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5150953408366137 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6725448238175584 .\n",
      "Completed iter 100 / 200 . Training loss:  3.662099315568577 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.275326592245064 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2651136255434374 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4765669685601672 .\n",
      "Completed iter 100 / 200 . Training loss:  2.472806037109327 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6122018407256067 .\n",
      "Completed iter 100 / 200 . Training loss:  2.600394331702175 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.765943465919442 .\n",
      "Completed iter 100 / 200 . Training loss:  2.763422372797619 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.8740386941502156 .\n",
      "Completed iter 100 / 200 . Training loss:  2.892920220549057 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.072660377899112 .\n",
      "Completed iter 100 / 200 . Training loss:  3.06219088585119 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.216005939550436 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1926911685531385 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4026288179724844 .\n",
      "Completed iter 100 / 200 . Training loss:  3.38936279593534 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.526170628311144 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5024774077113014 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.675961722983603 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6631360422634414 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3267757275418663 .\n",
      "Completed iter 100 / 200 . Training loss:  2.307873577390519 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4601827139255037 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4569029513122733 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.594962026902264 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5894546261308635 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7555331793119024 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7378545726745895 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9381955659248895 .\n",
      "Completed iter 100 / 200 . Training loss:  2.927640518451713 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.069378767172822 .\n",
      "Completed iter 100 / 200 . Training loss:  3.050616860125565 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2283135371859535 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2138939384839675 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3966649540504026 .\n",
      "Completed iter 100 / 200 . Training loss:  3.370383453782935 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5246347374220983 .\n",
      "Completed iter 100 / 200 . Training loss:  3.506393535682178 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6814361931055317 .\n",
      "Completed iter 100 / 200 . Training loss:  3.652400293268049 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.32505109856517 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.3368948333244397 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.2981020490637896 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.269542425187781 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.251319587062751 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.274197900282311 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2665346616206574 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.245192442201352 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.2268815816592107 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.221192961631796 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.2206098303058424 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.2074755113759594 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.2421156229993766 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.21104855163889 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.186117510804317 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.2202720152720716 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.1864140727234496 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.2184346664944297 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.1572080927796855 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.1520155199355098 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4875640432661346 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 2000 . Training loss:  2.5041976515847395 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.461238105913708 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.4402769203254255 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4268318791122527 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.4048658592635035 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.4218537986413526 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.4155852269018676 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.406658686376263 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.413311467797821 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4283384207999297 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.3811132261915877 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.395678868237192 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.3790125648961364 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.341265357957262 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.3320996553095674 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3221929240541233 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3463893486998395 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.3647710439938674 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3231102327142046 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.616823322422643 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6156883623849776 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.606634527513438 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.60169804087715 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5780161239602633 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.5609791866501563 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.5635771473163103 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.535981782557212 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.523454943220147 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.524701585694029 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4938810082371328 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.5134684096627136 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.4790733653129777 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.4655366456365027 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5163766822467135 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.492772652826851 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.480411193063443 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.4508302096637906 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.467364057229006 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.403824769052768 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.753152413300279 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7438759805665316 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.7240331625964687 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.709478372892442 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.7459811859723966 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.713353645724595 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.6751454619482935 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.6394107172716 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6463295200046777 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.6286680092731434 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.660302073954713 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.633788891985263 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.6157044505806897 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.626308799159398 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.6182784231986913 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.612942200840702 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.577105205474604 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.6642257555133995 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.572128802345923 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5522355380439232 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.9524372179784155 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.921177402754103 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.931534234364321 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.8802206060450413 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.8835899101147184 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8869883694432077 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.876134041521518 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.8623554098281554 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.8276771317583633 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.866395568159746 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.828420156902747 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.808552510256547 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.750009266339691 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8104261201907437 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.7773788797838868 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.790812512480081 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.7218971781517354 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.7269681370958176 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.795836391530882 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6946622899999895 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.0915078652020984 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.053239762456186 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0302221259377298 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0245341811550452 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0025991618994725 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.0273354929099714 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.942332096854531 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9724877961259515 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.9464515533254225 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.9531586310470086 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.889211948715349 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8994851731227707 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.935510231997693 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.900840503417002 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.9041410170562987 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.8709756266106163 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8522431491572324 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.858538600659249 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.826613802503571 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.812824241856814 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.2417522203355595 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.2230790649018655 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.210318091599583 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.1754996824823594 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.1592929648568244 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1840788765755548 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.1579240019126837 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.0987817974291274 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.0967785230998435 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1479508337150914 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.088916260375849 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.042410157326569 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.067592114991742 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.99155839954254 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0113071518701013 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.9926941674443865 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.961471702680309 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.021804480790796 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9769713458947757 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.91166167481306 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.3753303669667956 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3385672432571445 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.311367054565255 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.3147215980758817 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.27487015232037 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.2441836290213253 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 600 / 2000 . Training loss:  3.249734902391235 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.205016829683894 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.186391398005769 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.200760246269343 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1870682847425456 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.222778323593152 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.193549074644042 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1798527791327404 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0854643665294677 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.145510717013794 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1454416520738118 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0440700528608566 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.03339403790884 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.113180188246423 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.529186489057806 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.5296738378464045 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.472147179259449 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4200176142279965 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4167852184419387 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4295007947310676 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.4128285861937755 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.352574311185342 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3325865418122858 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.372669926288962 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.2927650201601484 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.268140544446827 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.302233423540173 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.279092172215792 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.2759900429285302 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.223421497218522 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.186247183164544 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.152003403867982 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.1853696891967007 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.200547188134931 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.7190326118430397 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.7216408934479968 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.6642345059668613 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.6469059923814626 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.6210380613738877 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.590651483039201 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.535464269367941 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.5671968497072637 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.4979690508741537 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.4797722441456758 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.4792920923286204 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.4516155000761675 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.405761672201756 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.3898063410945154 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.3877235936255525 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.3719806199524998 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.2856210370326067 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.327823944891313 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.2929388303242297 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.2662407433801803 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.33059948768774 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.301969629529863 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.283782489096365 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.2938353849698725 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.284882451092104 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2490282447997907 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.248618562678289 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.2488110910758565 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.250428039725991 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.2482813901293555 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.472688732130434 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.480773732107929 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4607187593800255 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.44775716495199 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.4241519615953835 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.428716558515034 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.4096547800772354 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.3964859874723086 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.416835732762431 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.392275130562648 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.6175812801713456 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.611497342618184 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.5906805480903903 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.5996331051829493 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5814770313138053 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.571132201650669 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.5529320659244132 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.539712327960855 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.538366069983521 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.53855260390584 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.782757742720505 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.756003031399142 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.765852237370946 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7618043833669583 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.733915152653107 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.705005589320295 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.724288836726226 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.70245744796474 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6838792952214554 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.674433868595069 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9327877921247696 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.9117515568934262 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.9117425491779523 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.891376937227567 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.878155873924422 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.848321193763087 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.839203732686425 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.8483909650375048 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.807229494841266 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.82057146413105 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.080361806482424 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.054022364616584 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.0384705844646938 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0384404319687515 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.0071186054534245 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9778484548835125 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9752835207500303 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.9731859328388417 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.9552531424564856 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9425027289763293 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.2102499648103415 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.2065300716999285 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.168292283488534 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1724032970488567 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1427797622377684 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.127522237862425 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.140866870420915 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.1104150274751774 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.09912882950512 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.0524334038202685 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.4086704014225235 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.37862527531293 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 200 / 1000 . Training loss:  3.3716504960048317 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.351861549874063 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.3172362888733726 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.301520409824351 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.2913340135370954 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.2489521362068077 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.243983900186974 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.235770611909703 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.5680509049513054 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.5320455492130565 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.539872463999247 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.4881815865790307 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.455998040535092 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.457749978132387 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.427559435183878 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4030367325385713 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.390184397972966 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.3801338053076293 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.651086605978893 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6090743419891576 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.6076191657165784 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5832856276980363 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.574143479368339 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.5361631102151905 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.4992554028368317 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4574524946093703 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.4410805291814577 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.443356164505147 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3078997172897444 .\n",
      "Completed iter 100 / 600 . Training loss:  2.291039067556606 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2787486318700965 .\n",
      "Completed iter 300 / 600 . Training loss:  2.2778985803379124 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2638216504483704 .\n",
      "Completed iter 500 / 600 . Training loss:  2.2404733855347247 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.46946398532583 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4729732257589556 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4447617624976052 .\n",
      "Completed iter 300 / 600 . Training loss:  2.448129379928742 .\n",
      "Completed iter 400 / 600 . Training loss:  2.425484742291821 .\n",
      "Completed iter 500 / 600 . Training loss:  2.4045865720076276 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.616528660202816 .\n",
      "Completed iter 100 / 600 . Training loss:  2.599527452156646 .\n",
      "Completed iter 200 / 600 . Training loss:  2.5861805712923185 .\n",
      "Completed iter 300 / 600 . Training loss:  2.5806892604628295 .\n",
      "Completed iter 400 / 600 . Training loss:  2.5750101116720954 .\n",
      "Completed iter 500 / 600 . Training loss:  2.571071098783202 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.7674765610214047 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7585436079177303 .\n",
      "Completed iter 200 / 600 . Training loss:  2.7422345951763245 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7250925515777498 .\n",
      "Completed iter 400 / 600 . Training loss:  2.7322592002069603 .\n",
      "Completed iter 500 / 600 . Training loss:  2.708187532013648 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.9239238547058033 .\n",
      "Completed iter 100 / 600 . Training loss:  2.908147088377971 .\n",
      "Completed iter 200 / 600 . Training loss:  2.8976171528905468 .\n",
      "Completed iter 300 / 600 . Training loss:  2.8921460291586287 .\n",
      "Completed iter 400 / 600 . Training loss:  2.884075303432344 .\n",
      "Completed iter 500 / 600 . Training loss:  2.864382304588064 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.101837889186606 .\n",
      "Completed iter 100 / 600 . Training loss:  3.0819008541587323 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0408120635976728 .\n",
      "Completed iter 300 / 600 . Training loss:  3.068086524096639 .\n",
      "Completed iter 400 / 600 . Training loss:  3.006942779633642 .\n",
      "Completed iter 500 / 600 . Training loss:  3.013082689467239 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2290461895759783 .\n",
      "Completed iter 100 / 600 . Training loss:  3.209547501621752 .\n",
      "Completed iter 200 / 600 . Training loss:  3.195259531075953 .\n",
      "Completed iter 300 / 600 . Training loss:  3.1977100110792334 .\n",
      "Completed iter 400 / 600 . Training loss:  3.1829176404415187 .\n",
      "Completed iter 500 / 600 . Training loss:  3.151921405766587 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3697763422699145 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3497782660116027 .\n",
      "Completed iter 200 / 600 . Training loss:  3.351238375872766 .\n",
      "Completed iter 300 / 600 . Training loss:  3.3248595319429697 .\n",
      "Completed iter 400 / 600 . Training loss:  3.273165256797336 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2917180071556973 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.5333437434154042 .\n",
      "Completed iter 100 / 600 . Training loss:  3.529195986721782 .\n",
      "Completed iter 200 / 600 . Training loss:  3.486154167843668 .\n",
      "Completed iter 300 / 600 . Training loss:  3.4701671172207162 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4683237859706133 .\n",
      "Completed iter 500 / 600 . Training loss:  3.4134723164450396 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.7050148131529093 .\n",
      "Completed iter 100 / 600 . Training loss:  3.6925132396798697 .\n",
      "Completed iter 200 / 600 . Training loss:  3.6653655707645694 .\n",
      "Completed iter 300 / 600 . Training loss:  3.63403817307192 .\n",
      "Completed iter 400 / 600 . Training loss:  3.6092768269099444 .\n",
      "Completed iter 500 / 600 . Training loss:  3.584002759180977 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3158888238849618 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2965702437086435 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2843160176156845 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2805684225694796 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4722916950577476 .\n",
      "Completed iter 100 / 400 . Training loss:  2.447771760031493 .\n",
      "Completed iter 200 / 400 . Training loss:  2.431241307993516 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4499021698578436 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6074226129249727 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5807941188224652 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6023234124777352 .\n",
      "Completed iter 300 / 400 . Training loss:  2.576723775387886 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.771132209502008 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7462870914487847 .\n",
      "Completed iter 200 / 400 . Training loss:  2.739661119379482 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7321151775847254 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.920241458159221 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9408699028193714 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9063381787999667 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9075500818261077 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0640674495437077 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0611450389616452 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0310620753530397 .\n",
      "Completed iter 300 / 400 . Training loss:  3.046159388000461 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2395646353776835 .\n",
      "Completed iter 100 / 400 . Training loss:  3.223918640433245 .\n",
      "Completed iter 200 / 400 . Training loss:  3.194592572186022 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1928474196542096 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.4300242337211353 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3903852815501887 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3591320847755597 .\n",
      "Completed iter 300 / 400 . Training loss:  3.350489386635141 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.519330277081089 .\n",
      "Completed iter 100 / 400 . Training loss:  3.4951686115195764 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4731016496866065 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 300 / 400 . Training loss:  3.450816614727267 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.7255019612556244 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6959704753801343 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6471013707859647 .\n",
      "Completed iter 300 / 400 . Training loss:  3.6363577864262195 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.2881955883505634 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3024550216893647 .\n",
      "Completed iter 200 / 400 . Training loss:  2.286231880282887 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2661396898041883 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.465388725202455 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4585524105140824 .\n",
      "Completed iter 200 / 400 . Training loss:  2.457703256733645 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4426793030821417 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6236314616132024 .\n",
      "Completed iter 100 / 400 . Training loss:  2.6091054654539074 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6160727012697973 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5992766142390207 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.791439325769773 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7774953924262666 .\n",
      "Completed iter 200 / 400 . Training loss:  2.768554226392247 .\n",
      "Completed iter 300 / 400 . Training loss:  2.749620765931387 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9411170835801794 .\n",
      "Completed iter 100 / 400 . Training loss:  2.923305374826086 .\n",
      "Completed iter 200 / 400 . Training loss:  2.903506619635967 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9003686571743947 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0823624306182755 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0548016585968707 .\n",
      "Completed iter 200 / 400 . Training loss:  3.039463714291485 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0252502140101365 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.218921575986013 .\n",
      "Completed iter 100 / 400 . Training loss:  3.2065171424566783 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1748407254580453 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1763050303037694 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3722601806737122 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3588482832453566 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3335749945928574 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3296772813165667 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5619697911729133 .\n",
      "Completed iter 100 / 400 . Training loss:  3.518580295913777 .\n",
      "Completed iter 200 / 400 . Training loss:  3.49229958246738 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4742390304585395 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6668436597295333 .\n",
      "Completed iter 100 / 400 . Training loss:  3.656537783588144 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6315390924076776 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5867386163244896 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3052910706521286 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3030673483666995 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4756544500824065 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4540224960788457 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6329207074941223 .\n",
      "Completed iter 100 / 200 . Training loss:  2.634491027594661 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.763612331336194 .\n",
      "Completed iter 100 / 200 . Training loss:  2.744770866027805 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.92857272377727 .\n",
      "Completed iter 100 / 200 . Training loss:  2.892717649274599 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.093512931877048 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0788033470628484 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.210077573126054 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1940476086419247 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3652445113231053 .\n",
      "Completed iter 100 / 200 . Training loss:  3.336442507801647 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5638416045146584 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5423907804888355 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6807935351641397 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6513698486927835 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3099718501974684 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2860338953493207 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4561530769386914 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4560814507153466 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6475202747896733 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6266877588077886 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.757415090844905 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7442965273356306 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9221596302089714 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9031158521404272 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0577071768649056 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0642947386875283 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.226978471724123 .\n",
      "Completed iter 100 / 200 . Training loss:  3.209412378608538 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.418820717602282 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3863069727344444 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5301844081749714 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5035881599878933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.686189777914035 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6547292664811244 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.323445705420134 .\n",
      "Completed iter 100 / 200 . Training loss:  2.310627100264456 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4485938389402047 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4486700272448374 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.5984263880874936 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5771332415413117 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7627781884343765 .\n",
      "Completed iter 100 / 200 . Training loss:  2.748115340622642 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.931261318167776 .\n",
      "Completed iter 100 / 200 . Training loss:  2.91934577601776 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0990521665329367 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0820269768226494 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2205520852227627 .\n",
      "Completed iter 100 / 200 . Training loss:  3.202807027932285 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4242183644833712 .\n",
      "Completed iter 100 / 200 . Training loss:  3.4028774431120636 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.541793033330809 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5269184650710637 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6752833720110862 .\n",
      "Completed iter 100 / 200 . Training loss:  3.644444800653883 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.325813460439621 .\n",
      "Completed iter 100 / 200 . Training loss:  2.317096389709733 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.459635657513428 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4647650509531807 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6284018606482027 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6111040295158467 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7607718019469534 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7495461670798242 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9510790423391686 .\n",
      "Completed iter 100 / 200 . Training loss:  2.930089978871715 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0858465851105414 .\n",
      "Completed iter 100 / 200 . Training loss:  3.05856258078522 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.241962042328077 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2212157242086916 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.406985994445264 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3730355327418042 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.546833917182478 .\n",
      "Completed iter 100 / 200 . Training loss:  3.517823840232249 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7226808990959714 .\n",
      "Completed iter 100 / 200 . Training loss:  3.681078231164725 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.295892574766757 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3009044340104534 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.481443881951057 .\n",
      "Completed iter 100 / 200 . Training loss:  2.467973746559183 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6199494510046444 .\n",
      "Completed iter 100 / 200 . Training loss:  2.607603443477571 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7529879939347395 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7498707633101684 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.8927581347359035 .\n",
      "Completed iter 100 / 200 . Training loss:  2.888859957563004 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0678673398723384 .\n",
      "Completed iter 100 / 200 . Training loss:  3.043655567579221 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2343964915992345 .\n",
      "Completed iter 100 / 200 . Training loss:  3.206647413186506 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3661591523798813 .\n",
      "Completed iter 100 / 200 . Training loss:  3.342217804657502 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.524160814348615 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5018044505441854 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7156034100142623 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6725289194933532 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.3006328462437535 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.3077353150603566 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.286716508929917 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.273137837274738 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.2500642897539196 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.240924677126835 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2318613485566736 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.2513841621796 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.237906289661743 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.2264168452105237 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.23224161180745 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.22058213100916 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.217746994011423 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.205127617979781 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.22799584063945 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.1724011271091497 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.210147512345206 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.190844450260469 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.161520320942556 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.1980541728305383 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.5103842820903504 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4597706768841894 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4607729105060003 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.472380251217227 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4680038992494473 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.4321855269371646 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.41070205895378 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.4443843063191615 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.422597575702034 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.399742495725469 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3406475533616056 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4238324836200444 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.3905826162451125 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.37527246393964 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.328722011406977 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.3051473923853676 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.309387827983425 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3596947011720544 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.2229975588270796 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.2987361352738067 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.6335689888505738 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6040908825443565 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.597956985576417 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.5505165762774036 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.565400468792638 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.5493135955560096 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.572978281579265 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.54471044906439 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.5485628763534764 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.5138094130100646 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.5132573683775976 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.5033657922430375 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.4664953824562628 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.4535369498442945 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.4304916883177583 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.401678493710905 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.4246108297831124 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.4880471054577358 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.4039873894217267 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3868581985231967 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7795229236678742 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7515842630558223 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.726027108954995 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.698602849446935 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.7146014482637666 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7041644416651174 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.6641898074027 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.646469348186292 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6165835898100402 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.6689781574940143 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.616383414115247 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.5996077081662348 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.5842637785993205 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.645918626166249 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.578075206364393 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.601662995850205 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.6418430270453745 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.621631427891743 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.548159223322465 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6004342071883517 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.9127309386561855 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.8985082089609007 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.892868918006009 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.866599110087069 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.887065126458111 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.847794508770032 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.844958276405186 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.859416996474274 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.852043040129363 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.782393151472789 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.79220855932395 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.775394237514168 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.734418016186105 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.7488827537282305 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.7343474241095853 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6343903295839057 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1600 / 2000 . Training loss:  2.743071556850954 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.761053429865356 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.7366931017581724 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6029548026628406 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.0691848779802013 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.05140569854057 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0594814504194816 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0270729749053995 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0068910565485254 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.984603720113166 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0161840774838273 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9686607076710194 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.956827643034952 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.9137705061143375 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9053312351560514 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8781400671632147 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.8685324649415027 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.941105238587428 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.8554187671022038 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.908517204569035 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.7874679582010646 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8357178860113987 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.8032196764503703 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8533147741840628 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.2763558572169575 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.2282817345649106 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.161055431714174 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.199182690036612 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.1872617490733957 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1288442608739646 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.104111371764264 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.0889447682402587 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.094188563996466 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.0850866666118444 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.0500003011190353 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0733133998256847 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.0466238076530403 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0101236997817225 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.984980122423373 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.9367661673074386 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.952534201627122 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9238973069454635 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9622540530099624 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8613983631041187 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.3761667780560947 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3974351170572334 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3330548637360407 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.279011049138572 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.30266167495266 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.258936401568219 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.2439840255668164 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.278769063750462 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.182834922906353 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.2035999208443933 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1563751682370143 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.1549095440062294 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.1471846160538357 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1445061605170563 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.1251713086476407 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.095529834215248 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.0957141125731233 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0312137147607268 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.048105920943809 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0580030939094325 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.522370401605066 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.5064941709061306 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.491916872764728 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.447096193487104 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4592229239042154 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.3897953185315277 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.384049910800033 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.372662745146847 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.364572156746896 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.306973601706151 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.295261883589231 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.247988016932861 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.2580641653973044 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.218120527772955 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.225230298171648 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.2059167158107007 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1874757560674145 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.177817543251755 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.162876009245909 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.1556373780352667 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.6873388037233212 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6773061597179733 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.619969805110705 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.580494433714435 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.581273143380625 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.5173541874880234 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.5121373569928496 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4616845236535227 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.433458900080769 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.4345445362759324 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.4709162366325126 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.4019113430765184 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.404031945441254 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.304803429739732 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.348399314386935 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.3131798365380334 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.262478175923728 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.269881680022356 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.2108666997874757 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.221520610800452 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.3255422704658035 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.301282424776181 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.282294159142245 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.265488732965583 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2518975284467597 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.26893610449504 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.260089929460683 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.229906283567942 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.222582556816871 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.22531157292554 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.4205922035745417 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4444488452379254 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4294023601249255 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.4105787548290962 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.417981808065047 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.3988365382069365 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.374352834085987 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.3963351206096206 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.34339096504871 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.368696220742305 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.6111736584570493 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 1000 . Training loss:  2.6034280547360185 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.599098633506124 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.5949591717744087 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5745373471271487 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.5398687855958006 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.534680783430563 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.537545064004698 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.5123687007774325 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.5430132333984408 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.8015897552854048 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7712338091260413 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7719766923559908 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7538197188589875 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.7464529389916463 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.7181406072697483 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.6773092457188836 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.698012667731975 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6583621458131876 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6803722226592495 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9235071129499817 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.9178183927560504 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.919331139269845 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.87903778788458 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.8604613396305947 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8396579914585134 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.834648485658127 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.8521258811929275 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.8166480489532937 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.7997373309171705 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0717809274110235 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.059841965702611 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.0367739996228664 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0193225612765007 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.0008287855661107 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9707869112428202 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.990186440484849 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.9722673155288617 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.9542850396669262 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9289063572875227 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.2074138705348676 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.191979275162259 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.152089687085608 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1732872886568657 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1451309407115358 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.1276147773617407 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.0913037927943003 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.07407928194554 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.065330893553866 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.090764554358933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.366981646689142 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.359328339072938 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.313208056122236 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.315102862080424 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.3045948843374364 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.2759796841898057 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.262223062863038 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.2498498376958285 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2189435893668876 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.1967222147783625 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.5097445561250833 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.48856372875999 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.457545000898989 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.4538680156537933 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.4120188553281774 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.3940489162993748 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.3559652490815397 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3358865681333025 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.307268193608648 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.292666086814841 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6875710422004913 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6627440607651662 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.6212026807184063 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5937087715204092 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.560123571826395 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.5080961248880067 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.4878825948587773 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.476787598844499 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.466414786026371 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.4521167426122976 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.333446762319714 .\n",
      "Completed iter 100 / 600 . Training loss:  2.3236490851941793 .\n",
      "Completed iter 200 / 600 . Training loss:  2.3192114351893127 .\n",
      "Completed iter 300 / 600 . Training loss:  2.290669118923528 .\n",
      "Completed iter 400 / 600 . Training loss:  2.266779271205012 .\n",
      "Completed iter 500 / 600 . Training loss:  2.287392005903736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.437617514455012 .\n",
      "Completed iter 100 / 600 . Training loss:  2.426261681951884 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4413278821737197 .\n",
      "Completed iter 300 / 600 . Training loss:  2.400950707044461 .\n",
      "Completed iter 400 / 600 . Training loss:  2.3990496971239788 .\n",
      "Completed iter 500 / 600 . Training loss:  2.4095235192916684 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.6201128033750782 .\n",
      "Completed iter 100 / 600 . Training loss:  2.583146077721797 .\n",
      "Completed iter 200 / 600 . Training loss:  2.57804837025098 .\n",
      "Completed iter 300 / 600 . Training loss:  2.5465553225356894 .\n",
      "Completed iter 400 / 600 . Training loss:  2.57178258439431 .\n",
      "Completed iter 500 / 600 . Training loss:  2.547824637547442 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.7630547248135646 .\n",
      "Completed iter 100 / 600 . Training loss:  2.769003036225807 .\n",
      "Completed iter 200 / 600 . Training loss:  2.76076311867183 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7309886209526084 .\n",
      "Completed iter 400 / 600 . Training loss:  2.7241992855637855 .\n",
      "Completed iter 500 / 600 . Training loss:  2.683046866303101 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.905355339580404 .\n",
      "Completed iter 100 / 600 . Training loss:  2.8865021510568276 .\n",
      "Completed iter 200 / 600 . Training loss:  2.863031452630306 .\n",
      "Completed iter 300 / 600 . Training loss:  2.8822909016459035 .\n",
      "Completed iter 400 / 600 . Training loss:  2.836675267513432 .\n",
      "Completed iter 500 / 600 . Training loss:  2.810385556307092 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.076689657215282 .\n",
      "Completed iter 100 / 600 . Training loss:  3.0697346058476747 .\n",
      "Completed iter 200 / 600 . Training loss:  3.032657204752898 .\n",
      "Completed iter 300 / 600 . Training loss:  3.029219806247279 .\n",
      "Completed iter 400 / 600 . Training loss:  3.009369461578729 .\n",
      "Completed iter 500 / 600 . Training loss:  2.97753256194839 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2097792802617784 .\n",
      "Completed iter 100 / 600 . Training loss:  3.204996040498756 .\n",
      "Completed iter 200 / 600 . Training loss:  3.159151519562192 .\n",
      "Completed iter 300 / 600 . Training loss:  3.1432599565426704 .\n",
      "Completed iter 400 / 600 . Training loss:  3.1528265083736153 .\n",
      "Completed iter 500 / 600 . Training loss:  3.119434227034254 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3825723123958706 .\n",
      "Completed iter 100 / 600 . Training loss:  3.356432570330558 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3332070532569262 .\n",
      "Completed iter 300 / 600 . Training loss:  3.3064421488719846 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 400 / 600 . Training loss:  3.287618763723858 .\n",
      "Completed iter 500 / 600 . Training loss:  3.283891450679458 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.530256237417645 .\n",
      "Completed iter 100 / 600 . Training loss:  3.4818512646268034 .\n",
      "Completed iter 200 / 600 . Training loss:  3.461599162165981 .\n",
      "Completed iter 300 / 600 . Training loss:  3.436743287940475 .\n",
      "Completed iter 400 / 600 . Training loss:  3.431109569190777 .\n",
      "Completed iter 500 / 600 . Training loss:  3.4149543325998755 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.6948767994243794 .\n",
      "Completed iter 100 / 600 . Training loss:  3.6707763571616834 .\n",
      "Completed iter 200 / 600 . Training loss:  3.639346135001589 .\n",
      "Completed iter 300 / 600 . Training loss:  3.6092556874283463 .\n",
      "Completed iter 400 / 600 . Training loss:  3.57344506359971 .\n",
      "Completed iter 500 / 600 . Training loss:  3.5396713092655037 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.321482949325513 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2946776839913983 .\n",
      "Completed iter 200 / 400 . Training loss:  2.277911781747311 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2771272871348556 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4812752700858804 .\n",
      "Completed iter 100 / 400 . Training loss:  2.438549170131324 .\n",
      "Completed iter 200 / 400 . Training loss:  2.435209458644837 .\n",
      "Completed iter 300 / 400 . Training loss:  2.441319028641039 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.617600748585518 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5919440888239276 .\n",
      "Completed iter 200 / 400 . Training loss:  2.591935845235919 .\n",
      "Completed iter 300 / 400 . Training loss:  2.595308052036344 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.773358313910908 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7670115764868317 .\n",
      "Completed iter 200 / 400 . Training loss:  2.758446529265189 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7449814102314187 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9101528953367044 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9010009943191406 .\n",
      "Completed iter 200 / 400 . Training loss:  2.889557546966897 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8654348165657737 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.067421233110809 .\n",
      "Completed iter 100 / 400 . Training loss:  3.051200525538823 .\n",
      "Completed iter 200 / 400 . Training loss:  3.02355229440579 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0176955075448335 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.276889769514959 .\n",
      "Completed iter 100 / 400 . Training loss:  3.255019371975995 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2226885678289654 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2030859851233258 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.4127519755987996 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3903694157602287 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3444810249250385 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3302043887171813 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5132522361343854 .\n",
      "Completed iter 100 / 400 . Training loss:  3.5032572718538897 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4854542266708326 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4631431768888694 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.681169371105285 .\n",
      "Completed iter 100 / 400 . Training loss:  3.668339181676525 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6263602284939163 .\n",
      "Completed iter 300 / 400 . Training loss:  3.602961220969008 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.339827091585532 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3186436845585696 .\n",
      "Completed iter 200 / 400 . Training loss:  2.301314798133249 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2878852095956583 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4851682078029103 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4805207044894653 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4418864516795837 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4518021368427374 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6310160515043153 .\n",
      "Completed iter 100 / 400 . Training loss:  2.612405921602924 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6053559662223913 .\n",
      "Completed iter 300 / 400 . Training loss:  2.591327581535988 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7534902944194397 .\n",
      "Completed iter 100 / 400 . Training loss:  2.737538708825314 .\n",
      "Completed iter 200 / 400 . Training loss:  2.716536494450581 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7095870017466446 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9387052209169755 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9183337943742025 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9013470947357023 .\n",
      "Completed iter 300 / 400 . Training loss:  2.898655400322646 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0548868303457266 .\n",
      "Completed iter 100 / 400 . Training loss:  3.044679030885688 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0318879642018217 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9867183191108984 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2079847096453467 .\n",
      "Completed iter 100 / 400 . Training loss:  3.191400692536082 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1725148674487156 .\n",
      "Completed iter 300 / 400 . Training loss:  3.160249377487382 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3829344274563096 .\n",
      "Completed iter 100 / 400 . Training loss:  3.346057636022855 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3401091147090916 .\n",
      "Completed iter 300 / 400 . Training loss:  3.312646409479431 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.550378882129123 .\n",
      "Completed iter 100 / 400 . Training loss:  3.517056654275719 .\n",
      "Completed iter 200 / 400 . Training loss:  3.5035679004684366 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4529020211119503 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.668734476173293 .\n",
      "Completed iter 100 / 400 . Training loss:  3.63609609186485 .\n",
      "Completed iter 200 / 400 . Training loss:  3.599878545998493 .\n",
      "Completed iter 300 / 400 . Training loss:  3.574223662204288 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3016745818686566 .\n",
      "Completed iter 100 / 200 . Training loss:  2.294296939592297 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4538198801257485 .\n",
      "Completed iter 100 / 200 . Training loss:  2.437818526947489 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6164498200423316 .\n",
      "Completed iter 100 / 200 . Training loss:  2.608504915064706 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.762828500293121 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7643726735154646 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9099708702996017 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8852667264525294 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0789616132026216 .\n",
      "Completed iter 100 / 200 . Training loss:  3.065846425748571 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.216337819547734 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2004518415151493 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.381058606987996 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3715323522808576 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.557605243797446 .\n",
      "Completed iter 100 / 200 . Training loss:  3.526253204900767 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7178334033021647 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6754037402091813 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3182755367874703 .\n",
      "Completed iter 100 / 200 . Training loss:  2.306046624568375 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4766944020057156 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4624408949308165 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.619722899304797 .\n",
      "Completed iter 100 / 200 . Training loss:  2.608091940756635 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.76591003751151 .\n",
      "Completed iter 100 / 200 . Training loss:  2.744245675273046 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.925984825175504 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9103630815556727 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.1243595174526133 .\n",
      "Completed iter 100 / 200 . Training loss:  3.097126172924919 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.232012442637302 .\n",
      "Completed iter 100 / 200 . Training loss:  3.200683457219968 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.369912183115738 .\n",
      "Completed iter 100 / 200 . Training loss:  3.350417767329021 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.530844016441524 .\n",
      "Completed iter 100 / 200 . Training loss:  3.525081145678714 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6773427822807507 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6434757379956606 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3049794348868433 .\n",
      "Completed iter 100 / 200 . Training loss:  2.301588741641282 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.461952276834213 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4316903226989606 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.593778764461978 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5825378709215068 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7599423393260514 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7342894080361333 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.907596805266903 .\n",
      "Completed iter 100 / 200 . Training loss:  2.889038154328432 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.083605567957126 .\n",
      "Completed iter 100 / 200 . Training loss:  3.060520758049668 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2227683646663485 .\n",
      "Completed iter 100 / 200 . Training loss:  3.193309443635659 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3639757787019597 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3480128252172365 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.545703337570895 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5034811257057656 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.70349652679547 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6701089586540316 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.315991340936433 .\n",
      "Completed iter 100 / 200 . Training loss:  2.296508501412834 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4791620102789187 .\n",
      "Completed iter 100 / 200 . Training loss:  2.468064228021805 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6243414364565854 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5942163576270363 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.766556223314959 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7612797981051105 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9304580075709414 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8926770189652027 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0668151912495194 .\n",
      "Completed iter 100 / 200 . Training loss:  3.041314618067726 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.248981655738553 .\n",
      "Completed iter 100 / 200 . Training loss:  3.224558825507539 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3929126720593423 .\n",
      "Completed iter 100 / 200 . Training loss:  3.369082210480429 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5623374797529763 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5227473861562073 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6967901429479157 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6776103777596485 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3005467946669507 .\n",
      "Completed iter 100 / 200 . Training loss:  2.288176949441847 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4629292387388912 .\n",
      "Completed iter 100 / 200 . Training loss:  2.454936922977722 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.611131802335743 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5956713776101177 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7896265357837753 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7698225904080016 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.93507093753475 .\n",
      "Completed iter 100 / 200 . Training loss:  2.929812553254749 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0789030638588573 .\n",
      "Completed iter 100 / 200 . Training loss:  3.062902695569768 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.248614924221563 .\n",
      "Completed iter 100 / 200 . Training loss:  3.22074886012561 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3666236965416982 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3584955771394207 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.54648561083401 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5152592596664882 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.681441818662117 .\n",
      "Completed iter 100 / 200 . Training loss:  3.66464483065638 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.3150464294731585 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.3059293004170978 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.3072963620536866 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.271810074868035 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.2740150516320976 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.233705703049925 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2428581292356307 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.2363027081223006 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.2123541321145437 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.229818266328333 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.2030535542070364 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.1775864561773557 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.187612544797347 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2219460095567523 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.1660765023526904 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.179804747088565 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.190490891443881 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.135834338037169 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.0697431732296523 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.154419089547614 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4596353496841847 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.451278963494099 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4349043702082622 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.4294739779758707 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.3888556230277462 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.3979038704687126 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.384974349465577 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.3709326060691875 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.408222716685795 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3679943430674073 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3318768137405392 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.346721459141923 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.288603531942664 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2915206593358795 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.3126533236754856 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.287347110985185 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3254208316427762 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3230002797672147 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.2575625641720025 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.275087778789186 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.5904127639948773 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6160985074538634 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.5708926035763153 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.608707593685562 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5622655206913736 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 500 / 2000 . Training loss:  2.558990774377306 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.541378995617087 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.5399541975456312 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.530143931658632 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.4877639066587247 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.491305693465943 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4803877713636875 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.4377417143788964 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.492858573454278 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.4295523736852846 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.4529138277564453 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.424316581870407 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.420341196696594 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.4971674522892853 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3479419476267154 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.801296432023703 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.753648348543316 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.742535277727281 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.7258829297638814 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.703439744564662 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7147343820197327 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.7006834331922267 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.666100482783716 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.661731394294269 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.6735100381502908 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.6169230800448977 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.606199249313252 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.6320515343290536 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.654755408500224 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5856356892852355 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.5217252727647526 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.56957173512113 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.55533950343374 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.4919928787545924 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5536114621037487 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.9159338428815 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.9194324880079154 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.9162568103802804 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.869251229220695 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.8928157976573967 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8502248189114088 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.84461835568235 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.8065195538729757 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.8224391981023853 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.8339924983394793 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.7836061723695966 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.7883765080000704 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.8127023575119603 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.7725468164764635 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.7094372970494147 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.74459742630369 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.661632800445155 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.604414512077634 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.7156887979613535 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6607482839988936 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.067216544742084 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.0666168921352144 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.014284020134397 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0334099462876516 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.020719311578903 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9628291008631447 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.99983312235355 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.96918305130332 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.9116587332227946 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.918454802955736 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.900633790282158 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8187993790867414 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.832330397742593 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8635791731008915 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.838651956705265 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.872375399488949 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8112882806920867 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8285177957703285 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.787456266888953 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8260252203603766 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.22648866303967 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.1796340625555146 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.179413996855739 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.2017934846377956 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.1565094325760947 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1053480591454403 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.1119766367785866 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.064282713129307 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1146201294687557 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.030209967837217 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.0335726865000856 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0067154409223447 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9831087168911035 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9771965225079544 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.996977281719963 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.0110635171230586 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.965454629755391 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.913577408968777 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.917138218423107 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9309615124556188 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.418941192816327 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3761576962452278 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.387476145658195 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.3242254363073807 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.326378424626079 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.2571324459791624 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.198013145454833 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1949408222534483 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1862397127387814 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1517378429226595 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1421688140927806 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.152671837026412 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.124217501804943 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.096320780876974 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0839062948238114 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.0821364673896436 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1060181856405817 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.058578532594033 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.003456394801349 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9606694751232667 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.509470306123828 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.460068830899666 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.437759416086508 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4144609142966997 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.416078691258561 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.3299400341593817 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.3775553951507193 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3061000173499764 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.2913077235829453 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.2532265902779773 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.266614964577588 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1100 / 2000 . Training loss:  3.270311853115731 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.219319108569512 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.2282744447853764 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.137089296711619 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.183929517163729 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1682970605051355 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.1447866099951503 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0693682113962257 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.072340229389396 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.6955644840939055 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6101158686294115 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.614434416094795 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.5866400237322336 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.5518385038731566 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.553402124937982 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.5221610950165765 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4206676189441154 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.4182215663181132 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.383279258899802 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.3752137278648435 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.335073294873607 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.3418657418685473 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.289022935328301 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.2639090648557025 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.2353933930679415 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.19584517614424 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.241199665698276 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.2075385747169376 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0942011775802323 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.321129858110338 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.3024926046599243 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.317802450538232 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.2558254210249817 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2587933231140873 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.248238878275448 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.266334869078368 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.214025983057071 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.210810753306763 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.220506734827672 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.456866656242513 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4722367238085172 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4229842382297258 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.440627146979506 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.403118972486823 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.395967347943552 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.3549940997245122 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.388987728433343 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.350545226700011 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.319989119638093 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.611615736069661 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.5901798450686226 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.569999688239833 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.5578627838900254 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5727944732431434 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.5322030213953726 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.5465596947972893 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.5538971178037597 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.499364228904241 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.5220738691833113 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7818783016430606 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7811936555082224 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.747567578214103 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7425157367167796 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.710236673905173 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.7291094097822595 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.703280995693033 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.691336891863095 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.668269977454811 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6435695538569655 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9216063358434248 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.904132030351628 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8940553925317283 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.8827929623588577 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.857030865305043 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8402564353830035 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.826599537894371 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.8132679584816396 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.794717805603173 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.8008184298496963 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0773281728485085 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0532923143587767 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.0322646999888017 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0284666823518696 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.0364353939958297 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.982944970025807 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.964012393346839 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.9544579220833764 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.939035654170392 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.931280979957112 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.217488296506897 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.2154528155795155 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.2023270840921567 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.160845907847854 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.150333520539263 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.1350072502907778 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.069024881997802 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.0981431098213514 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.0548394525122857 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.0524321645758317 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.358791063510803 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.3126919750982644 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.314024989770113 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.2867027414786953 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.235147066475959 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.227693097870484 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.2220296509122344 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.173243606461325 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.1878671247977075 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.1385981636861153 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.5330000035549753 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.505756855268733 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.462875498804415 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.456666357129772 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.4200788555729034 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.421064551236957 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.3897815880336326 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3233409992540066 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2850593604583667 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.309369518621259 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.691440977975366 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6422677704194113 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.587705838860244 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.588586676056482 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.5721020625017674 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.523326912955031 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 600 / 1000 . Training loss:  3.4858853883619467 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4739219573767386 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.4380560530671964 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.4121390420665914 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3035900850107383 .\n",
      "Completed iter 100 / 600 . Training loss:  2.296853964619366 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2940739584851175 .\n",
      "Completed iter 300 / 600 . Training loss:  2.2824446677254677 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2594111182634165 .\n",
      "Completed iter 500 / 600 . Training loss:  2.26446211367372 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.4589375384728194 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4535004743283504 .\n",
      "Completed iter 200 / 600 . Training loss:  2.44748900517672 .\n",
      "Completed iter 300 / 600 . Training loss:  2.436068749773896 .\n",
      "Completed iter 400 / 600 . Training loss:  2.4078424815180957 .\n",
      "Completed iter 500 / 600 . Training loss:  2.3974150443441506 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.609276955886929 .\n",
      "Completed iter 100 / 600 . Training loss:  2.592328841501167 .\n",
      "Completed iter 200 / 600 . Training loss:  2.584753337851334 .\n",
      "Completed iter 300 / 600 . Training loss:  2.5788349718060912 .\n",
      "Completed iter 400 / 600 . Training loss:  2.5706401617204264 .\n",
      "Completed iter 500 / 600 . Training loss:  2.5423246181231103 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.765469887343897 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7557164279077546 .\n",
      "Completed iter 200 / 600 . Training loss:  2.733931885095217 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7420463284316265 .\n",
      "Completed iter 400 / 600 . Training loss:  2.732174177458893 .\n",
      "Completed iter 500 / 600 . Training loss:  2.7104658549489566 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.9161487765106626 .\n",
      "Completed iter 100 / 600 . Training loss:  2.91401458839275 .\n",
      "Completed iter 200 / 600 . Training loss:  2.8867151782110994 .\n",
      "Completed iter 300 / 600 . Training loss:  2.868486343082343 .\n",
      "Completed iter 400 / 600 . Training loss:  2.849105927806613 .\n",
      "Completed iter 500 / 600 . Training loss:  2.838948314775502 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.0758331006226505 .\n",
      "Completed iter 100 / 600 . Training loss:  3.0420807413670357 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0217119082446944 .\n",
      "Completed iter 300 / 600 . Training loss:  2.9988211734562276 .\n",
      "Completed iter 400 / 600 . Training loss:  2.983536336629267 .\n",
      "Completed iter 500 / 600 . Training loss:  2.9771248473058263 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2470037564022585 .\n",
      "Completed iter 100 / 600 . Training loss:  3.1999548107268616 .\n",
      "Completed iter 200 / 600 . Training loss:  3.178627117183605 .\n",
      "Completed iter 300 / 600 . Training loss:  3.149335073395934 .\n",
      "Completed iter 400 / 600 . Training loss:  3.1240280889323153 .\n",
      "Completed iter 500 / 600 . Training loss:  3.118001156747719 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3703382548688117 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3834187515606016 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3311006856085266 .\n",
      "Completed iter 300 / 600 . Training loss:  3.305778189290509 .\n",
      "Completed iter 400 / 600 . Training loss:  3.2824253282721427 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2570692985089713 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.537439398546126 .\n",
      "Completed iter 100 / 600 . Training loss:  3.4897245462396747 .\n",
      "Completed iter 200 / 600 . Training loss:  3.464066634589977 .\n",
      "Completed iter 300 / 600 . Training loss:  3.450678584338092 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4132350409639476 .\n",
      "Completed iter 500 / 600 . Training loss:  3.3860378866500107 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.672811566515357 .\n",
      "Completed iter 100 / 600 . Training loss:  3.6393443824741336 .\n",
      "Completed iter 200 / 600 . Training loss:  3.599026628609624 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5622038052215776 .\n",
      "Completed iter 400 / 600 . Training loss:  3.5514464628420637 .\n",
      "Completed iter 500 / 600 . Training loss:  3.4996915979419967 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.322860363958611 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2988228507734196 .\n",
      "Completed iter 200 / 400 . Training loss:  2.3020181246904694 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2698116731106754 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4450352329881855 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4254994341313783 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4078101550546243 .\n",
      "Completed iter 300 / 400 . Training loss:  2.3950062805754206 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6362361108519 .\n",
      "Completed iter 100 / 400 . Training loss:  2.614530279362433 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6095099725023747 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5696628657905407 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7563898632378585 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7329185175733404 .\n",
      "Completed iter 200 / 400 . Training loss:  2.707934173909247 .\n",
      "Completed iter 300 / 400 . Training loss:  2.6966098062672827 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.913546602094514 .\n",
      "Completed iter 100 / 400 . Training loss:  2.913397052825672 .\n",
      "Completed iter 200 / 400 . Training loss:  2.872605305253537 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8862826578258987 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.066434162881097 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0525087862538847 .\n",
      "Completed iter 200 / 400 . Training loss:  3.027245569707659 .\n",
      "Completed iter 300 / 400 . Training loss:  3.006137349861714 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.211492433017278 .\n",
      "Completed iter 100 / 400 . Training loss:  3.195977034687606 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1815173183590595 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1452863186016584 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3917661245282664 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3780634543104027 .\n",
      "Completed iter 200 / 400 . Training loss:  3.356232411378943 .\n",
      "Completed iter 300 / 400 . Training loss:  3.310926764108369 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5495007706340234 .\n",
      "Completed iter 100 / 400 . Training loss:  3.5024584306613193 .\n",
      "Completed iter 200 / 400 . Training loss:  3.476437221179277 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4393607301331146 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.697762294252466 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6558349130584986 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6381013819904595 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5873438836567995 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.291806170227801 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2814133756467143 .\n",
      "Completed iter 200 / 400 . Training loss:  2.251970377152692 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2645443297318657 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.458978345733172 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4486513420155505 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4489279872976693 .\n",
      "Completed iter 300 / 400 . Training loss:  2.423245331965463 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.61480484219563 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5921116133478357 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5881737701201364 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5745639219857184 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7893805262917657 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7730762602306043 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7407461525245354 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7204946860684447 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9123317396515094 .\n",
      "Completed iter 100 / 400 . Training loss:  2.915641143290205 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9008679159770274 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 300 / 400 . Training loss:  2.8739986496117016 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.084043410464351 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0693800018061936 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0431927728636325 .\n",
      "Completed iter 300 / 400 . Training loss:  3.024916560759053 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.254087866081483 .\n",
      "Completed iter 100 / 400 . Training loss:  3.215459079435987 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2057764584863553 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1891271360494797 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3621611445144124 .\n",
      "Completed iter 100 / 400 . Training loss:  3.336031976784112 .\n",
      "Completed iter 200 / 400 . Training loss:  3.312313918092591 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2654460330846566 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5894275683297674 .\n",
      "Completed iter 100 / 400 . Training loss:  3.5501826383225996 .\n",
      "Completed iter 200 / 400 . Training loss:  3.5195110516894252 .\n",
      "Completed iter 300 / 400 . Training loss:  3.468150748745656 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.68549345967336 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6496856094486825 .\n",
      "Completed iter 200 / 400 . Training loss:  3.616601275231802 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5765577997057356 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3093367479632607 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2942753082892624 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4719314331049103 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4535220803606546 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6316392369634065 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6198218548754886 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.753938983089826 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7360489530302305 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.94433290299554 .\n",
      "Completed iter 100 / 200 . Training loss:  2.915614399226622 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.073189388957962 .\n",
      "Completed iter 100 / 200 . Training loss:  3.073856487809181 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2053822089871145 .\n",
      "Completed iter 100 / 200 . Training loss:  3.20375660837874 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.383879962220634 .\n",
      "Completed iter 100 / 200 . Training loss:  3.360548874729582 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.541876375654526 .\n",
      "Completed iter 100 / 200 . Training loss:  3.509482024849731 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.691527334144541 .\n",
      "Completed iter 100 / 200 . Training loss:  3.667416263795906 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.306893586779868 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2927855182414256 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.478186430991441 .\n",
      "Completed iter 100 / 200 . Training loss:  2.471871517975326 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.612940555154711 .\n",
      "Completed iter 100 / 200 . Training loss:  2.609277988077962 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.767950849836598 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7685891293682916 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.946312670239051 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9198190402229818 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0993669793448104 .\n",
      "Completed iter 100 / 200 . Training loss:  3.070079831427511 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2602316575204435 .\n",
      "Completed iter 100 / 200 . Training loss:  3.251616433551773 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3981646900269453 .\n",
      "Completed iter 100 / 200 . Training loss:  3.373840523293146 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.546521487365194 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5314454049715844 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.675238663064279 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6310390385447264 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3369176720642026 .\n",
      "Completed iter 100 / 200 . Training loss:  2.313953165259571 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4707716525700683 .\n",
      "Completed iter 100 / 200 . Training loss:  2.459149000157595 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6245751750615818 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6080461517870583 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7664161903840485 .\n",
      "Completed iter 100 / 200 . Training loss:  2.751191448038825 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9357113355370084 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9105558112279786 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0972090474590384 .\n",
      "Completed iter 100 / 200 . Training loss:  3.06142058551652 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.1911368013845154 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1698111574479118 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.382080688031739 .\n",
      "Completed iter 100 / 200 . Training loss:  3.360461391095194 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.546175113124816 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5239142822300904 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.683844385476314 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6444130329341156 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.2936145132028227 .\n",
      "Completed iter 100 / 200 . Training loss:  2.294262407131384 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4721359313971636 .\n",
      "Completed iter 100 / 200 . Training loss:  2.459236140845117 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6162281365192497 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5922341976325995 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.789268214168082 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7672858284963557 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9016569210385974 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8955497293555807 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.082169368750158 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0482651009613733 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.222516863804966 .\n",
      "Completed iter 100 / 200 . Training loss:  3.198549209236691 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3790467817384062 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3545131102383188 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5565081883050405 .\n",
      "Completed iter 100 / 200 . Training loss:  3.510660643350132 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.699472006719488 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6511644518452564 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.2804645293006662 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2793293426129355 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4536777052578564 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4525619123822677 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.641621771948738 .\n",
      "Completed iter 100 / 200 . Training loss:  2.618581096752215 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.755811720592282 .\n",
      "Completed iter 100 / 200 . Training loss:  2.725921784384882 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9328490033291024 .\n",
      "Completed iter 100 / 200 . Training loss:  2.922303972251033 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.073736052928468 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0612358294016904 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2364964426867546 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2116746644395104 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.385253045870892 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 200 . Training loss:  3.3481411250287563 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.535902683117635 .\n",
      "Completed iter 100 / 200 . Training loss:  3.505336148076963 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.68017757613458 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6259879903500583 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.324729781577366 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.3081270231257354 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.2867698078609457 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.230130312697173 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.272289819390529 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.220540518276305 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2125893208291147 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.1731365482640235 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.188546925294756 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.1712033491228047 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.187481380057167 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.1690069192176904 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.19692697910537 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.138227837829394 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.057411278567945 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.1722051615398907 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.0990538387512325 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.166482946167655 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.073082140671651 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.121567328796945 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4722485107609558 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4333197872532533 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4337173323818995 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.4296797491257456 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4460740620740586 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.4043912610197316 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.370941351604265 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.3414169416241104 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.3483844939760887 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.330743734227842 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3610185410172364 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.3916513708680744 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.3073665349998405 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2917631507996874 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.264420921342658 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.302046052679772 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.2900110681189174 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.284039860249902 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.1770861572009714 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3054388408056106 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.637140128035179 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6182038229257443 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.606730531557849 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.577182636663613 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5851279167629073 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.562084217351082 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.5472063085132897 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.572365669536262 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.520724566485262 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.499032697542383 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4717744498946472 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4616569903550545 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.4823819242189558 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.470735327151313 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.409453716499581 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.4296363466870154 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.422245547646011 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.4278450633710666 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.3908468830177796 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.4125369315122547 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7636448841879493 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7606921468627634 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.7520684186063282 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.727306824892135 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.7311803271053336 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7068958284567834 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.703022790897855 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.656034333844675 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6556756227380065 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.617941272811276 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.6484825702131864 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.5959093936808815 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.617031893681747 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.6433180180091957 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5955521035987363 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.60471115955649 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.518374124194804 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.460456435316458 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.533791757649565 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5506131526489773 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.916059924132737 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.892309618537052 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.871687477959079 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.861260926124843 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.8541399526154265 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8031254341144187 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.8361794017525295 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.794522497664873 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.7560818234553874 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.7612307447605744 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.78805940392336 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.740630468563891 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.6855057032511813 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.712356680426022 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.656948518862097 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6943709194274663 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.743980849855416 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.696988828917887 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.6462724855661905 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.619690184485213 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.0642528409020446 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.0584240468522403 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0209258198864224 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0416135613787896 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.974046046812319 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.96089437778424 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0177578600644335 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9308710638507325 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.928146019407346 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.9001983983918502 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.8474076158764796 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9093184505765266 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.874761531552523 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8426602688694915 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.774650127471581 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.825961622330108 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.80392085499301 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8124496912325156 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.819523150667899 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.7138788464999273 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.1752253233163326 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.189719734744947 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 200 / 2000 . Training loss:  3.1419088496186793 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.145696447705772 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.100324351806573 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.0812121280573743 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.09118962458938 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.051635041403706 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.0309024133513436 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.005150368108132 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9574180709927846 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0098518284717115 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9226629073483497 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.920710605689244 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.9148672447866333 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.87883182400527 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.814155828690016 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.889315985753378 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.911349680302572 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8788849966625674 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.3421602789031777 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3708363525559015 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3169304052693054 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.302981070650458 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.27611747331342 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.244845780486308 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.205185255673396 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1951772735173534 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1699811676024727 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1300354921327935 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.159237680375316 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.118108203244854 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.0878577065323283 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0693129678825253 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0552048130430407 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.102488674800633 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.035808618178325 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0009323041358673 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9031717636532286 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8980546421553797 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.47484205269673 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.4786942539293597 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.4421549619158496 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4502897896878 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.3739239730305144 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.367507218595496 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.294423261217336 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3297946916841337 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.291936783133786 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.298016705293012 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.2888532534591253 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.162284710901607 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.2370753446574927 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1289979948714675 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.1793310329008255 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.084754123830387 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.068137519380891 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.025107967526398 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0156370641219867 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.988163731566239 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.6908850797572597 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6798374241228156 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.652252368989287 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.603668300643583 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.552651608171679 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.5032721851620003 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.488072980458503 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4667652924768677 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3704198936430934 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.366627131149828 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.3216507858981172 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.323498944985636 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.3067552583710373 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.226787242434114 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.2423770833237078 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.2428565590232967 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.167340818577893 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.2139947297816374 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.1656854771238776 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.1331338584340296 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.3002642300986262 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.291624216073718 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.291382213351214 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.2807799149655357 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2478300770475697 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2352920798958924 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.2361912545122906 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.246761971730331 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.2243896393214757 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.19447850803163 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.497646428993175 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4719815323895773 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4354967595593857 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.4478641450716085 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.418882576087782 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.410610929477203 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.3885071382185132 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.395204857148736 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.388594982883668 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.3635514481980984 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.5986416211665024 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.604252625006377 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.565622919056792 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.561825299066198 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5335733885429654 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.525076663046674 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.541511339936673 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.5166047993520246 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.5163432152847554 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.4801875449513244 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7882259028404723 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7634085919982363 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7342465533347595 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7324320773248023 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.7421346339612778 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.68933187917342 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.6672323048034636 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.6826484406155457 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.632250050581939 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6207834474018092 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9274556208810694 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.894488589529913 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8795949841903092 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.878803397894852 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.8613775109785644 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8175524647948644 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.800996609642453 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.786429382578478 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 800 / 1000 . Training loss:  2.772840375655581 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.7935752736925887 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0710985546024294 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0520817034215493 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.005675031935143 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0145479204939134 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.9730967648085938 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9406707915379147 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9670401290781516 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.987908106937859 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.922891743948462 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.8994740298916666 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.2194291544256286 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.214328059846753 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1564530764855885 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1759356787387456 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.153137595757932 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.099051028349835 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.067902922317278 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.0742138706771236 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.0404485121704505 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.032556862809022 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.380991095970362 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.3422628062003104 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.3417248483640947 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.311967553190037 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.27488350973394 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.242828940770095 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.21099506122615 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.2022639514723004 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.173190537876883 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.1908094662656232 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.540311899479196 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.5002912203616736 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.4586733246065506 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.454087716006197 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.407866017856802 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.394561274339489 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.352201179953061 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.334629341955269 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2882686081795347 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.27433165073657 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6932186606145625 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6358820727586183 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.6281307109017 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5680315044111346 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.527744767799748 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.5030051389181702 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.4652688273761236 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.445500043945721 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.379617727608452 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.3798300364370286 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3165128058966307 .\n",
      "Completed iter 100 / 600 . Training loss:  2.2996626271462888 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2818619271882694 .\n",
      "Completed iter 300 / 600 . Training loss:  2.2490293659463365 .\n",
      "Completed iter 400 / 600 . Training loss:  2.228946349605805 .\n",
      "Completed iter 500 / 600 . Training loss:  2.2488868714966337 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.498294074442884 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4588418657907627 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4651699778645937 .\n",
      "Completed iter 300 / 600 . Training loss:  2.426201792360493 .\n",
      "Completed iter 400 / 600 . Training loss:  2.4220225254984746 .\n",
      "Completed iter 500 / 600 . Training loss:  2.4207873943371565 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.63322920817912 .\n",
      "Completed iter 100 / 600 . Training loss:  2.6224712758561655 .\n",
      "Completed iter 200 / 600 . Training loss:  2.602832017750548 .\n",
      "Completed iter 300 / 600 . Training loss:  2.59379662431879 .\n",
      "Completed iter 400 / 600 . Training loss:  2.566836918402272 .\n",
      "Completed iter 500 / 600 . Training loss:  2.548564211413464 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.780052150211798 .\n",
      "Completed iter 100 / 600 . Training loss:  2.745267542078099 .\n",
      "Completed iter 200 / 600 . Training loss:  2.7182127699783516 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7220121201944236 .\n",
      "Completed iter 400 / 600 . Training loss:  2.704578414879774 .\n",
      "Completed iter 500 / 600 . Training loss:  2.6904382866620145 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.9159404683813404 .\n",
      "Completed iter 100 / 600 . Training loss:  2.921461184783931 .\n",
      "Completed iter 200 / 600 . Training loss:  2.884038840668633 .\n",
      "Completed iter 300 / 600 . Training loss:  2.9008435364024345 .\n",
      "Completed iter 400 / 600 . Training loss:  2.855534274980862 .\n",
      "Completed iter 500 / 600 . Training loss:  2.8334937944352236 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.065808166392424 .\n",
      "Completed iter 100 / 600 . Training loss:  3.064604363926108 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0108972187968974 .\n",
      "Completed iter 300 / 600 . Training loss:  3.0236601136681394 .\n",
      "Completed iter 400 / 600 . Training loss:  3.0081630409312585 .\n",
      "Completed iter 500 / 600 . Training loss:  2.975196017386105 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2130583663072025 .\n",
      "Completed iter 100 / 600 . Training loss:  3.1641176728883385 .\n",
      "Completed iter 200 / 600 . Training loss:  3.1692586018328432 .\n",
      "Completed iter 300 / 600 . Training loss:  3.131305412313166 .\n",
      "Completed iter 400 / 600 . Training loss:  3.09973413054799 .\n",
      "Completed iter 500 / 600 . Training loss:  3.0941982441771 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.38390165343034 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3424933285354776 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3116312355786057 .\n",
      "Completed iter 300 / 600 . Training loss:  3.2610043468348144 .\n",
      "Completed iter 400 / 600 . Training loss:  3.251423600926121 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2161555184588586 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.531029316653973 .\n",
      "Completed iter 100 / 600 . Training loss:  3.4985885918539634 .\n",
      "Completed iter 200 / 600 . Training loss:  3.46695133061411 .\n",
      "Completed iter 300 / 600 . Training loss:  3.425210630023998 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4065180456888298 .\n",
      "Completed iter 500 / 600 . Training loss:  3.378551509011128 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.738294105608774 .\n",
      "Completed iter 100 / 600 . Training loss:  3.6710011901656068 .\n",
      "Completed iter 200 / 600 . Training loss:  3.6492528041794277 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5987829861204266 .\n",
      "Completed iter 400 / 600 . Training loss:  3.574071826568228 .\n",
      "Completed iter 500 / 600 . Training loss:  3.520755312406843 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3052651411609526 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3004576533390275 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2646717536287184 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2685115521951293 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4233212545275786 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4418719822630433 .\n",
      "Completed iter 200 / 400 . Training loss:  2.432862447559202 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4119413803481202 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.614856762309377 .\n",
      "Completed iter 100 / 400 . Training loss:  2.626725534290372 .\n",
      "Completed iter 200 / 400 . Training loss:  2.581752951669942 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5755065106010875 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7578202303562294 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7540879336286475 .\n",
      "Completed iter 200 / 400 . Training loss:  2.750312636898309 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7169117988594045 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.8977452856264154 .\n",
      "Completed iter 100 / 400 . Training loss:  2.89505332340138 .\n",
      "Completed iter 200 / 400 . Training loss:  2.885273506834757 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8438865297369933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0863244110977233 .\n",
      "Completed iter 100 / 400 . Training loss:  3.058384697032485 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0354965571127233 .\n",
      "Completed iter 300 / 400 . Training loss:  3.004090297749813 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.211820491547673 .\n",
      "Completed iter 100 / 400 . Training loss:  3.1892712651082697 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1772641093291734 .\n",
      "Completed iter 300 / 400 . Training loss:  3.159078954376138 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.351774205245441 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3217492355205978 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2812279106791236 .\n",
      "Completed iter 300 / 400 . Training loss:  3.245245333777861 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5597034416209494 .\n",
      "Completed iter 100 / 400 . Training loss:  3.531875679387303 .\n",
      "Completed iter 200 / 400 . Training loss:  3.482741882196693 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4649536081026406 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.7156980113194966 .\n",
      "Completed iter 100 / 400 . Training loss:  3.675421546749216 .\n",
      "Completed iter 200 / 400 . Training loss:  3.638888205636075 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5911371065638393 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.2989728968778027 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2870332186719877 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2735633139296887 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2632278087362345 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4839260810704835 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4434185830133353 .\n",
      "Completed iter 200 / 400 . Training loss:  2.430286465878153 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4326411494865487 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6729495892224353 .\n",
      "Completed iter 100 / 400 . Training loss:  2.6380245242887637 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6164485721833013 .\n",
      "Completed iter 300 / 400 . Training loss:  2.614194258361019 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7474637045072554 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7438963934723155 .\n",
      "Completed iter 200 / 400 . Training loss:  2.695670708804594 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7071221503370437 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.94191021299887 .\n",
      "Completed iter 100 / 400 . Training loss:  2.928560268840943 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9115796051819474 .\n",
      "Completed iter 300 / 400 . Training loss:  2.854015145171533 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.071904870275385 .\n",
      "Completed iter 100 / 400 . Training loss:  3.053549360560105 .\n",
      "Completed iter 200 / 400 . Training loss:  3.045965796071919 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0292553482004356 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.271075651799402 .\n",
      "Completed iter 100 / 400 . Training loss:  3.232814701405795 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2068407238480723 .\n",
      "Completed iter 300 / 400 . Training loss:  3.187266037339869 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.392691932411507 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3590460969616514 .\n",
      "Completed iter 200 / 400 . Training loss:  3.324472634194705 .\n",
      "Completed iter 300 / 400 . Training loss:  3.305723229618851 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.527852224404823 .\n",
      "Completed iter 100 / 400 . Training loss:  3.4956822986743483 .\n",
      "Completed iter 200 / 400 . Training loss:  3.469201185471862 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4219167051652932 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6958073970305247 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6547087632942628 .\n",
      "Completed iter 200 / 400 . Training loss:  3.624758880958778 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5736168498339844 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.283156622735964 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2697244106826164 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.457866578935288 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4307144290728036 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.613035908001115 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5926894801870755 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7793200354194254 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7746404178519 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.907963780148274 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8984440790861705 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.064760205180395 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0351968127722264 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2054292422139197 .\n",
      "Completed iter 100 / 200 . Training loss:  3.189376474576971 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.376467549088977 .\n",
      "Completed iter 100 / 200 . Training loss:  3.356329571065866 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5856560412475944 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5473765800761017 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.680714108153781 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6433069564165086 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3128698972708945 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3016198443423637 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4548162157476887 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4410819959522883 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6611181905514183 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6411596544340585 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7775191674375597 .\n",
      "Completed iter 100 / 200 . Training loss:  2.763385860588059 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9440264034421375 .\n",
      "Completed iter 100 / 200 . Training loss:  2.926668096433293 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.080763681389522 .\n",
      "Completed iter 100 / 200 . Training loss:  3.062807178974195 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2174567831165293 .\n",
      "Completed iter 100 / 200 . Training loss:  3.200277229061504 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3661381335278424 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3431771012035014 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.538287758499262 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5025714841031688 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6907552718872148 .\n",
      "Completed iter 100 / 200 . Training loss:  3.659946016783591 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3107902460843284 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2943232583971245 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4518108774936254 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4448513632787936 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.616467339455915 .\n",
      "Completed iter 100 / 200 . Training loss:  2.600317986924628 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.766525758482424 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7510220147868414 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.895851689662034 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8912807079442877 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0607755828539274 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0511285093158955 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.241081278351201 .\n",
      "Completed iter 100 / 200 . Training loss:  3.233813818622652 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.382541999625513 .\n",
      "Completed iter 100 / 200 . Training loss:  3.354904743336765 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.547977662830391 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5028944885777857 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.72691285166546 .\n",
      "Completed iter 100 / 200 . Training loss:  3.674585887370774 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.297053874426278 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2968394833979078 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4676405989522086 .\n",
      "Completed iter 100 / 200 . Training loss:  2.470674809945004 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6247141567175913 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6126996817641417 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7557229709588853 .\n",
      "Completed iter 100 / 200 . Training loss:  2.75580811355732 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.898944633525206 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8633970983688037 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.057700921880021 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0432650428186556 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2232902328787922 .\n",
      "Completed iter 100 / 200 . Training loss:  3.206418594666746 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3858345524099533 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3564913393421696 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5246900634177356 .\n",
      "Completed iter 100 / 200 . Training loss:  3.491623978352825 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.702012726258081 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6832918608222682 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.293187787118557 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2810870721930843 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4747523522794976 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4507777848421304 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.584406250084541 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5864695130326982 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.8161066086136355 .\n",
      "Completed iter 100 / 200 . Training loss:  2.785958015063327 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9173915668996226 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8894631982934693 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.081532943115896 .\n",
      "Completed iter 100 / 200 . Training loss:  3.059533986083867 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.24706201095517 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2185463294510326 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3986859049540548 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3747788523421023 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5572948854406556 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5275049358447585 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.706113736930905 .\n",
      "Completed iter 100 / 200 . Training loss:  3.650594428833659 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.327710336827832 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.2937441939241707 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.2817754358697995 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.277722811960096 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.238152121755492 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.242882362724595 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2054412180505363 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.21513906976711 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.2492528923100847 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.1974086839434075 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.193006528486704 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.19674960226036 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.1244717481907465 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.1447331933710396 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.146750925213185 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.139909892431967 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.0982970151647464 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.1278845906341206 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.084374365401097 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.0678240012623075 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.483821991034119 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4771627678435224 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4297846185258867 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.413610338113383 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.420185272928558 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.419541314129306 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.3874039809575223 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.3244322651452354 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.3907735455994574 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3644661907354934 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3155266765161295 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.3018699819191406 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.2388440930499325 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.3408319696008886 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.2983542313291747 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.287058267370946 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3038440194635563 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.263756049653363 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.1704468340454004 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.2590245540670955 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.612157698951723 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6275109962368735 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.5991170489701436 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.5653471549685065 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5962341959187176 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.5550001413044865 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.546686471551487 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.519940745844171 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.534624177788371 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.495798004372176 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4830402488412733 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.449640626477162 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.453107430632556 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.434742072191725 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.445479009700971 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.500769874135462 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3624838917723094 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.383309070346454 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.407126360172702 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.4061263607453536 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.754533279306256 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7554849043094602 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.7395440156395603 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.7222332737796435 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.716328522468683 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.6451799337800264 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.6574784455060034 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.6557276999931303 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6644224862198995 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.6497183945791534 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.6163984273135616 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1100 / 2000 . Training loss:  2.5995311322756667 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.5726437471378683 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.555909465135251 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.597097143224156 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.5858712960406773 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.529291545224287 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.548204602319526 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.5718302720590778 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5424148980207675 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.924007853469166 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.900785032332566 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.8722445859285033 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.835071884386622 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.7896989212840198 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.828145393956033 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.7943743693398693 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.7917510289564156 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.7909194195983096 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.756766623307004 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.7549447302235506 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.7994890822089813 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.695468930639713 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.7007899481682713 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.6563779047420715 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6713508657397185 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.6773973988170665 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.6632395814830296 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.653367552645748 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6482957942306347 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.101737357422216 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.0739403281936055 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0619934893365324 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.075347928789116 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0042179787353547 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9797410025828177 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.955853904274342 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9434661304913 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.9230801157559814 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.923001983967624 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.879036071493712 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9040981243685353 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.8755936653872163 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8896941301412884 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.8441537863595294 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.777886357062534 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.87889909751369 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8212447772799285 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.7104469693666084 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.712044658267515 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.2530016420905463 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.19826832714815 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.197288359522296 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.145827456681042 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0878742585839647 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1107718911219226 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0362562745641233 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.042567253916906 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.0196192247881806 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.996633006702748 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9869695126716564 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9810114256699 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9646342735797644 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9071756035148124 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.9544919041499993 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.918435787212605 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8735242391583635 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.8525454547122915 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.848727638608298 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.833725625123735 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.4085184610888404 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.373660514879523 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3436702155691362 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.308561416796285 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.2785328378546845 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.243090215821204 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.261895492932896 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.2033556560592786 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.138988286904298 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1740125497349676 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1300840132357464 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.1019098117154176 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.0487980077089283 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1162150657283147 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.019162054586592 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.0595297009509954 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.061902145301205 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.965898423347936 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.951962746161644 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.918435045479142 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.565886972802204 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.5526649937713026 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.5030727554845917 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4200683102861054 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.415117358127015 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4267717340606403 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.33240334672483 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3323091180500377 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.29092345521697 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.250603230548683 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.2398249576054177 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.2029470422741144 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.1550171733762316 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.111439574669837 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0892053621490025 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.073083561626418 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.070344774407191 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0812043021231554 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.033009891632708 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0008172469546595 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.668562771500971 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.673847832931078 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.621392240932792 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.569859405070053 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.5388190128951127 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4807597790863656 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.4416706712444136 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.4447490601302633 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3832284580231446 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.375306626815419 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.297919294826075 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.326206237222641 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.2718093454658455 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.2649507782876506 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.2576390550817362 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.207314734763154 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.155396222307159 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.1354088609884627 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1800 / 2000 . Training loss:  3.0743203791977773 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0832597192010365 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.309385445350161 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.2954320972102256 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.278701379829526 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.2540026178392165 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2376896450114834 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.241490298413562 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.215420636261135 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.2102035705906213 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.227469444266821 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.192887019529575 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.498791427087775 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4639954826944064 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.432586485767366 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.4322534946109737 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.4285496737871957 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.3764832727820613 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.4220682707804366 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.3643299276494387 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.3586834450347025 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.3621299684006365 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.638708489017645 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.5913463349819352 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.579987608338634 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.551630130588319 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5314661939795498 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.496910633317569 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.495478733239957 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.5007707937270305 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.475917055091214 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.472295356637791 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7581222357445263 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7345527491017 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7259213658445556 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.701949296419597 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.692638069893074 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.698336971212101 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.680029015823927 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.6903723285303216 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6585785083055717 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.616826423772636 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9581924342085553 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.9016920054160877 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8818414965094394 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.8446388228611603 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.822121564716713 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.814172379307144 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.789853509727834 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.7785299502664844 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.7763994318874534 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.771462701954063 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0296304943187615 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0388134702699072 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.983468440527836 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.989474600774444 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.9728990116225775 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9469231614816076 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9431525225154136 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.8961642703632133 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.8880485378196123 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.8630066634477522 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.21916963180231 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.196947357675616 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1837601073685513 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.154008718454568 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1275865105474683 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.0837503732988014 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.074435747182812 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.051805545228631 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.069456530827121 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.040118752574082 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.3734440577212292 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.337050248275342 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.28725396036522 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.282291623798262 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.233758359431686 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.225523486725793 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.18711218733473 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.160035227510078 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.1849445603648103 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.11831810056533 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.559675668914233 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.5205091156403743 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.490466759089526 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.442636538187313 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.409175624425113 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.3968386977998453 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.3566577822080763 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3053045397138217 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.280696431218052 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.2492515828440798 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6590926525852683 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.643214367666377 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.5774409800204996 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5499486926080106 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.5125677692658033 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.4756643154372906 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.4673957008197167 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4019464004331157 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.3994329022655125 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.3611594806133556 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3233095882099977 .\n",
      "Completed iter 100 / 600 . Training loss:  2.2882481330770976 .\n",
      "Completed iter 200 / 600 . Training loss:  2.2751067090259562 .\n",
      "Completed iter 300 / 600 . Training loss:  2.251334608086969 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2331139666953144 .\n",
      "Completed iter 500 / 600 . Training loss:  2.240788241067052 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.449167623696725 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4307471605919027 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4100492518377354 .\n",
      "Completed iter 300 / 600 . Training loss:  2.4035759766018727 .\n",
      "Completed iter 400 / 600 . Training loss:  2.390347868371468 .\n",
      "Completed iter 500 / 600 . Training loss:  2.3922207496637964 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.6118490295719345 .\n",
      "Completed iter 100 / 600 . Training loss:  2.5992335942353635 .\n",
      "Completed iter 200 / 600 . Training loss:  2.568436462613647 .\n",
      "Completed iter 300 / 600 . Training loss:  2.5729290950695836 .\n",
      "Completed iter 400 / 600 . Training loss:  2.5502984667571167 .\n",
      "Completed iter 500 / 600 . Training loss:  2.5388725950364432 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.783873811733359 .\n",
      "Completed iter 100 / 600 . Training loss:  2.742356295433622 .\n",
      "Completed iter 200 / 600 . Training loss:  2.7448976514670016 .\n",
      "Completed iter 300 / 600 . Training loss:  2.734953283272716 .\n",
      "Completed iter 400 / 600 . Training loss:  2.7058057273734337 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 500 / 600 . Training loss:  2.6852848156956077 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.9015355149057145 .\n",
      "Completed iter 100 / 600 . Training loss:  2.88140814605491 .\n",
      "Completed iter 200 / 600 . Training loss:  2.8623340118846903 .\n",
      "Completed iter 300 / 600 . Training loss:  2.8485152791467163 .\n",
      "Completed iter 400 / 600 . Training loss:  2.8176153074347328 .\n",
      "Completed iter 500 / 600 . Training loss:  2.816215712748584 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.0443716553631894 .\n",
      "Completed iter 100 / 600 . Training loss:  3.030216684638402 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0196448870194916 .\n",
      "Completed iter 300 / 600 . Training loss:  2.9870651343005705 .\n",
      "Completed iter 400 / 600 . Training loss:  2.993689842189452 .\n",
      "Completed iter 500 / 600 . Training loss:  2.950191161967254 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.219321204313027 .\n",
      "Completed iter 100 / 600 . Training loss:  3.210244308271464 .\n",
      "Completed iter 200 / 600 . Training loss:  3.1811104463985287 .\n",
      "Completed iter 300 / 600 . Training loss:  3.1297366062464658 .\n",
      "Completed iter 400 / 600 . Training loss:  3.110691330577824 .\n",
      "Completed iter 500 / 600 . Training loss:  3.0882445764404567 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3757580858350185 .\n",
      "Completed iter 100 / 600 . Training loss:  3.375812482816661 .\n",
      "Completed iter 200 / 600 . Training loss:  3.321983370611103 .\n",
      "Completed iter 300 / 600 . Training loss:  3.295288128478799 .\n",
      "Completed iter 400 / 600 . Training loss:  3.2448968330454435 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2516638967258302 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.5642965193463363 .\n",
      "Completed iter 100 / 600 . Training loss:  3.538835559020991 .\n",
      "Completed iter 200 / 600 . Training loss:  3.485678784062067 .\n",
      "Completed iter 300 / 600 . Training loss:  3.4561683608989027 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4277225650418286 .\n",
      "Completed iter 500 / 600 . Training loss:  3.3932967053693472 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.6954025929214525 .\n",
      "Completed iter 100 / 600 . Training loss:  3.664199673039273 .\n",
      "Completed iter 200 / 600 . Training loss:  3.6197146123715145 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5834090610224023 .\n",
      "Completed iter 400 / 600 . Training loss:  3.5363961918824813 .\n",
      "Completed iter 500 / 600 . Training loss:  3.5103122663883246 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3220269779542337 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3026703956849146 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2639612944701617 .\n",
      "Completed iter 300 / 400 . Training loss:  2.265764349648832 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4664337008567823 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4625910459599316 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4497939227985976 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4362091374569586 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6261242579969144 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5810657120500795 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5782444263870095 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5564626510719495 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7610954746029908 .\n",
      "Completed iter 100 / 400 . Training loss:  2.758737121729028 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7195374898665525 .\n",
      "Completed iter 300 / 400 . Training loss:  2.719951152403108 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9197790055044317 .\n",
      "Completed iter 100 / 400 . Training loss:  2.8736934019726794 .\n",
      "Completed iter 200 / 400 . Training loss:  2.9068816479047497 .\n",
      "Completed iter 300 / 400 . Training loss:  2.862535568652575 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.07365694348233 .\n",
      "Completed iter 100 / 400 . Training loss:  3.064049377918118 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0289968749600544 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9895959795476594 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2614217853557568 .\n",
      "Completed iter 100 / 400 . Training loss:  3.2140033308672686 .\n",
      "Completed iter 200 / 400 . Training loss:  3.187550366483843 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1487612058721486 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3824135372392394 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3382556270498416 .\n",
      "Completed iter 200 / 400 . Training loss:  3.31755851227571 .\n",
      "Completed iter 300 / 400 . Training loss:  3.279619340531542 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.521387387763756 .\n",
      "Completed iter 100 / 400 . Training loss:  3.467427670107437 .\n",
      "Completed iter 200 / 400 . Training loss:  3.453107773882639 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4099462909402516 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6661204124364435 .\n",
      "Completed iter 100 / 400 . Training loss:  3.628553207612374 .\n",
      "Completed iter 200 / 400 . Training loss:  3.57666418557974 .\n",
      "Completed iter 300 / 400 . Training loss:  3.542764627373338 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3120214581819547 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2859156389371744 .\n",
      "Completed iter 200 / 400 . Training loss:  2.270380747180236 .\n",
      "Completed iter 300 / 400 . Training loss:  2.238248585645371 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.481013374482854 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4594234310770084 .\n",
      "Completed iter 200 / 400 . Training loss:  2.424071993453474 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4259035185123596 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6222275605087213 .\n",
      "Completed iter 100 / 400 . Training loss:  2.622554814442006 .\n",
      "Completed iter 200 / 400 . Training loss:  2.585616358191759 .\n",
      "Completed iter 300 / 400 . Training loss:  2.577989330493762 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.776098040968243 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7522722434612232 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7419396776617835 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7060710583613687 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9209814796129887 .\n",
      "Completed iter 100 / 400 . Training loss:  2.8855177699966363 .\n",
      "Completed iter 200 / 400 . Training loss:  2.871710352529745 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8642836662239484 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0774159313014557 .\n",
      "Completed iter 100 / 400 . Training loss:  3.044724674167451 .\n",
      "Completed iter 200 / 400 . Training loss:  3.036703295985528 .\n",
      "Completed iter 300 / 400 . Training loss:  2.984514257093582 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2309645750074845 .\n",
      "Completed iter 100 / 400 . Training loss:  3.2059127471858706 .\n",
      "Completed iter 200 / 400 . Training loss:  3.173154906663666 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1382802028618277 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.414173276413616 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3800756479338108 .\n",
      "Completed iter 200 / 400 . Training loss:  3.341274134998484 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3094698625876635 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5206325894642814 .\n",
      "Completed iter 100 / 400 . Training loss:  3.483235841457633 .\n",
      "Completed iter 200 / 400 . Training loss:  3.462355774501789 .\n",
      "Completed iter 300 / 400 . Training loss:  3.415216459741223 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6992214693463827 .\n",
      "Completed iter 100 / 400 . Training loss:  3.649161312626094 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6250624332540546 .\n",
      "Completed iter 300 / 400 . Training loss:  3.566493538095643 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3171651349434494 .\n",
      "Completed iter 100 / 200 . Training loss:  2.304312770524822 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4537860001047855 .\n",
      "Completed iter 100 / 200 . Training loss:  2.428285830977125 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6039567209962646 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 200 . Training loss:  2.5824196254021397 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7708692142119964 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7458393060490653 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.916330193963478 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8823843956261856 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0454204452402607 .\n",
      "Completed iter 100 / 200 . Training loss:  3.024453307101172 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.207410749087139 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1954677807204828 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3628767780524083 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3333084814487712 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5434661093367543 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5055616950986215 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7198777697888534 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6818689069810464 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.308009320873322 .\n",
      "Completed iter 100 / 200 . Training loss:  2.281009932459839 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4568674395225525 .\n",
      "Completed iter 100 / 200 . Training loss:  2.438497153510992 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.644896516957184 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6211651329821026 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.775469093292048 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7526927162039048 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9067286496441147 .\n",
      "Completed iter 100 / 200 . Training loss:  2.866958422059213 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0890464155399346 .\n",
      "Completed iter 100 / 200 . Training loss:  3.069893715178313 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2273378521942924 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2093877533549175 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.369463533636819 .\n",
      "Completed iter 100 / 200 . Training loss:  3.325900270966955 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5316888077154793 .\n",
      "Completed iter 100 / 200 . Training loss:  3.503115554653302 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.688896264287613 .\n",
      "Completed iter 100 / 200 . Training loss:  3.658093236433012 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.308206534288918 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2941116123043686 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4523401228913073 .\n",
      "Completed iter 100 / 200 . Training loss:  2.439335989543025 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.635475050256162 .\n",
      "Completed iter 100 / 200 . Training loss:  2.620219755765831 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.756070289881389 .\n",
      "Completed iter 100 / 200 . Training loss:  2.738434059313498 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.915869741077003 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8985919965476143 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.091353294595689 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0546348865748945 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2118775103730974 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1765367687768173 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3970838555530767 .\n",
      "Completed iter 100 / 200 . Training loss:  3.356926728686969 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5600067235024873 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5160962794227397 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.686895409000888 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6425185312805306 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3222821446068025 .\n",
      "Completed iter 100 / 200 . Training loss:  2.306223914003918 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4670770100695325 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4575834236698006 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6199714627598834 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5960550812240197 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7739151313718873 .\n",
      "Completed iter 100 / 200 . Training loss:  2.751445624014601 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9377209618548754 .\n",
      "Completed iter 100 / 200 . Training loss:  2.900693839460144 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.06369868901903 .\n",
      "Completed iter 100 / 200 . Training loss:  3.047998921337988 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2339463532084585 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2059887428918965 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.377216743669731 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3427931637427912 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.532242698601606 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5058460010119736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.714726337069533 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6538382232995215 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3003061663256776 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2850660830459946 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4730493995634486 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4490357916482792 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6151880262339438 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5872381215601545 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.77939674735959 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7488812089325485 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.934758550493666 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9009059115425164 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0851236772237836 .\n",
      "Completed iter 100 / 200 . Training loss:  3.057686316860643 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2415552370831495 .\n",
      "Completed iter 100 / 200 . Training loss:  3.209610636878101 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.362878064597702 .\n",
      "Completed iter 100 / 200 . Training loss:  3.326094913624207 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5391607671403156 .\n",
      "Completed iter 100 / 200 . Training loss:  3.4839165954666687 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6814383190957747 .\n",
      "Completed iter 100 / 200 . Training loss:  3.644703326741098 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.322203242559653 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.3044030749758457 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.2536700063265105 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.2465391607372514 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.2233601155800313 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.249843128347587 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2089403251049617 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.2756669237310674 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.210177676305231 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.216158432470311 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.1527967940797454 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.144531554611284 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.1732625086431216 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.090917594222989 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.0976952613676874 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.125400923853347 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.1229320986747098 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.0960427176899508 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.159245758977984 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.0964047040439238 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.428571770579858 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 2000 . Training loss:  2.446231740181525 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.4176346867496195 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.450063999085669 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.4094017431776935 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.3759383248046455 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.364610107553446 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.3501471804123697 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.280244203910322 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3784848416561752 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.274158297598535 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.248702491856341 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.307834485689348 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2806648997165677 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.268910087955567 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.2516511677205098 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.315206707656826 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.261985470076313 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.18788711466375 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.2407522167114644 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.633970189071182 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.607707766625559 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.6351670615818104 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.563470930252646 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.569322989569297 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.511430504010138 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.5398326392437363 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.5472056701350088 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.470569506174447 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.5122037415785003 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.475101631922541 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.515996155873692 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.4454677446523174 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.4347904545946384 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.388596745988962 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.4218950849521175 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.322493708581688 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.401872437741439 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.352904443655717 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3995033687429763 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7616816261525337 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.7459419671655625 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.734362985331807 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.6996348630528386 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.694352534376043 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.7218834122535673 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.6732216243087894 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.6509315569011536 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.6624823940174793 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.635429106906679 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.6090084277624195 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.583633114528342 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.5841540836958785 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.565312265252516 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5576958494038022 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.5001887825385025 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.507504561100414 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.568146647112582 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.5293272714100556 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5416118684852034 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.9074864749204288 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.8811514939027294 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.8988940907080303 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.888241621746351 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.850840949238118 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.795102412629712 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.7925172224034456 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.7708091019425023 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.7253922407035702 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.7786054865005174 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.776111195353656 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.6452530270396273 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.672642265427104 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.682953565418287 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.675369609385947 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6810272301979885 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.661195702314407 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.6056059611255895 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.629401572356104 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.5889138705365324 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.0872200794837745 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.080932609217281 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.0185536790583516 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.001909779146021 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.9891964484629128 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9885979830829337 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.9476965651060967 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.9435627391188155 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.891635757473752 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.8548309292895224 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.896971182348105 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9151788047263403 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.801932188026757 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.8625345270621594 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.795353120223909 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.8374026829420522 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.805229813093552 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.7533271387924287 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.760363471852089 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.75252631736094 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.186794453314732 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.1693868856117096 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.1636492707413604 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.1394518102340654 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0599722269219516 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.0912791377838857 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.062528305378211 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.0207661092606672 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.9920565170580042 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.9720849708646684 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9370949434446167 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.969597380730051 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.951011070969379 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9256140136872464 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.875961584833131 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.835598693005266 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8544174497097288 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9261978830883013 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.90190220547142 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.806206176492049 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.3951301190421685 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.393217220006913 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3481879999506967 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.328117325962296 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.290051175126693 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.2486514409295646 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.2257698258374123 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1516204198743742 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 800 / 2000 . Training loss:  3.141135659601739 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.157285783805839 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.0715384287925778 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.1056264525366393 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.030746837904452 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9959960015330394 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0133450586188917 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.94522865862131 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.995295712336743 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9324612047774075 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0169372305787796 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8806886847525637 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.549480697744969 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.5271488781732394 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.459547666008974 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4457859263070323 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4124594669314003 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.347500601867848 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.3049119286108306 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3144769192073094 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.212045709406775 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.2103817416300178 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1605680768183237 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.169746767090114 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.1993987719689314 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1263614254635135 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.0442167506527613 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.9478249509913486 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.059393063182195 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0106765398114197 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.0497553588998407 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9489228621646166 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.7171426971048818 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.636208557419951 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.600378287556045 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.5413845702221427 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.5062709867152355 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4715075346276665 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.443628549169355 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.415139999652742 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.337385095594173 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.3021371128042203 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.335894877924989 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.2382980760607527 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.2233688859899634 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.17188640978856 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.153337056748031 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.1186020228076528 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.135119677641191 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0343222353391424 .\n",
      "Completed iter 1800 / 2000 . Training loss:  3.079002053029072 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9913124639230113 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.3081993260959126 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.297937539955329 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.2494608625352526 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.2470554623296874 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.251009442413577 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2103743731719234 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.2070201938722205 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.185737845095807 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.2057794783641014 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.16278023167427 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.464628641168085 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4525851946274533 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4381408906964834 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.4328989703220865 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.375009132339622 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.3931455744311747 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.3612930355814292 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.3506089901534826 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.334771492287835 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.3524482005925695 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.5813436070043565 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.570517596685091 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.589480194060525 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.5559842223895464 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.530993062955133 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.514682104380347 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.5131033500194926 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.4809242878306654 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.5122904461535027 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.4728629249237217 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7667492884976705 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7515328489300996 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.737407408054092 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7148482599578756 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.6804174750552257 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.699170855413683 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.665740824206348 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.650651711958337 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6318841331284575 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.62270157080504 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9471751011242255 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.901066579325417 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8753111527331394 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.8498294812676876 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.858368361468686 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.8539778202609467 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.7874979537583666 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.7859160674692856 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.801228911493019 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.7114427402646966 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.131460985784404 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0535472507657815 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.0639175272479977 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.0358891866965365 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.996479895235507 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.989580403455082 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9845708398961146 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.957570036949873 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.9435547329097047 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9136722236914485 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.2243229966859515 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.205334065065661 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1998425095770697 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1473153765376063 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.119763137358761 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.093247644919079 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.087593924538651 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.050601142438325 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.0418753803469767 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.0159626068255436 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.3699953527644015 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.362579325039982 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.3024035314236633 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 300 / 1000 . Training loss:  3.2928929220040706 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.2540372237757134 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.227276742901025 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.1817795187035256 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.145170030914815 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.142188300550206 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.1098470620906165 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.542814135672452 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.498178683796925 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.4639397203038946 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.421226657423081 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.400782226162439 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.373516145846844 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.325314596158444 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3228624693422386 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.252695758560164 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.215034902087986 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6791446560845853 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6503522638385584 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.594916464900459 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.526915821369638 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.480796461845604 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.468482479935224 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.438567386153945 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.4058139727119032 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.3603565428570725 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.311619899971062 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3244974407592904 .\n",
      "Completed iter 100 / 600 . Training loss:  2.2907942560611687 .\n",
      "Completed iter 200 / 600 . Training loss:  2.283444029335787 .\n",
      "Completed iter 300 / 600 . Training loss:  2.2594906539092876 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2386098430421493 .\n",
      "Completed iter 500 / 600 . Training loss:  2.2485062207347295 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.43021334178825 .\n",
      "Completed iter 100 / 600 . Training loss:  2.42959876357528 .\n",
      "Completed iter 200 / 600 . Training loss:  2.414188205748725 .\n",
      "Completed iter 300 / 600 . Training loss:  2.391953037189431 .\n",
      "Completed iter 400 / 600 . Training loss:  2.3658008688491163 .\n",
      "Completed iter 500 / 600 . Training loss:  2.357714796508366 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.5896877384692036 .\n",
      "Completed iter 100 / 600 . Training loss:  2.5863187542581594 .\n",
      "Completed iter 200 / 600 . Training loss:  2.5598976519606413 .\n",
      "Completed iter 300 / 600 . Training loss:  2.554311196870449 .\n",
      "Completed iter 400 / 600 . Training loss:  2.5444125777732602 .\n",
      "Completed iter 500 / 600 . Training loss:  2.554525420436678 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.7885954842232423 .\n",
      "Completed iter 100 / 600 . Training loss:  2.770560082838799 .\n",
      "Completed iter 200 / 600 . Training loss:  2.7412354184801564 .\n",
      "Completed iter 300 / 600 . Training loss:  2.718601224241989 .\n",
      "Completed iter 400 / 600 . Training loss:  2.697163889770943 .\n",
      "Completed iter 500 / 600 . Training loss:  2.7074213777355904 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.9088423405275172 .\n",
      "Completed iter 100 / 600 . Training loss:  2.893383626976594 .\n",
      "Completed iter 200 / 600 . Training loss:  2.861479177726236 .\n",
      "Completed iter 300 / 600 . Training loss:  2.86863043478976 .\n",
      "Completed iter 400 / 600 . Training loss:  2.853457497870085 .\n",
      "Completed iter 500 / 600 . Training loss:  2.801041136706441 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.0990689349798437 .\n",
      "Completed iter 100 / 600 . Training loss:  3.0728992821156407 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0457393406055564 .\n",
      "Completed iter 300 / 600 . Training loss:  3.029090998013594 .\n",
      "Completed iter 400 / 600 . Training loss:  2.9850411641690395 .\n",
      "Completed iter 500 / 600 . Training loss:  2.968106796630811 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2178413481743453 .\n",
      "Completed iter 100 / 600 . Training loss:  3.219185458063829 .\n",
      "Completed iter 200 / 600 . Training loss:  3.153068770031922 .\n",
      "Completed iter 300 / 600 . Training loss:  3.148748517505574 .\n",
      "Completed iter 400 / 600 . Training loss:  3.1248094618144293 .\n",
      "Completed iter 500 / 600 . Training loss:  3.1004841053782184 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.39778262006342 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3742220779631826 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3521439696210638 .\n",
      "Completed iter 300 / 600 . Training loss:  3.313910637101192 .\n",
      "Completed iter 400 / 600 . Training loss:  3.267809060690907 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2382737504338586 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.5276398884584412 .\n",
      "Completed iter 100 / 600 . Training loss:  3.487134790494734 .\n",
      "Completed iter 200 / 600 . Training loss:  3.4551464668527156 .\n",
      "Completed iter 300 / 600 . Training loss:  3.4081318277856765 .\n",
      "Completed iter 400 / 600 . Training loss:  3.3765873751752187 .\n",
      "Completed iter 500 / 600 . Training loss:  3.358666451800213 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.6682251567123885 .\n",
      "Completed iter 100 / 600 . Training loss:  3.627631223563262 .\n",
      "Completed iter 200 / 600 . Training loss:  3.573305850670554 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5501322615607966 .\n",
      "Completed iter 400 / 600 . Training loss:  3.5046208459413113 .\n",
      "Completed iter 500 / 600 . Training loss:  3.469341703946793 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.300747708784269 .\n",
      "Completed iter 100 / 400 . Training loss:  2.277682367003257 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2890874166441217 .\n",
      "Completed iter 300 / 400 . Training loss:  2.262736195203616 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.46224788815627 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4230355199078284 .\n",
      "Completed iter 200 / 400 . Training loss:  2.410434837444171 .\n",
      "Completed iter 300 / 400 . Training loss:  2.401704079514613 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6306747403803077 .\n",
      "Completed iter 100 / 400 . Training loss:  2.598690096748231 .\n",
      "Completed iter 200 / 400 . Training loss:  2.567182626962924 .\n",
      "Completed iter 300 / 400 . Training loss:  2.566051461960239 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.785875142269875 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7756125637866833 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7469968498265036 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7208235598883017 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9280114365019934 .\n",
      "Completed iter 100 / 400 . Training loss:  2.8871219677407476 .\n",
      "Completed iter 200 / 400 . Training loss:  2.8904678235159182 .\n",
      "Completed iter 300 / 400 . Training loss:  2.852331511914855 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0756158786697423 .\n",
      "Completed iter 100 / 400 . Training loss:  3.052410973924194 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0395859226423037 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9816592522819887 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2238059765751927 .\n",
      "Completed iter 100 / 400 . Training loss:  3.1811360204859658 .\n",
      "Completed iter 200 / 400 . Training loss:  3.153192455911036 .\n",
      "Completed iter 300 / 400 . Training loss:  3.145511543513112 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.372807197314926 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3227331649589735 .\n",
      "Completed iter 200 / 400 . Training loss:  3.2938612653125277 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2690383353406736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5452536919761477 .\n",
      "Completed iter 100 / 400 . Training loss:  3.509106147138491 .\n",
      "Completed iter 200 / 400 . Training loss:  3.478924384852811 .\n",
      "Completed iter 300 / 400 . Training loss:  3.437475998938158 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.694294968979465 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 400 . Training loss:  3.6349675502670395 .\n",
      "Completed iter 200 / 400 . Training loss:  3.6006941199636167 .\n",
      "Completed iter 300 / 400 . Training loss:  3.550951139304275 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3007765453143065 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2738987118164977 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2873083323922385 .\n",
      "Completed iter 300 / 400 . Training loss:  2.266599067235098 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4772245308186904 .\n",
      "Completed iter 100 / 400 . Training loss:  2.452566926726822 .\n",
      "Completed iter 200 / 400 . Training loss:  2.430369072813453 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4032580529483742 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.60719655092432 .\n",
      "Completed iter 100 / 400 . Training loss:  2.604439514851839 .\n",
      "Completed iter 200 / 400 . Training loss:  2.584620062747357 .\n",
      "Completed iter 300 / 400 . Training loss:  2.549307420634034 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.777759709265791 .\n",
      "Completed iter 100 / 400 . Training loss:  2.757065386369515 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7312728998474767 .\n",
      "Completed iter 300 / 400 . Training loss:  2.7070995653955814 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.9307238018462005 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9051087339599784 .\n",
      "Completed iter 200 / 400 . Training loss:  2.8781766665170787 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8600279205608454 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0894176193984735 .\n",
      "Completed iter 100 / 400 . Training loss:  3.066010207204072 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0415669426037426 .\n",
      "Completed iter 300 / 400 . Training loss:  3.0073459151265673 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.235219647462481 .\n",
      "Completed iter 100 / 400 . Training loss:  3.201079566734343 .\n",
      "Completed iter 200 / 400 . Training loss:  3.16935288391404 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1487077756973454 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.389811695509361 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3572501758610898 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3305126159174105 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2900460673595973 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.54280240835886 .\n",
      "Completed iter 100 / 400 . Training loss:  3.488924949569175 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4455425383531875 .\n",
      "Completed iter 300 / 400 . Training loss:  3.413265924172941 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6947804042895354 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6511596155199633 .\n",
      "Completed iter 200 / 400 . Training loss:  3.600077303436537 .\n",
      "Completed iter 300 / 400 . Training loss:  3.568789687763803 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.331369631168182 .\n",
      "Completed iter 100 / 200 . Training loss:  2.313621283395843 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.470922990705924 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4526149654553193 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.610392239654064 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6075446685270234 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.786712465727617 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7665362530370827 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9261901951060785 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8937190554061463 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.07313965754416 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0360651861931975 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2348485470901798 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2037014815544556 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3698999169871655 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3388357787264913 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5367044899319757 .\n",
      "Completed iter 100 / 200 . Training loss:  3.490117890813936 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.718296471893148 .\n",
      "Completed iter 100 / 200 . Training loss:  3.653327998734554 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3065380432730294 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3057552662797276 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4442255983948904 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4299413374334136 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6343233526906324 .\n",
      "Completed iter 100 / 200 . Training loss:  2.610211228610074 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7621019235005138 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7409471145297952 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9447908917091747 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9216114833326907 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.065784910369843 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0480172969961474 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.213631416953608 .\n",
      "Completed iter 100 / 200 . Training loss:  3.185476996472846 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.379610227421146 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3405783817356243 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.566436775338051 .\n",
      "Completed iter 100 / 200 . Training loss:  3.515890108594124 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.709332087707777 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6458412098813406 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3012733571825303 .\n",
      "Completed iter 100 / 200 . Training loss:  2.282435829054816 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4584208064237623 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4556184866948625 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.5994172676104963 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5863233013245917 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.771177816136177 .\n",
      "Completed iter 100 / 200 . Training loss:  2.73996815969837 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9276168109902323 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8957253329109736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0395672036683057 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0250621420075348 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2336137316740823 .\n",
      "Completed iter 100 / 200 . Training loss:  3.189236006862659 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3711433119503678 .\n",
      "Completed iter 100 / 200 . Training loss:  3.33664225810275 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.537360671599142 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5009130176918744 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6872522447395166 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6423129501361005 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.343189188634271 .\n",
      "Completed iter 100 / 200 . Training loss:  2.32676699087966 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.471326730030421 .\n",
      "Completed iter 100 / 200 . Training loss:  2.443822432549753 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.589712603666587 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5752234803740666 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7559014781515203 .\n",
      "Completed iter 100 / 200 . Training loss:  2.744271777279502 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.918726854171 .\n",
      "Completed iter 100 / 200 . Training loss:  2.895152146082588 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0650470833793575 .\n",
      "Completed iter 100 / 200 . Training loss:  3.033247411778431 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2263833143022485 .\n",
      "Completed iter 100 / 200 . Training loss:  3.202985460762333 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.379150278011565 .\n",
      "Completed iter 100 / 200 . Training loss:  3.352040068086848 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.534763349722375 .\n",
      "Completed iter 100 / 200 . Training loss:  3.4967721082660574 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.689616497376387 .\n",
      "Completed iter 100 / 200 . Training loss:  3.650722069752889 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.323209547594352 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3039398261000734 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4801955380976834 .\n",
      "Completed iter 100 / 200 . Training loss:  2.447955136255402 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.622903113968854 .\n",
      "Completed iter 100 / 200 . Training loss:  2.609749128295182 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7766126676577594 .\n",
      "Completed iter 100 / 200 . Training loss:  2.758491266848174 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.919607942141027 .\n",
      "Completed iter 100 / 200 . Training loss:  2.895970942939644 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.07922712492988 .\n",
      "Completed iter 100 / 200 . Training loss:  3.053615849707586 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2581000875522004 .\n",
      "Completed iter 100 / 200 . Training loss:  3.223185016960773 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3675459917029253 .\n",
      "Completed iter 100 / 200 . Training loss:  3.33862380588812 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.550279049108619 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5132243488509016 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.704606867262795 .\n",
      "Completed iter 100 / 200 . Training loss:  3.654638586749033 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.34084228592176 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.313355757813606 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.296001993408383 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.258267596471526 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.2600886968902216 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.2249139832110534 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.312781716420578 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.203653349034983 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.1936297824115645 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.183064491697377 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.1387908537461837 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.178562282882352 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.123771146786924 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.1524861504555046 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.0997979770615682 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.1284086785988894 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.1030263444815525 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.0799325410290224 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.079764221537679 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.1005951264545346 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4406095227346847 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4293105062486164 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.449414931612378 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.410721313570674 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.3684775678974175 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.388304776329807 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.358151491000772 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.375399586290492 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.310291467144374 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.295321746358227 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.3241002254816845 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.3770477341704432 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.2187153611165535 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.3127952943847667 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.284553159133653 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.205105425988772 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.1802708089148393 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.275502109258705 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.117808952735604 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.1885842276384855 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.614463114149413 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.6055230335240345 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.592360466834654 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.5447550121757674 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5437133479315994 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.502127983464061 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.5181386793501703 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.4964228531632813 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.487550163095384 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.4923557746322333 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.452040321275066 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4123397253750003 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.425353927111015 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.382823477151837 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.3708474536221047 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.403625056529945 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3861500674755693 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.341200647988503 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.406630406341874 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.384885222516278 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7446470320683907 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.729635078659422 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.727500415111934 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.658474797717697 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.6762604745845304 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.641888283433082 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.645666781085967 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.6527472908027017 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.632980708664229 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.587649202991849 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.598684181106469 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.523422563274718 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.584641988558877 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.5296606841258344 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.5704199720059573 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.551043093862888 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.5251037356484307 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.515746819637473 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.4724161809662855 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.4669380997640573 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.896395683482136 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.9056604857436445 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.8848704209669878 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.864501991885772 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.8181175463680823 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.8401381890457675 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.801126318818574 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.7426467902997698 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.7177287975726685 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.7308062145633873 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.7930815544069674 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.723248162431375 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.7243192857931358 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.6666086836924077 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.7363605765927717 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.668510146559158 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.669280873343711 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.637839151008537 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1800 / 2000 . Training loss:  2.6767216479219917 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.573158351229307 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.075082824392532 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.0013076895985735 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.9501267079447486 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.958604566311294 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.928293923021435 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.958105658142676 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.945282669509112 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.8154592898775124 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.8572121020141417 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.7890096957485624 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.7754611370249638 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.782669576065782 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.8161919055555615 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.737287763013723 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.7553532938340286 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.7333947567155756 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.7186061092051577 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.666672574187871 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.7222535548736957 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.6801512401444727 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.241306208404852 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.225880401081236 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.172373183603768 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.1432983194329394 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.125230450658083 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.125863557130361 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0763925301786728 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.062972077585275 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.019181199245751 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.983418565985378 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.9517344274855084 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.9407463941914056 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.889841458278536 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.953076915787105 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.9334297029313494 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.8768198140163284 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.827042588870409 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9028823751251887 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.737893536628862 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8173075418356124 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.4027303115667036 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3091142794770234 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.3161251286254156 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.258128628197236 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.228008677284619 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1943109134014573 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.1987366431740423 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1410950312007193 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.1360079428024648 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.1523758437244584 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.118894555323363 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.0596238060517815 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.0006291760524517 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.9872051335542844 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.973279624122632 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.962134968228874 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.9032082250463858 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9310658831713483 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.852316955046602 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8421060596365773 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.565337808946758 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.531574785907437 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.4519271476046547 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.4456047668284215 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.4299510765367174 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.362984181291808 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.3097962933446192 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.2380837907868267 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.2455531539685056 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.189426270975308 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1698090667725625 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.151143931331424 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.108563201183842 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0999012526137255 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.088710838531639 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.030485172131813 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.9623262543894775 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.9823083959551266 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9044629953744776 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.9023756901524966 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.7279066402934986 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.652396402507172 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.5883415836275363 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.573193130306553 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.547905645358835 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4566042949119082 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.4400942159678047 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.38695703881609 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3619982031200637 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.2625497203327285 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.29070581032923 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.2408221245500903 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.220271407707786 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1831264820995298 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.143579795328824 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.1101737488112478 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.1522070760359218 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0462137672879415 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.9759898200771464 .\n",
      "Completed iter 1900 / 2000 . Training loss:  3.0286908089265054 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.305807280699516 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.297952827529374 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.2636137474018003 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.230298698895973 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.243343120092102 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.203889946778823 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.216996122903163 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.210550307429662 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.1506803216704573 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.1618593896235625 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.4477663631240265 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.417599995782288 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4310460351338867 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.3864503020069523 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.36754899591406 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.364895607779262 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.331377943747527 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.3182558334707806 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.313782137427014 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.3264031147478663 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.619005416116668 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.5972021930973552 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.5749303888652864 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 300 / 1000 . Training loss:  2.5505337490724154 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.5173397198843213 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.508655180776936 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.528877559711467 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.5076846738073346 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.503979199726751 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.4882793185302483 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7388795587477714 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.7264321282648942 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7117594133879317 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7062522149297865 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.63980390991174 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.660824804050224 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.624753420902007 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.598985826111918 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6035749301599775 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.612441243758799 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9059063100255145 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.876733811887946 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.8574796180865247 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.8252009645041323 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.8282666149760787 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.771650599408234 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.7551600585793996 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.760936769787964 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.7319498983493737 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6961668822260476 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0713931983852385 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0506526141811743 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.02670856327318 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.014500813454066 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.9669245387580476 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.937370440208426 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9198759178393336 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.9408416364492767 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.8555984713944724 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.875278123334626 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.2349962189492834 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.204231921621429 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.15320820972536 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.127112979843609 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1248680654138576 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.0483008861515737 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.055384448938057 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.018311171429699 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.9933023554876064 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.9663522204654678 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.388290721826512 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.3595611261733658 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.324573116241449 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.2813996752780006 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.2330121085519057 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.225499556625091 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.1979673305469296 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.174830089700354 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.1246966165905703 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.0811417552268985 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.542587635218712 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.4755530966394668 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.427746318465054 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.3821532266190433 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.3807641126728205 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.3257694814386447 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.2844329345149523 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.2370210849381627 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2106320624696676 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.2090623112412517 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.7166674318811452 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6680064185407484 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.5959683177415727 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5437317975223595 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.523599671748018 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.457740361372247 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.414496551603324 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3829644429444192 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.3541548827698056 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.2965924789567893 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.277016724277513 .\n",
      "Completed iter 100 / 600 . Training loss:  2.2623745710627956 .\n",
      "Completed iter 200 / 600 . Training loss:  2.268446388018167 .\n",
      "Completed iter 300 / 600 . Training loss:  2.234975746061046 .\n",
      "Completed iter 400 / 600 . Training loss:  2.23233176766809 .\n",
      "Completed iter 500 / 600 . Training loss:  2.227718286657987 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.470654696833115 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4202783405103103 .\n",
      "Completed iter 200 / 600 . Training loss:  2.391127222100633 .\n",
      "Completed iter 300 / 600 . Training loss:  2.412858309208914 .\n",
      "Completed iter 400 / 600 . Training loss:  2.3650643102937545 .\n",
      "Completed iter 500 / 600 . Training loss:  2.380077069403651 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.6183495775346346 .\n",
      "Completed iter 100 / 600 . Training loss:  2.6017154142972116 .\n",
      "Completed iter 200 / 600 . Training loss:  2.56219210962056 .\n",
      "Completed iter 300 / 600 . Training loss:  2.557975326298258 .\n",
      "Completed iter 400 / 600 . Training loss:  2.5142311396297443 .\n",
      "Completed iter 500 / 600 . Training loss:  2.476450500905188 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.7820762588171135 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7577745831634894 .\n",
      "Completed iter 200 / 600 . Training loss:  2.749599333022997 .\n",
      "Completed iter 300 / 600 . Training loss:  2.7105034912395007 .\n",
      "Completed iter 400 / 600 . Training loss:  2.697841646554588 .\n",
      "Completed iter 500 / 600 . Training loss:  2.689198425301643 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.968138845954694 .\n",
      "Completed iter 100 / 600 . Training loss:  2.941957602682442 .\n",
      "Completed iter 200 / 600 . Training loss:  2.900825831561877 .\n",
      "Completed iter 300 / 600 . Training loss:  2.872095631051698 .\n",
      "Completed iter 400 / 600 . Training loss:  2.8625156311396234 .\n",
      "Completed iter 500 / 600 . Training loss:  2.8523854272827753 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.071157016719794 .\n",
      "Completed iter 100 / 600 . Training loss:  3.077474802021481 .\n",
      "Completed iter 200 / 600 . Training loss:  3.02995143712253 .\n",
      "Completed iter 300 / 600 . Training loss:  3.0009568706658687 .\n",
      "Completed iter 400 / 600 . Training loss:  2.995911067642275 .\n",
      "Completed iter 500 / 600 . Training loss:  2.963279544521355 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2299183593435687 .\n",
      "Completed iter 100 / 600 . Training loss:  3.186487303297914 .\n",
      "Completed iter 200 / 600 . Training loss:  3.1366232714357904 .\n",
      "Completed iter 300 / 600 . Training loss:  3.1391906797868057 .\n",
      "Completed iter 400 / 600 . Training loss:  3.092931827001957 .\n",
      "Completed iter 500 / 600 . Training loss:  3.077758940903429 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.3828203885937755 .\n",
      "Completed iter 100 / 600 . Training loss:  3.3414048077847713 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3059377278533706 .\n",
      "Completed iter 300 / 600 . Training loss:  3.257966272295609 .\n",
      "Completed iter 400 / 600 . Training loss:  3.257433678875872 .\n",
      "Completed iter 500 / 600 . Training loss:  3.210106889932801 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.530283259945846 .\n",
      "Completed iter 100 / 600 . Training loss:  3.490131962448669 .\n",
      "Completed iter 200 / 600 . Training loss:  3.4356411890565175 .\n",
      "Completed iter 300 / 600 . Training loss:  3.4251209080708165 .\n",
      "Completed iter 400 / 600 . Training loss:  3.3467244060059014 .\n",
      "Completed iter 500 / 600 . Training loss:  3.3064284790212652 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.672620758070792 .\n",
      "Completed iter 100 / 600 . Training loss:  3.642990088754784 .\n",
      "Completed iter 200 / 600 . Training loss:  3.57557923268178 .\n",
      "Completed iter 300 / 600 . Training loss:  3.5317160170806976 .\n",
      "Completed iter 400 / 600 . Training loss:  3.4868224899843154 .\n",
      "Completed iter 500 / 600 . Training loss:  3.461760186061208 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3067177140070236 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2793575777462483 .\n",
      "Completed iter 200 / 400 . Training loss:  2.272217435215114 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2598651556787166 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.439676386013069 .\n",
      "Completed iter 100 / 400 . Training loss:  2.441730578843797 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4197455886942594 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4069229974705166 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6077100177037082 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5910109599982976 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5709824399334957 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5520989082107635 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7571834621748015 .\n",
      "Completed iter 100 / 400 . Training loss:  2.729277639926264 .\n",
      "Completed iter 200 / 400 . Training loss:  2.7169382219344915 .\n",
      "Completed iter 300 / 400 . Training loss:  2.6950123532128427 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.912875976244345 .\n",
      "Completed iter 100 / 400 . Training loss:  2.904210833803342 .\n",
      "Completed iter 200 / 400 . Training loss:  2.870457470141191 .\n",
      "Completed iter 300 / 400 . Training loss:  2.869476534103827 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0906172345786516 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0452353462136745 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0304032667739778 .\n",
      "Completed iter 300 / 400 . Training loss:  3.009511749428414 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.230471737620972 .\n",
      "Completed iter 100 / 400 . Training loss:  3.1875765854877383 .\n",
      "Completed iter 200 / 400 . Training loss:  3.164273271029034 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1310466118311675 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.3693890013607253 .\n",
      "Completed iter 100 / 400 . Training loss:  3.3346749235595086 .\n",
      "Completed iter 200 / 400 . Training loss:  3.299441949024327 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2717265255554286 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.5739472507024432 .\n",
      "Completed iter 100 / 400 . Training loss:  3.498237066195343 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4516541029077836 .\n",
      "Completed iter 300 / 400 . Training loss:  3.417328218728574 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6928672572406116 .\n",
      "Completed iter 100 / 400 . Training loss:  3.649642299838865 .\n",
      "Completed iter 200 / 400 . Training loss:  3.586179722918448 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5448081945671013 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.322154773563316 .\n",
      "Completed iter 100 / 400 . Training loss:  2.2965848291092397 .\n",
      "Completed iter 200 / 400 . Training loss:  2.277880881781594 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2596772563189145 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.4739157029619467 .\n",
      "Completed iter 100 / 400 . Training loss:  2.4478175815908854 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4369368269260328 .\n",
      "Completed iter 300 / 400 . Training loss:  2.42111754455805 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.618650491460849 .\n",
      "Completed iter 100 / 400 . Training loss:  2.596322696153952 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5732889876852396 .\n",
      "Completed iter 300 / 400 . Training loss:  2.5567679381348665 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.773375567916902 .\n",
      "Completed iter 100 / 400 . Training loss:  2.764437547337578 .\n",
      "Completed iter 200 / 400 . Training loss:  2.715734077854163 .\n",
      "Completed iter 300 / 400 . Training loss:  2.706327603225785 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.912653032402007 .\n",
      "Completed iter 100 / 400 . Training loss:  2.907652604428477 .\n",
      "Completed iter 200 / 400 . Training loss:  2.873382371269712 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8250911062911905 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.07093988131958 .\n",
      "Completed iter 100 / 400 . Training loss:  3.042254819810971 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0067960265033116 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9868145941339677 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.250307911021012 .\n",
      "Completed iter 100 / 400 . Training loss:  3.195172156802759 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1653408999844617 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1427723994769963 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.345941076950059 .\n",
      "Completed iter 100 / 400 . Training loss:  3.319850978338934 .\n",
      "Completed iter 200 / 400 . Training loss:  3.281833982723154 .\n",
      "Completed iter 300 / 400 . Training loss:  3.2447050840094036 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.509471695120717 .\n",
      "Completed iter 100 / 400 . Training loss:  3.4807752646163452 .\n",
      "Completed iter 200 / 400 . Training loss:  3.435425454327536 .\n",
      "Completed iter 300 / 400 . Training loss:  3.398502944042505 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.718969428076581 .\n",
      "Completed iter 100 / 400 . Training loss:  3.642567831221163 .\n",
      "Completed iter 200 / 400 . Training loss:  3.598078209183405 .\n",
      "Completed iter 300 / 400 . Training loss:  3.5636851230138262 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3040301939759016 .\n",
      "Completed iter 100 / 200 . Training loss:  2.295487531495109 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.450353266740865 .\n",
      "Completed iter 100 / 200 . Training loss:  2.437092987076911 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6265370079512644 .\n",
      "Completed iter 100 / 200 . Training loss:  2.586779251668843 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.765599792697878 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7416233026491073 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.917968659880211 .\n",
      "Completed iter 100 / 200 . Training loss:  2.908536503544849 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0632490995827277 .\n",
      "Completed iter 100 / 200 . Training loss:  3.036198880574178 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2454157996520814 .\n",
      "Completed iter 100 / 200 . Training loss:  3.198695879191888 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3775954860772526 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3439428106934326 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.545325234491247 .\n",
      "Completed iter 100 / 200 . Training loss:  3.496184015179878 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6799969739935183 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6339512591434087 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.328260744145682 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3054498814159152 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4675335988328713 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4614894497689557 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6344745549845574 .\n",
      "Completed iter 100 / 200 . Training loss:  2.603812791903462 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7646808738991586 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7483675982537075 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.925963832295731 .\n",
      "Completed iter 100 / 200 . Training loss:  2.894133097100439 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.071765646346006 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0615705535937696 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.227369492355226 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1932617783718005 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3595069279397336 .\n",
      "Completed iter 100 / 200 . Training loss:  3.321894920082289 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.550577752322514 .\n",
      "Completed iter 100 / 200 . Training loss:  3.52249601264249 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.679739117554621 .\n",
      "Completed iter 100 / 200 . Training loss:  3.631849350594536 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3002611574110103 .\n",
      "Completed iter 100 / 200 . Training loss:  2.3035290433619933 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.45112775284315 .\n",
      "Completed iter 100 / 200 . Training loss:  2.431044814329313 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.639259765212648 .\n",
      "Completed iter 100 / 200 . Training loss:  2.6300205584663408 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.766172558486352 .\n",
      "Completed iter 100 / 200 . Training loss:  2.747969852384202 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.914590270118584 .\n",
      "Completed iter 100 / 200 . Training loss:  2.88749232083984 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0886141961308087 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0374759764835133 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2208921287824244 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1854633910430645 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.4071978612664298 .\n",
      "Completed iter 100 / 200 . Training loss:  3.373271740055551 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.542997163609059 .\n",
      "Completed iter 100 / 200 . Training loss:  3.4970643735557534 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.664688424111686 .\n",
      "Completed iter 100 / 200 . Training loss:  3.626408869257948 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3216922435908702 .\n",
      "Completed iter 100 / 200 . Training loss:  2.294337659837372 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4654595508886508 .\n",
      "Completed iter 100 / 200 . Training loss:  2.436930476404311 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6022300277278108 .\n",
      "Completed iter 100 / 200 . Training loss:  2.588255885916879 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.766386072354373 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7546135624512154 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9185575092411837 .\n",
      "Completed iter 100 / 200 . Training loss:  2.901885923154952 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0516336675666804 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0292599270556106 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2280525711485426 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2028648228154823 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3721270789689903 .\n",
      "Completed iter 100 / 200 . Training loss:  3.324944461831574 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5610797142543698 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5124464172384022 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.7048298909611703 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6551328796919274 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3017606536233535 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2873332118518444 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.487187553217989 .\n",
      "Completed iter 100 / 200 . Training loss:  2.462164334311755 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.602903283005295 .\n",
      "Completed iter 100 / 200 . Training loss:  2.574657243669684 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.765295419900028 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7609030278001927 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.957996117384525 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9152517603542245 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.084043026260061 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0639588650982743 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.1963012602324854 .\n",
      "Completed iter 100 / 200 . Training loss:  3.1682891844434264 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3925835705257015 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3537263204848484 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.517029706674852 .\n",
      "Completed iter 100 / 200 . Training loss:  3.471321599038845 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6958238961822687 .\n",
      "Completed iter 100 / 200 . Training loss:  3.647620393977488 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.365990017889308 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.295089511487627 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.2708441479422845 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.27539359311743 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.235602040014806 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.258341837834984 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.2028959862338295 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.1790259412570414 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.158655483605216 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.1897468148633963 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.173464260627895 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.1586613294772024 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.1258397575313155 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.1532211483719412 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.08487874362218 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.097148132862961 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.0842694963974635 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.0619061248159425 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.027998304621382 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.104743883538443 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.4379098246284814 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.4189477587746775 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.3967383365282506 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.4182690545191305 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.3688053905210986 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.3593132278842437 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.3602644314589116 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.30690409096936 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.285896132945867 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.3153186367275835 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.343504661711467 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.2841018090137837 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.32452222142033 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.2341427838305297 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.2832302901954478 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.2490868175349945 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.2280048604121667 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.2185764133313866 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.133517644728638 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.1766313220464832 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.6506660584972677 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.5852904753219175 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.564254989625592 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.529759400252053 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.5101706136213657 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.553913717756756 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.494113661520152 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.5074878335844755 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 800 / 2000 . Training loss:  2.435549672684122 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.5085234045198597 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.4360166695440397 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.4039799626346103 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.3903929001711237 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.442038969154286 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.3416681708587674 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.358040205417775 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.3711496707552375 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.3552123204752897 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.3701705257240384 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.3662562189619827 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.7277914111985737 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.752607578738136 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.6941943699940416 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.7266567488555293 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.68711069701675 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.644700009417501 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.6688726252295876 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.65792661300559 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.658336186168931 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.5686201051957727 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.626542789531423 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.6045853309874567 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.5630145461530005 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.5712400713055854 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.487900629571974 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.473678509249516 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.4685719320836927 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.5147176463429175 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.5120381784023094 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.491267289331427 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  2.938980573814639 .\n",
      "Completed iter 100 / 2000 . Training loss:  2.8962874834464074 .\n",
      "Completed iter 200 / 2000 . Training loss:  2.83534644625965 .\n",
      "Completed iter 300 / 2000 . Training loss:  2.842132004835809 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.838562256598562 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.81491854414071 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.7514763500929917 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.7721385632091557 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.729574053455102 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.794905812146387 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.693344756644285 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.6910656445019576 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.7158456427348114 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.6650745722351505 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.6569050920724577 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.6140475814317954 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.5960626006584633 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.5508889544328337 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.541495789040334 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.576434531997112 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.076682107753186 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.067408145736169 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.033645478907095 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.0230461929759547 .\n",
      "Completed iter 400 / 2000 . Training loss:  2.981624040457418 .\n",
      "Completed iter 500 / 2000 . Training loss:  2.9360654074066885 .\n",
      "Completed iter 600 / 2000 . Training loss:  2.910360775846887 .\n",
      "Completed iter 700 / 2000 . Training loss:  2.8708568091157236 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.844457946254522 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.8857192704689525 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.826277994376193 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8240889084343506 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.7957231018843247 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.766303846207638 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.740713326746596 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.7536706007923417 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.699755035994729 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.6929413206682966 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.769989695781198 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.703198023974192 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.2325439222348114 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.2101224888434525 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.1457861839418246 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.141969775087629 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.0928490337990007 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.0647496885119474 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.0436422798289833 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.043605495158433 .\n",
      "Completed iter 800 / 2000 . Training loss:  2.98147883787147 .\n",
      "Completed iter 900 / 2000 . Training loss:  2.9929592573017745 .\n",
      "Completed iter 1000 / 2000 . Training loss:  2.91741919978236 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.8988893935257716 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.891966241038514 .\n",
      "Completed iter 1300 / 2000 . Training loss:  2.942414679374221 .\n",
      "Completed iter 1400 / 2000 . Training loss:  2.8353186230943215 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.8356838823877144 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8592102083850666 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.790170623128711 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.6971097375804747 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.7358980914525186 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.3753549693948495 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.3242164184697547 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.2954182560625274 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.270529130840069 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.247949929324011 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.1630097517112175 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.1417048714060853 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.1002925724187396 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.0794068727881827 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.110238752751011 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.0262900117055915 .\n",
      "Completed iter 1100 / 2000 . Training loss:  2.976861779885693 .\n",
      "Completed iter 1200 / 2000 . Training loss:  2.9791413035029515 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0009714837002934 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.000895708170141 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.9393185072527426 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8658550238750955 .\n",
      "Completed iter 1700 / 2000 . Training loss:  2.86946199177524 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.835409276260122 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.865394449165014 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.5155529226743942 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.497929621788188 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.4747346595242186 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.3967705713814267 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.3536056587705922 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.3420753230911844 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.2808548827382955 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.217576343217453 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.2608100029047717 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.196295794778214 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.1097386194848555 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.1857126184178424 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.080341531878889 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.0695948548137464 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 1400 / 2000 . Training loss:  2.995454522754154 .\n",
      "Completed iter 1500 / 2000 . Training loss:  2.982386759720379 .\n",
      "Completed iter 1600 / 2000 . Training loss:  2.8946311731606675 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.0260560931846108 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.943812699299711 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.8895141042926804 .\n",
      "Finished training!\n",
      "Completed iter 0 / 2000 . Training loss:  3.7037320766512942 .\n",
      "Completed iter 100 / 2000 . Training loss:  3.6537867360158973 .\n",
      "Completed iter 200 / 2000 . Training loss:  3.5894202956992483 .\n",
      "Completed iter 300 / 2000 . Training loss:  3.5674055849530615 .\n",
      "Completed iter 400 / 2000 . Training loss:  3.515072906329897 .\n",
      "Completed iter 500 / 2000 . Training loss:  3.4312445227144117 .\n",
      "Completed iter 600 / 2000 . Training loss:  3.4082711418965674 .\n",
      "Completed iter 700 / 2000 . Training loss:  3.3390851927474445 .\n",
      "Completed iter 800 / 2000 . Training loss:  3.3315088595602798 .\n",
      "Completed iter 900 / 2000 . Training loss:  3.287527026147821 .\n",
      "Completed iter 1000 / 2000 . Training loss:  3.241895909815164 .\n",
      "Completed iter 1100 / 2000 . Training loss:  3.1860374686740576 .\n",
      "Completed iter 1200 / 2000 . Training loss:  3.199108603698966 .\n",
      "Completed iter 1300 / 2000 . Training loss:  3.1225696471172446 .\n",
      "Completed iter 1400 / 2000 . Training loss:  3.042925037870274 .\n",
      "Completed iter 1500 / 2000 . Training loss:  3.084967114223879 .\n",
      "Completed iter 1600 / 2000 . Training loss:  3.0538781361849847 .\n",
      "Completed iter 1700 / 2000 . Training loss:  3.062066293766195 .\n",
      "Completed iter 1800 / 2000 . Training loss:  2.91815599252764 .\n",
      "Completed iter 1900 / 2000 . Training loss:  2.990346456419487 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.319940552168748 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.31714949559618 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.2646230978569286 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.255025427523312 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.2792483810220254 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.2326491102178023 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.2135341146986502 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.1919929843503585 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.159973758929664 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.1939217645483367 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.4580942637071006 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.4392560924646216 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.4280200669686445 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.3735065100438026 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.3729655837712107 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.337287795272132 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.341175677889873 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.361545475221605 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.3281428104168937 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.3356673066934053 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.6223818383853055 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.6331314095627656 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.56381164954295 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.559002525595414 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.537950424852163 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.5076292553041357 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.4949666095959344 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.4819160032438026 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.4751069628525504 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.4723580562639276 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.7929093117543 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.763193096485937 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.7380386805025636 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.7293862315640744 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.7109296048018474 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.6810062926729477 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.6582558430628374 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.678622074932658 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.6347695499024377 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.6238804526133963 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  2.9191017355540367 .\n",
      "Completed iter 100 / 1000 . Training loss:  2.890313589171722 .\n",
      "Completed iter 200 / 1000 . Training loss:  2.872133035631082 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.830923031205884 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.804568659083599 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.7687273960307435 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.7758790839935457 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.7534063445987735 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.7201252289716775 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.719611224099652 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.0843350784632877 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.0462745086414516 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.028536314625545 .\n",
      "Completed iter 300 / 1000 . Training loss:  2.973515781638147 .\n",
      "Completed iter 400 / 1000 . Training loss:  2.971276056656979 .\n",
      "Completed iter 500 / 1000 . Training loss:  2.9248262742248343 .\n",
      "Completed iter 600 / 1000 . Training loss:  2.9061497371536817 .\n",
      "Completed iter 700 / 1000 . Training loss:  2.9199117659844305 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.8552345064854547 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.8326832657848993 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.26136483557495 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.2180158398881424 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.1604857699904056 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.1519117811747543 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.1230035994180683 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.081086025691977 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.0593828777452288 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.035768098674514 .\n",
      "Completed iter 800 / 1000 . Training loss:  2.987190835776077 .\n",
      "Completed iter 900 / 1000 . Training loss:  2.967792498453871 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.381571160227028 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.323244588982309 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.3122490031972296 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.269582432214443 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.2399170580087375 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.2111462487083475 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.170374259374642 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.1515016619245433 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.106922484802242 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.0538262839161865 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.528510344279891 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.4868555990419177 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.469077332780686 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.396515494329414 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.388250207348956 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.33441026999336 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.3072601215137816 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.2309701582075694 .\n",
      "Completed iter 800 / 1000 . Training loss:  3.2310574636538156 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.19712849270045 .\n",
      "Finished training!\n",
      "Completed iter 0 / 1000 . Training loss:  3.6703328539719506 .\n",
      "Completed iter 100 / 1000 . Training loss:  3.6185842283385465 .\n",
      "Completed iter 200 / 1000 . Training loss:  3.5863425906610957 .\n",
      "Completed iter 300 / 1000 . Training loss:  3.5260337498956753 .\n",
      "Completed iter 400 / 1000 . Training loss:  3.491159493199868 .\n",
      "Completed iter 500 / 1000 . Training loss:  3.4369633521274765 .\n",
      "Completed iter 600 / 1000 . Training loss:  3.412454271718974 .\n",
      "Completed iter 700 / 1000 . Training loss:  3.3576723971521183 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 800 / 1000 . Training loss:  3.3034067440957124 .\n",
      "Completed iter 900 / 1000 . Training loss:  3.2381993218404217 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.3319736107964815 .\n",
      "Completed iter 100 / 600 . Training loss:  2.320320030499205 .\n",
      "Completed iter 200 / 600 . Training loss:  2.3046886133042452 .\n",
      "Completed iter 300 / 600 . Training loss:  2.244763438144937 .\n",
      "Completed iter 400 / 600 . Training loss:  2.2549330797314435 .\n",
      "Completed iter 500 / 600 . Training loss:  2.2110417431862426 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.459083660860952 .\n",
      "Completed iter 100 / 600 . Training loss:  2.4476125908532014 .\n",
      "Completed iter 200 / 600 . Training loss:  2.4371644329279056 .\n",
      "Completed iter 300 / 600 . Training loss:  2.399314593710953 .\n",
      "Completed iter 400 / 600 . Training loss:  2.3837506667454047 .\n",
      "Completed iter 500 / 600 . Training loss:  2.3766471328065473 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.6378203804666107 .\n",
      "Completed iter 100 / 600 . Training loss:  2.583742768680475 .\n",
      "Completed iter 200 / 600 . Training loss:  2.5677124648926197 .\n",
      "Completed iter 300 / 600 . Training loss:  2.5637487784088893 .\n",
      "Completed iter 400 / 600 . Training loss:  2.561171717994091 .\n",
      "Completed iter 500 / 600 . Training loss:  2.523842757392217 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.779033426710458 .\n",
      "Completed iter 100 / 600 . Training loss:  2.7461148613301556 .\n",
      "Completed iter 200 / 600 . Training loss:  2.722745879139956 .\n",
      "Completed iter 300 / 600 . Training loss:  2.702966979653311 .\n",
      "Completed iter 400 / 600 . Training loss:  2.6797709975629203 .\n",
      "Completed iter 500 / 600 . Training loss:  2.6660648393805895 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  2.928810211455954 .\n",
      "Completed iter 100 / 600 . Training loss:  2.908058595112208 .\n",
      "Completed iter 200 / 600 . Training loss:  2.885760532392522 .\n",
      "Completed iter 300 / 600 . Training loss:  2.856074753510954 .\n",
      "Completed iter 400 / 600 . Training loss:  2.862878138550426 .\n",
      "Completed iter 500 / 600 . Training loss:  2.785037424797824 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.08735352507461 .\n",
      "Completed iter 100 / 600 . Training loss:  3.057271502867066 .\n",
      "Completed iter 200 / 600 . Training loss:  3.0151695599838915 .\n",
      "Completed iter 300 / 600 . Training loss:  2.9896725720965787 .\n",
      "Completed iter 400 / 600 . Training loss:  2.942782604161609 .\n",
      "Completed iter 500 / 600 . Training loss:  2.930783587966174 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.2231382127983608 .\n",
      "Completed iter 100 / 600 . Training loss:  3.1663917391697964 .\n",
      "Completed iter 200 / 600 . Training loss:  3.1465913065524367 .\n",
      "Completed iter 300 / 600 . Training loss:  3.101741095366877 .\n",
      "Completed iter 400 / 600 . Training loss:  3.072125034157625 .\n",
      "Completed iter 500 / 600 . Training loss:  3.0538585476700106 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.43349670415655 .\n",
      "Completed iter 100 / 600 . Training loss:  3.4012145976685955 .\n",
      "Completed iter 200 / 600 . Training loss:  3.3316635348021117 .\n",
      "Completed iter 300 / 600 . Training loss:  3.3005735690125855 .\n",
      "Completed iter 400 / 600 . Training loss:  3.269449337487958 .\n",
      "Completed iter 500 / 600 . Training loss:  3.2094135094480625 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.552552980674921 .\n",
      "Completed iter 100 / 600 . Training loss:  3.504827964178524 .\n",
      "Completed iter 200 / 600 . Training loss:  3.4527605057531257 .\n",
      "Completed iter 300 / 600 . Training loss:  3.4101764843591322 .\n",
      "Completed iter 400 / 600 . Training loss:  3.3740477397905173 .\n",
      "Completed iter 500 / 600 . Training loss:  3.3282679087387304 .\n",
      "Finished training!\n",
      "Completed iter 0 / 600 . Training loss:  3.704958872654029 .\n",
      "Completed iter 100 / 600 . Training loss:  3.621929437221478 .\n",
      "Completed iter 200 / 600 . Training loss:  3.5885199473702705 .\n",
      "Completed iter 300 / 600 . Training loss:  3.546012839328125 .\n",
      "Completed iter 400 / 600 . Training loss:  3.513445900082881 .\n",
      "Completed iter 500 / 600 . Training loss:  3.4679426484881657 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3279906676147806 .\n",
      "Completed iter 100 / 400 . Training loss:  2.3106991601936047 .\n",
      "Completed iter 200 / 400 . Training loss:  2.279617380499464 .\n",
      "Completed iter 300 / 400 . Training loss:  2.276913023515953 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.445409269469064 .\n",
      "Completed iter 100 / 400 . Training loss:  2.435766151866449 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4007554400999367 .\n",
      "Completed iter 300 / 400 . Training loss:  2.3928451238561244 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.6183951783964585 .\n",
      "Completed iter 100 / 400 . Training loss:  2.6061229505450028 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5761740258714028 .\n",
      "Completed iter 300 / 400 . Training loss:  2.57120692424888 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.764174363981001 .\n",
      "Completed iter 100 / 400 . Training loss:  2.7124129214744643 .\n",
      "Completed iter 200 / 400 . Training loss:  2.6878433212753814 .\n",
      "Completed iter 300 / 400 . Training loss:  2.691372906532593 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.928290453784932 .\n",
      "Completed iter 100 / 400 . Training loss:  2.8970832601383463 .\n",
      "Completed iter 200 / 400 . Training loss:  2.8813677082113243 .\n",
      "Completed iter 300 / 400 . Training loss:  2.850136933507963 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0760554808926126 .\n",
      "Completed iter 100 / 400 . Training loss:  3.0561697868289137 .\n",
      "Completed iter 200 / 400 . Training loss:  3.0273505369416065 .\n",
      "Completed iter 300 / 400 . Training loss:  2.980802510325329 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2278187898206956 .\n",
      "Completed iter 100 / 400 . Training loss:  3.20812216218353 .\n",
      "Completed iter 200 / 400 . Training loss:  3.151460500528019 .\n",
      "Completed iter 300 / 400 . Training loss:  3.121349519423869 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.402034039365181 .\n",
      "Completed iter 100 / 400 . Training loss:  3.358564544851302 .\n",
      "Completed iter 200 / 400 . Training loss:  3.3168545580244038 .\n",
      "Completed iter 300 / 400 . Training loss:  3.281132983180731 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.54315007195762 .\n",
      "Completed iter 100 / 400 . Training loss:  3.4548779110074896 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4210387826687345 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3883242138449834 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.6872675682959706 .\n",
      "Completed iter 100 / 400 . Training loss:  3.6395212993642447 .\n",
      "Completed iter 200 / 400 . Training loss:  3.583121595147556 .\n",
      "Completed iter 300 / 400 . Training loss:  3.534733451418034 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.3407391381116094 .\n",
      "Completed iter 100 / 400 . Training loss:  2.297982369942192 .\n",
      "Completed iter 200 / 400 . Training loss:  2.2831774115677272 .\n",
      "Completed iter 300 / 400 . Training loss:  2.2720801637289374 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.478100811310551 .\n",
      "Completed iter 100 / 400 . Training loss:  2.454364411188894 .\n",
      "Completed iter 200 / 400 . Training loss:  2.4466061594941984 .\n",
      "Completed iter 300 / 400 . Training loss:  2.4078962306359606 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.596193280436849 .\n",
      "Completed iter 100 / 400 . Training loss:  2.5993292648490662 .\n",
      "Completed iter 200 / 400 . Training loss:  2.5720193351373335 .\n",
      "Completed iter 300 / 400 . Training loss:  2.548934728731217 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.7494665006309384 .\n",
      "Completed iter 100 / 400 . Training loss:  2.724095205686389 .\n",
      "Completed iter 200 / 400 . Training loss:  2.698358663685614 .\n",
      "Completed iter 300 / 400 . Training loss:  2.702443385071761 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  2.947320796098694 .\n",
      "Completed iter 100 / 400 . Training loss:  2.9188120874295183 .\n",
      "Completed iter 200 / 400 . Training loss:  2.889307640880416 .\n",
      "Completed iter 300 / 400 . Training loss:  2.8533475896120795 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.0869836716023977 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 400 . Training loss:  3.049620126496554 .\n",
      "Completed iter 200 / 400 . Training loss:  3.032771251099531 .\n",
      "Completed iter 300 / 400 . Training loss:  2.9841477562919607 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.2107932718903394 .\n",
      "Completed iter 100 / 400 . Training loss:  3.1886879732905795 .\n",
      "Completed iter 200 / 400 . Training loss:  3.1379172726290436 .\n",
      "Completed iter 300 / 400 . Training loss:  3.1143506689716642 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.411582718546337 .\n",
      "Completed iter 100 / 400 . Training loss:  3.368459110334254 .\n",
      "Completed iter 200 / 400 . Training loss:  3.34735308479447 .\n",
      "Completed iter 300 / 400 . Training loss:  3.3025891557603213 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.55332067251501 .\n",
      "Completed iter 100 / 400 . Training loss:  3.501696779580763 .\n",
      "Completed iter 200 / 400 . Training loss:  3.4427648011686793 .\n",
      "Completed iter 300 / 400 . Training loss:  3.4106196216166733 .\n",
      "Finished training!\n",
      "Completed iter 0 / 400 . Training loss:  3.690011704403589 .\n",
      "Completed iter 100 / 400 . Training loss:  3.615648120830376 .\n",
      "Completed iter 200 / 400 . Training loss:  3.5632644064521317 .\n",
      "Completed iter 300 / 400 . Training loss:  3.536712822196736 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.2901174691525017 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2760824276776335 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.471056900803966 .\n",
      "Completed iter 100 / 200 . Training loss:  2.444245876390327 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6128018668794977 .\n",
      "Completed iter 100 / 200 . Training loss:  2.5796588913343594 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.790378907846806 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7626557324218615 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.932906442263414 .\n",
      "Completed iter 100 / 200 . Training loss:  2.917418350470976 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.06890607885123 .\n",
      "Completed iter 100 / 200 . Training loss:  3.032409501217229 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.271311101638352 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2332238197848424 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3907656353854483 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3478191789226983 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5433402687220763 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5076245265170063 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6806009866023124 .\n",
      "Completed iter 100 / 200 . Training loss:  3.635215979751763 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3048806557726933 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2864537625896624 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.452649569839841 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4350070187568114 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6194823508803284 .\n",
      "Completed iter 100 / 200 . Training loss:  2.592315833095551 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7763381768189195 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7539164197304977 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9062283834900535 .\n",
      "Completed iter 100 / 200 . Training loss:  2.8898160407447846 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.061062062373292 .\n",
      "Completed iter 100 / 200 . Training loss:  3.038759934431342 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2460911794119887 .\n",
      "Completed iter 100 / 200 . Training loss:  3.228201949357386 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3736010910435064 .\n",
      "Completed iter 100 / 200 . Training loss:  3.335898755905553 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.540225407186208 .\n",
      "Completed iter 100 / 200 . Training loss:  3.5046307031481416 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.690164768664009 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6491064053238595 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.3055831616758784 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2867767141495965 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4787993253018894 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4535994632452827 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.627430961746352 .\n",
      "Completed iter 100 / 200 . Training loss:  2.601981685860075 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.74253495869755 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7244419907440163 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.910197792136427 .\n",
      "Completed iter 100 / 200 . Training loss:  2.893476142158697 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.0982793856192785 .\n",
      "Completed iter 100 / 200 . Training loss:  3.0666417384632085 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2596050641564425 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2216109407183064 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.357441962407965 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3108094621335615 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5306964140060955 .\n",
      "Completed iter 100 / 200 . Training loss:  3.475263915770787 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6938724020474436 .\n",
      "Completed iter 100 / 200 . Training loss:  3.647656828636014 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.303795078761577 .\n",
      "Completed iter 100 / 200 . Training loss:  2.2750713301028576 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.457508766942234 .\n",
      "Completed iter 100 / 200 . Training loss:  2.438582243610921 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.6083490821335666 .\n",
      "Completed iter 100 / 200 . Training loss:  2.600442669336631 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.7537663278258964 .\n",
      "Completed iter 100 / 200 . Training loss:  2.7468212303449815 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9216927201725365 .\n",
      "Completed iter 100 / 200 . Training loss:  2.893543020238888 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.063975055711925 .\n",
      "Completed iter 100 / 200 . Training loss:  3.035292468295922 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.2294830856615286 .\n",
      "Completed iter 100 / 200 . Training loss:  3.174050309623113 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3935569804175634 .\n",
      "Completed iter 100 / 200 . Training loss:  3.3531332535292617 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5330780621385225 .\n",
      "Completed iter 100 / 200 . Training loss:  3.470297383467789 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6994860030078067 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6483764776719516 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.327364921972354 .\n",
      "Completed iter 100 / 200 . Training loss:  2.292125346499618 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.4640331828447986 .\n",
      "Completed iter 100 / 200 . Training loss:  2.4339831575902213 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.635719637919041 .\n",
      "Completed iter 100 / 200 . Training loss:  2.634774747527696 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.751106202213695 .\n",
      "Completed iter 100 / 200 . Training loss:  2.727662748467016 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  2.9483359424309277 .\n",
      "Completed iter 100 / 200 . Training loss:  2.9239353993885695 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.057902530654317 .\n",
      "Completed iter 100 / 200 . Training loss:  3.039610700310564 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.246523778037748 .\n",
      "Completed iter 100 / 200 . Training loss:  3.2243083678639453 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.3935568778243046 .\n",
      "Completed iter 100 / 200 . Training loss:  3.354270917320976 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.5309722660233094 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 100 / 200 . Training loss:  3.4845572316251303 .\n",
      "Finished training!\n",
      "Completed iter 0 / 200 . Training loss:  3.6642122207653736 .\n",
      "Completed iter 100 / 200 . Training loss:  3.6066464467762067 .\n",
      "Finished training!\n",
      "[0.228, 0.286, 0.258, 0.228, 0.252, 0.268, 0.246, 0.27, 0.272, 0.266, 0.204, 0.168, 0.18, 0.214, 0.17, 0.156, 0.202, 0.208, 0.192, 0.226, 0.196, 0.164, 0.148, 0.188, 0.17, 0.148, 0.156, 0.142, 0.162, 0.162, 0.16, 0.122, 0.17, 0.144, 0.144, 0.126, 0.16, 0.102, 0.158, 0.138, 0.188, 0.16, 0.198, 0.118, 0.12, 0.152, 0.136, 0.15, 0.172, 0.156, 0.124, 0.12, 0.128, 0.118, 0.116, 0.17, 0.096, 0.09, 0.154, 0.11, 0.128, 0.086, 0.118, 0.126, 0.134, 0.134, 0.122, 0.12, 0.148, 0.098, 0.128, 0.092, 0.14, 0.118, 0.11, 0.084, 0.106, 0.116, 0.114, 0.13, 0.078, 0.128, 0.114, 0.132, 0.152, 0.158, 0.122, 0.136, 0.096, 0.142, 0.11, 0.114, 0.138, 0.128, 0.164, 0.092, 0.106, 0.124, 0.136, 0.142, 0.284, 0.244, 0.282, 0.23, 0.26, 0.28, 0.23, 0.306, 0.3, 0.24, 0.212, 0.208, 0.196, 0.208, 0.208, 0.238, 0.204, 0.202, 0.198, 0.232, 0.16, 0.196, 0.194, 0.168, 0.17, 0.178, 0.146, 0.186, 0.146, 0.138, 0.152, 0.122, 0.13, 0.156, 0.116, 0.112, 0.166, 0.142, 0.172, 0.126, 0.126, 0.134, 0.146, 0.1, 0.152, 0.132, 0.182, 0.146, 0.178, 0.15, 0.128, 0.11, 0.148, 0.102, 0.088, 0.152, 0.118, 0.1, 0.054, 0.1, 0.146, 0.138, 0.13, 0.136, 0.094, 0.182, 0.166, 0.076, 0.136, 0.096, 0.134, 0.12, 0.14, 0.106, 0.13, 0.12, 0.128, 0.08, 0.108, 0.11, 0.156, 0.094, 0.14, 0.124, 0.188, 0.124, 0.14, 0.088, 0.12, 0.134, 0.09, 0.134, 0.172, 0.132, 0.11, 0.132, 0.124, 0.106, 0.09, 0.13, 0.26, 0.268, 0.276, 0.284, 0.27, 0.244, 0.26, 0.28, 0.27, 0.262, 0.25, 0.194, 0.212, 0.198, 0.212, 0.224, 0.244, 0.232, 0.216, 0.23, 0.202, 0.172, 0.208, 0.15, 0.218, 0.178, 0.196, 0.228, 0.198, 0.172, 0.134, 0.16, 0.174, 0.148, 0.118, 0.16, 0.152, 0.13, 0.16, 0.092, 0.17, 0.126, 0.158, 0.128, 0.152, 0.14, 0.184, 0.15, 0.128, 0.164, 0.136, 0.118, 0.086, 0.146, 0.142, 0.126, 0.136, 0.16, 0.132, 0.13, 0.142, 0.114, 0.092, 0.148, 0.128, 0.116, 0.146, 0.1, 0.15, 0.138, 0.106, 0.146, 0.168, 0.122, 0.11, 0.098, 0.116, 0.104, 0.14, 0.184, 0.114, 0.118, 0.14, 0.128, 0.112, 0.138, 0.1, 0.096, 0.14, 0.112, 0.134, 0.112, 0.116, 0.162, 0.188, 0.13, 0.16, 0.146, 0.112, 0.124, 0.282, 0.252, 0.296, 0.278, 0.26, 0.29, 0.286, 0.31, 0.282, 0.268, 0.228, 0.218, 0.212, 0.216, 0.256, 0.24, 0.232, 0.224, 0.23, 0.238, 0.166, 0.196, 0.212, 0.202, 0.21, 0.208, 0.228, 0.166, 0.22, 0.192, 0.192, 0.164, 0.144, 0.144, 0.18, 0.174, 0.114, 0.172, 0.162, 0.152, 0.138, 0.128, 0.146, 0.192, 0.176, 0.17, 0.17, 0.196, 0.176, 0.184, 0.16, 0.146, 0.156, 0.122, 0.146, 0.12, 0.134, 0.112, 0.122, 0.124, 0.126, 0.102, 0.138, 0.128, 0.138, 0.102, 0.132, 0.148, 0.114, 0.156, 0.162, 0.136, 0.154, 0.144, 0.152, 0.112, 0.154, 0.158, 0.12, 0.128, 0.114, 0.114, 0.136, 0.136, 0.106, 0.126, 0.108, 0.108, 0.116, 0.128, 0.148, 0.128, 0.124, 0.124, 0.13, 0.144, 0.1, 0.13, 0.14, 0.134, 0.284, 0.288, 0.3, 0.264, 0.284, 0.274, 0.278, 0.298, 0.31, 0.284, 0.242, 0.246, 0.218, 0.234, 0.25, 0.246, 0.236, 0.24, 0.248, 0.236, 0.224, 0.212, 0.216, 0.222, 0.196, 0.198, 0.23, 0.186, 0.224, 0.214, 0.188, 0.222, 0.18, 0.192, 0.172, 0.188, 0.188, 0.18, 0.162, 0.174, 0.192, 0.158, 0.198, 0.17, 0.178, 0.174, 0.158, 0.18, 0.14, 0.22, 0.14, 0.12, 0.108, 0.12, 0.086, 0.112, 0.14, 0.152, 0.156, 0.132, 0.16, 0.126, 0.154, 0.15, 0.106, 0.114, 0.088, 0.128, 0.136, 0.122, 0.094, 0.146, 0.142, 0.126, 0.128, 0.122, 0.17, 0.136, 0.108, 0.136, 0.142, 0.112, 0.12, 0.134, 0.156, 0.104, 0.164, 0.168, 0.12, 0.146, 0.178, 0.158, 0.108, 0.17, 0.082, 0.116, 0.142, 0.148, 0.144, 0.144, 0.298, 0.288, 0.278, 0.286, 0.286, 0.288, 0.306, 0.296, 0.312, 0.312, 0.234, 0.262, 0.246, 0.238, 0.246, 0.25, 0.226, 0.246, 0.25, 0.226, 0.25, 0.184, 0.216, 0.206, 0.2, 0.232, 0.204, 0.232, 0.214, 0.18, 0.202, 0.19, 0.182, 0.166, 0.214, 0.174, 0.214, 0.21, 0.156, 0.166, 0.182, 0.198, 0.132, 0.188, 0.18, 0.184, 0.158, 0.194, 0.16, 0.194, 0.146, 0.134, 0.17, 0.108, 0.186, 0.148, 0.156, 0.148, 0.106, 0.17, 0.122, 0.14, 0.096, 0.12, 0.114, 0.12, 0.198, 0.158, 0.146, 0.118, 0.15, 0.148, 0.156, 0.172, 0.136, 0.166, 0.088, 0.16, 0.114, 0.114, 0.154, 0.158, 0.124, 0.148, 0.174, 0.178, 0.132, 0.146, 0.142, 0.138, 0.16, 0.146, 0.174, 0.112, 0.132, 0.11, 0.11, 0.108, 0.094, 0.154, 0.298, 0.294, 0.31, 0.286, 0.314, 0.29, 0.304, 0.292, 0.3, 0.29, 0.238, 0.256, 0.27, 0.262, 0.262, 0.254, 0.244, 0.258, 0.282, 0.268, 0.232, 0.238, 0.236, 0.214, 0.248, 0.236, 0.234, 0.222, 0.208, 0.234, 0.21, 0.188, 0.218, 0.194, 0.172, 0.202, 0.176, 0.192, 0.2, 0.224, 0.172, 0.196, 0.168, 0.178, 0.18, 0.204, 0.178, 0.16, 0.192, 0.186, 0.112, 0.19, 0.178, 0.126, 0.168, 0.186, 0.184, 0.174, 0.168, 0.09, 0.156, 0.138, 0.114, 0.156, 0.166, 0.12, 0.114, 0.198, 0.132, 0.11, 0.174, 0.164, 0.11, 0.182, 0.142, 0.114, 0.178, 0.128, 0.146, 0.126, 0.118, 0.128, 0.156, 0.138, 0.152, 0.16, 0.142, 0.164, 0.16, 0.118, 0.162, 0.172, 0.14, 0.148, 0.12, 0.146, 0.142, 0.19, 0.138, 0.144, 0.294, 0.288, 0.31, 0.286, 0.304, 0.282, 0.312, 0.326, 0.296, 0.322, 0.25, 0.25, 0.254, 0.296, 0.234, 0.232, 0.248, 0.288, 0.27, 0.262, 0.228, 0.228, 0.206, 0.21, 0.216, 0.212, 0.236, 0.206, 0.222, 0.256, 0.202, 0.19, 0.182, 0.138, 0.182, 0.2, 0.224, 0.182, 0.206, 0.174, 0.202, 0.174, 0.184, 0.174, 0.176, 0.162, 0.188, 0.168, 0.204, 0.226, 0.126, 0.15, 0.144, 0.118, 0.17, 0.184, 0.148, 0.136, 0.162, 0.124, 0.176, 0.168, 0.162, 0.162, 0.12, 0.148, 0.152, 0.15, 0.132, 0.13, 0.18, 0.136, 0.168, 0.15, 0.162, 0.176, 0.126, 0.152, 0.17, 0.18, 0.096, 0.15, 0.176, 0.156, 0.134, 0.154, 0.14, 0.176, 0.168, 0.16, 0.156, 0.106, 0.148, 0.138, 0.138, 0.106, 0.122, 0.162, 0.144, 0.162, 0.296, 0.322, 0.312, 0.322, 0.304, 0.296, 0.29, 0.326, 0.31, 0.304, 0.25, 0.284, 0.278, 0.26, 0.256, 0.258, 0.26, 0.264, 0.25, 0.252, 0.236, 0.258, 0.23, 0.218, 0.186, 0.248, 0.21, 0.21, 0.256, 0.236, 0.212, 0.194, 0.216, 0.226, 0.184, 0.2, 0.204, 0.21, 0.198, 0.21, 0.188, 0.174, 0.23, 0.206, 0.238, 0.214, 0.22, 0.222, 0.194, 0.206, 0.154, 0.16, 0.144, 0.182, 0.16, 0.148, 0.158, 0.158, 0.158, 0.144, 0.116, 0.13, 0.14, 0.148, 0.14, 0.14, 0.126, 0.158, 0.134, 0.166, 0.162, 0.164, 0.102, 0.172, 0.162, 0.138, 0.19, 0.106, 0.176, 0.194, 0.144, 0.176, 0.186, 0.138, 0.18, 0.18, 0.166, 0.186, 0.118, 0.116, 0.204, 0.152, 0.17, 0.136, 0.12, 0.146, 0.2, 0.146, 0.176, 0.136, 0.306, 0.322, 0.32, 0.308, 0.304, 0.314, 0.32, 0.324, 0.306, 0.306, 0.266, 0.274, 0.292, 0.25, 0.292, 0.26, 0.276, 0.28, 0.284, 0.278, 0.236, 0.236, 0.21, 0.24, 0.216, 0.216, 0.274, 0.22, 0.208, 0.282, 0.202, 0.214, 0.198, 0.216, 0.198, 0.184, 0.23, 0.222, 0.224, 0.204, 0.182, 0.2, 0.22, 0.23, 0.196, 0.214, 0.202, 0.17, 0.198, 0.192, 0.17, 0.136, 0.178, 0.118, 0.162, 0.152, 0.138, 0.182, 0.15, 0.18, 0.156, 0.164, 0.178, 0.13, 0.19, 0.162, 0.152, 0.164, 0.15, 0.18, 0.186, 0.186, 0.16, 0.176, 0.17, 0.142, 0.122, 0.222, 0.176, 0.198, 0.166, 0.19, 0.162, 0.162, 0.176, 0.2, 0.152, 0.146, 0.146, 0.174, 0.16, 0.152, 0.124, 0.146, 0.136, 0.16, 0.116, 0.144, 0.138, 0.184]\n",
      "[0.232, 0.224, 0.22, 0.238, 0.222, 0.246, 0.2, 0.264, 0.252, 0.258, 0.2, 0.176, 0.174, 0.206, 0.164, 0.16, 0.186, 0.198, 0.166, 0.2, 0.186, 0.196, 0.116, 0.182, 0.166, 0.11, 0.114, 0.142, 0.144, 0.17, 0.124, 0.116, 0.154, 0.108, 0.116, 0.134, 0.17, 0.112, 0.168, 0.148, 0.164, 0.144, 0.14, 0.116, 0.11, 0.148, 0.136, 0.126, 0.146, 0.148, 0.094, 0.104, 0.122, 0.114, 0.126, 0.14, 0.072, 0.14, 0.154, 0.1, 0.144, 0.11, 0.118, 0.12, 0.13, 0.116, 0.136, 0.1, 0.126, 0.082, 0.084, 0.1, 0.15, 0.144, 0.146, 0.106, 0.11, 0.12, 0.13, 0.108, 0.09, 0.094, 0.102, 0.144, 0.146, 0.106, 0.122, 0.124, 0.114, 0.162, 0.102, 0.126, 0.132, 0.13, 0.146, 0.058, 0.156, 0.096, 0.118, 0.114, 0.24, 0.236, 0.232, 0.234, 0.222, 0.244, 0.226, 0.21, 0.276, 0.252, 0.192, 0.184, 0.19, 0.186, 0.184, 0.204, 0.218, 0.2, 0.218, 0.208, 0.17, 0.204, 0.146, 0.156, 0.176, 0.184, 0.158, 0.18, 0.166, 0.168, 0.168, 0.132, 0.094, 0.146, 0.12, 0.11, 0.134, 0.118, 0.126, 0.124, 0.118, 0.122, 0.174, 0.108, 0.144, 0.114, 0.168, 0.188, 0.166, 0.162, 0.07, 0.114, 0.164, 0.106, 0.082, 0.104, 0.12, 0.076, 0.086, 0.114, 0.128, 0.128, 0.12, 0.158, 0.096, 0.186, 0.164, 0.07, 0.118, 0.122, 0.148, 0.11, 0.136, 0.124, 0.122, 0.1, 0.102, 0.104, 0.12, 0.14, 0.15, 0.084, 0.118, 0.144, 0.172, 0.086, 0.102, 0.08, 0.134, 0.116, 0.096, 0.134, 0.186, 0.122, 0.122, 0.13, 0.126, 0.122, 0.112, 0.118, 0.238, 0.272, 0.242, 0.234, 0.228, 0.24, 0.252, 0.252, 0.256, 0.27, 0.2, 0.204, 0.214, 0.192, 0.238, 0.198, 0.23, 0.194, 0.196, 0.228, 0.25, 0.188, 0.146, 0.192, 0.194, 0.14, 0.164, 0.19, 0.164, 0.17, 0.124, 0.138, 0.152, 0.18, 0.116, 0.142, 0.142, 0.17, 0.15, 0.126, 0.18, 0.13, 0.152, 0.132, 0.164, 0.128, 0.138, 0.144, 0.144, 0.18, 0.108, 0.098, 0.1, 0.144, 0.088, 0.142, 0.152, 0.15, 0.116, 0.11, 0.152, 0.102, 0.114, 0.148, 0.122, 0.11, 0.104, 0.124, 0.168, 0.122, 0.12, 0.148, 0.188, 0.164, 0.106, 0.1, 0.094, 0.116, 0.124, 0.148, 0.144, 0.096, 0.134, 0.122, 0.098, 0.12, 0.114, 0.096, 0.122, 0.118, 0.146, 0.132, 0.124, 0.164, 0.174, 0.112, 0.124, 0.12, 0.114, 0.112, 0.25, 0.23, 0.226, 0.27, 0.242, 0.278, 0.234, 0.26, 0.244, 0.226, 0.188, 0.236, 0.24, 0.176, 0.224, 0.184, 0.242, 0.23, 0.214, 0.202, 0.144, 0.192, 0.164, 0.186, 0.212, 0.232, 0.174, 0.172, 0.202, 0.206, 0.128, 0.16, 0.16, 0.136, 0.178, 0.17, 0.108, 0.142, 0.176, 0.118, 0.142, 0.118, 0.13, 0.158, 0.168, 0.192, 0.178, 0.186, 0.122, 0.19, 0.14, 0.132, 0.15, 0.138, 0.128, 0.146, 0.112, 0.146, 0.082, 0.122, 0.128, 0.122, 0.128, 0.144, 0.136, 0.13, 0.13, 0.138, 0.104, 0.13, 0.144, 0.144, 0.152, 0.15, 0.148, 0.12, 0.134, 0.134, 0.124, 0.122, 0.138, 0.12, 0.126, 0.146, 0.12, 0.162, 0.128, 0.11, 0.114, 0.108, 0.1, 0.172, 0.14, 0.078, 0.132, 0.134, 0.118, 0.15, 0.134, 0.144, 0.234, 0.282, 0.242, 0.23, 0.248, 0.266, 0.266, 0.248, 0.274, 0.258, 0.244, 0.214, 0.186, 0.254, 0.232, 0.19, 0.218, 0.222, 0.22, 0.218, 0.224, 0.204, 0.228, 0.208, 0.172, 0.24, 0.192, 0.196, 0.234, 0.202, 0.154, 0.214, 0.18, 0.178, 0.154, 0.206, 0.188, 0.128, 0.118, 0.154, 0.206, 0.16, 0.192, 0.168, 0.166, 0.14, 0.172, 0.178, 0.166, 0.174, 0.118, 0.114, 0.13, 0.156, 0.092, 0.12, 0.128, 0.15, 0.122, 0.138, 0.136, 0.1, 0.17, 0.134, 0.096, 0.132, 0.118, 0.138, 0.118, 0.138, 0.122, 0.116, 0.146, 0.12, 0.122, 0.132, 0.172, 0.114, 0.118, 0.132, 0.164, 0.116, 0.12, 0.148, 0.132, 0.138, 0.144, 0.152, 0.124, 0.134, 0.174, 0.112, 0.108, 0.186, 0.118, 0.098, 0.122, 0.166, 0.138, 0.108, 0.272, 0.256, 0.256, 0.246, 0.246, 0.282, 0.244, 0.258, 0.26, 0.256, 0.216, 0.216, 0.216, 0.214, 0.22, 0.222, 0.24, 0.264, 0.258, 0.248, 0.212, 0.194, 0.188, 0.204, 0.192, 0.206, 0.21, 0.222, 0.204, 0.186, 0.202, 0.194, 0.16, 0.198, 0.19, 0.168, 0.19, 0.218, 0.152, 0.192, 0.196, 0.188, 0.126, 0.174, 0.19, 0.172, 0.124, 0.164, 0.172, 0.18, 0.16, 0.152, 0.144, 0.122, 0.184, 0.16, 0.138, 0.146, 0.098, 0.136, 0.13, 0.112, 0.1, 0.134, 0.098, 0.13, 0.156, 0.154, 0.122, 0.132, 0.142, 0.138, 0.18, 0.166, 0.136, 0.128, 0.07, 0.172, 0.116, 0.082, 0.172, 0.12, 0.124, 0.172, 0.156, 0.128, 0.12, 0.116, 0.16, 0.108, 0.15, 0.128, 0.17, 0.088, 0.136, 0.128, 0.118, 0.108, 0.068, 0.146, 0.246, 0.262, 0.274, 0.248, 0.256, 0.296, 0.284, 0.244, 0.256, 0.262, 0.23, 0.256, 0.24, 0.224, 0.244, 0.24, 0.238, 0.238, 0.218, 0.258, 0.244, 0.242, 0.228, 0.196, 0.23, 0.212, 0.206, 0.216, 0.194, 0.176, 0.208, 0.186, 0.192, 0.178, 0.174, 0.168, 0.174, 0.148, 0.206, 0.198, 0.178, 0.182, 0.194, 0.192, 0.16, 0.186, 0.182, 0.176, 0.242, 0.172, 0.14, 0.164, 0.2, 0.166, 0.146, 0.18, 0.182, 0.158, 0.138, 0.074, 0.178, 0.182, 0.112, 0.126, 0.148, 0.11, 0.126, 0.17, 0.132, 0.102, 0.12, 0.156, 0.104, 0.172, 0.168, 0.134, 0.182, 0.128, 0.146, 0.136, 0.112, 0.098, 0.132, 0.13, 0.136, 0.16, 0.122, 0.188, 0.158, 0.116, 0.152, 0.168, 0.166, 0.15, 0.136, 0.146, 0.12, 0.164, 0.192, 0.164, 0.292, 0.278, 0.264, 0.242, 0.252, 0.268, 0.272, 0.244, 0.25, 0.278, 0.234, 0.238, 0.246, 0.242, 0.258, 0.23, 0.262, 0.248, 0.228, 0.234, 0.22, 0.212, 0.228, 0.174, 0.206, 0.218, 0.194, 0.212, 0.232, 0.198, 0.19, 0.192, 0.194, 0.15, 0.17, 0.17, 0.18, 0.196, 0.178, 0.184, 0.188, 0.18, 0.212, 0.156, 0.186, 0.176, 0.17, 0.142, 0.196, 0.192, 0.128, 0.138, 0.164, 0.106, 0.174, 0.16, 0.178, 0.146, 0.124, 0.146, 0.144, 0.182, 0.152, 0.202, 0.138, 0.15, 0.142, 0.138, 0.104, 0.136, 0.134, 0.142, 0.162, 0.166, 0.144, 0.16, 0.118, 0.114, 0.146, 0.17, 0.092, 0.15, 0.212, 0.168, 0.134, 0.156, 0.142, 0.188, 0.158, 0.156, 0.106, 0.106, 0.132, 0.118, 0.166, 0.124, 0.132, 0.17, 0.16, 0.164, 0.272, 0.278, 0.26, 0.276, 0.28, 0.27, 0.286, 0.288, 0.274, 0.266, 0.236, 0.252, 0.252, 0.234, 0.256, 0.254, 0.23, 0.228, 0.254, 0.246, 0.24, 0.214, 0.268, 0.178, 0.168, 0.196, 0.208, 0.234, 0.246, 0.206, 0.192, 0.184, 0.196, 0.19, 0.16, 0.19, 0.168, 0.186, 0.194, 0.172, 0.168, 0.184, 0.198, 0.186, 0.212, 0.208, 0.2, 0.208, 0.198, 0.212, 0.17, 0.184, 0.156, 0.166, 0.16, 0.164, 0.114, 0.122, 0.156, 0.132, 0.134, 0.12, 0.134, 0.16, 0.124, 0.11, 0.104, 0.188, 0.154, 0.18, 0.152, 0.162, 0.124, 0.2, 0.164, 0.158, 0.18, 0.116, 0.186, 0.194, 0.138, 0.148, 0.162, 0.158, 0.14, 0.158, 0.176, 0.162, 0.094, 0.134, 0.146, 0.134, 0.13, 0.146, 0.122, 0.156, 0.19, 0.118, 0.158, 0.132, 0.27, 0.276, 0.268, 0.258, 0.264, 0.248, 0.27, 0.258, 0.27, 0.294, 0.228, 0.256, 0.242, 0.236, 0.262, 0.234, 0.238, 0.25, 0.23, 0.278, 0.212, 0.238, 0.192, 0.23, 0.202, 0.218, 0.22, 0.202, 0.232, 0.248, 0.174, 0.228, 0.196, 0.2, 0.19, 0.15, 0.212, 0.202, 0.198, 0.208, 0.168, 0.218, 0.198, 0.196, 0.164, 0.204, 0.204, 0.198, 0.21, 0.2, 0.176, 0.136, 0.146, 0.102, 0.12, 0.168, 0.126, 0.15, 0.148, 0.178, 0.134, 0.168, 0.154, 0.132, 0.168, 0.156, 0.158, 0.148, 0.146, 0.154, 0.158, 0.15, 0.12, 0.202, 0.184, 0.146, 0.146, 0.214, 0.19, 0.208, 0.182, 0.168, 0.178, 0.184, 0.18, 0.194, 0.178, 0.158, 0.156, 0.156, 0.17, 0.176, 0.116, 0.184, 0.124, 0.162, 0.124, 0.15, 0.146, 0.16]\n",
      "0.00010999999999999999 50 0.5\n"
     ]
    }
   ],
   "source": [
    "learning_rate, regularization, batch_size = grid_search([0.00005,0.00015],[50,550],[0,1])\n",
    "print(learning_rate, regularization, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Evaluate best model on test set\n",
    "\n",
    "**Question 4:** Now that you have \"good\" parameter values recorded, train a new model with the best learning rate, regularization strength, and batch size values. What accuracy do you get on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 0 / 100000 . Training loss:  3.0454034704795245 .\n",
      "Completed iter 100 / 100000 . Training loss:  3.0450442509059754 .\n",
      "Completed iter 200 / 100000 . Training loss:  3.0008071610236384 .\n",
      "Completed iter 300 / 100000 . Training loss:  2.9997653808816107 .\n",
      "Completed iter 400 / 100000 . Training loss:  2.995755694697558 .\n",
      "Completed iter 500 / 100000 . Training loss:  2.966522313573195 .\n",
      "Completed iter 600 / 100000 . Training loss:  2.906663088514678 .\n",
      "Completed iter 700 / 100000 . Training loss:  2.8680626964277716 .\n",
      "Completed iter 800 / 100000 . Training loss:  2.8593133643823894 .\n",
      "Completed iter 900 / 100000 . Training loss:  2.8529173823201868 .\n",
      "Completed iter 1000 / 100000 . Training loss:  2.854506803763792 .\n",
      "Completed iter 1100 / 100000 . Training loss:  2.8419191353943893 .\n",
      "Completed iter 1200 / 100000 . Training loss:  2.8357178815570325 .\n",
      "Completed iter 1300 / 100000 . Training loss:  2.786670063001638 .\n",
      "Completed iter 1400 / 100000 . Training loss:  2.840111585555045 .\n",
      "Completed iter 1500 / 100000 . Training loss:  2.7424958881720505 .\n",
      "Completed iter 1600 / 100000 . Training loss:  2.7025027260955783 .\n",
      "Completed iter 1700 / 100000 . Training loss:  2.7318690470520597 .\n",
      "Completed iter 1800 / 100000 . Training loss:  2.765478974446577 .\n",
      "Completed iter 1900 / 100000 . Training loss:  2.6734113822435845 .\n",
      "Completed iter 2000 / 100000 . Training loss:  2.7721987831116612 .\n",
      "Completed iter 2100 / 100000 . Training loss:  2.626585615057772 .\n",
      "Completed iter 2200 / 100000 . Training loss:  2.6403775942673353 .\n",
      "Completed iter 2300 / 100000 . Training loss:  2.666549011136775 .\n",
      "Completed iter 2400 / 100000 . Training loss:  2.6418448738856943 .\n",
      "Completed iter 2500 / 100000 . Training loss:  2.6317277569134347 .\n",
      "Completed iter 2600 / 100000 . Training loss:  2.5606447577214575 .\n",
      "Completed iter 2700 / 100000 . Training loss:  2.6836813382520717 .\n",
      "Completed iter 2800 / 100000 . Training loss:  2.6237100240470133 .\n",
      "Completed iter 2900 / 100000 . Training loss:  2.6559204547338338 .\n",
      "Completed iter 3000 / 100000 . Training loss:  2.5965742788418913 .\n",
      "Completed iter 3100 / 100000 . Training loss:  2.650686794177913 .\n",
      "Completed iter 3200 / 100000 . Training loss:  2.6518048334568864 .\n",
      "Completed iter 3300 / 100000 . Training loss:  2.616165072882336 .\n",
      "Completed iter 3400 / 100000 . Training loss:  2.5377484842032763 .\n",
      "Completed iter 3500 / 100000 . Training loss:  2.4831770512446756 .\n",
      "Completed iter 3600 / 100000 . Training loss:  2.517148335472031 .\n",
      "Completed iter 3700 / 100000 . Training loss:  2.572498301301768 .\n",
      "Completed iter 3800 / 100000 . Training loss:  2.5571758364549524 .\n",
      "Completed iter 3900 / 100000 . Training loss:  2.479570136110038 .\n",
      "Completed iter 4000 / 100000 . Training loss:  2.549721649371001 .\n",
      "Completed iter 4100 / 100000 . Training loss:  2.4396590048418876 .\n",
      "Completed iter 4200 / 100000 . Training loss:  2.424873868647138 .\n",
      "Completed iter 4300 / 100000 . Training loss:  2.5623234629144895 .\n",
      "Completed iter 4400 / 100000 . Training loss:  2.529934372490478 .\n",
      "Completed iter 4500 / 100000 . Training loss:  2.4890918021595385 .\n",
      "Completed iter 4600 / 100000 . Training loss:  2.614373209208723 .\n",
      "Completed iter 4700 / 100000 . Training loss:  2.3703362599580275 .\n",
      "Completed iter 4800 / 100000 . Training loss:  2.453676194185231 .\n",
      "Completed iter 4900 / 100000 . Training loss:  2.41982828592541 .\n",
      "Completed iter 5000 / 100000 . Training loss:  2.495286683362675 .\n",
      "Completed iter 5100 / 100000 . Training loss:  2.3737927915061987 .\n",
      "Completed iter 5200 / 100000 . Training loss:  2.427994231428422 .\n",
      "Completed iter 5300 / 100000 . Training loss:  2.518449501811995 .\n",
      "Completed iter 5400 / 100000 . Training loss:  2.4848461152383914 .\n",
      "Completed iter 5500 / 100000 . Training loss:  2.4590363637056005 .\n",
      "Completed iter 5600 / 100000 . Training loss:  2.3513128108168453 .\n",
      "Completed iter 5700 / 100000 . Training loss:  2.4235893926363925 .\n",
      "Completed iter 5800 / 100000 . Training loss:  2.462674117799452 .\n",
      "Completed iter 5900 / 100000 . Training loss:  2.3366239426709123 .\n",
      "Completed iter 6000 / 100000 . Training loss:  2.412989880833364 .\n",
      "Completed iter 6100 / 100000 . Training loss:  2.438508296113411 .\n",
      "Completed iter 6200 / 100000 . Training loss:  2.448267479309309 .\n",
      "Completed iter 6300 / 100000 . Training loss:  2.302987637632131 .\n",
      "Completed iter 6400 / 100000 . Training loss:  2.3562459701145393 .\n",
      "Completed iter 6500 / 100000 . Training loss:  2.3595809707221393 .\n",
      "Completed iter 6600 / 100000 . Training loss:  2.244857865827992 .\n",
      "Completed iter 6700 / 100000 . Training loss:  2.3236215985777626 .\n",
      "Completed iter 6800 / 100000 . Training loss:  2.3130576227437296 .\n",
      "Completed iter 6900 / 100000 . Training loss:  2.358623685501858 .\n",
      "Completed iter 7000 / 100000 . Training loss:  2.3671305720038984 .\n",
      "Completed iter 7100 / 100000 . Training loss:  2.263951417367986 .\n",
      "Completed iter 7200 / 100000 . Training loss:  2.4781453171864154 .\n",
      "Completed iter 7300 / 100000 . Training loss:  2.322792717456339 .\n",
      "Completed iter 7400 / 100000 . Training loss:  2.2589647431077116 .\n",
      "Completed iter 7500 / 100000 . Training loss:  2.312021623874977 .\n",
      "Completed iter 7600 / 100000 . Training loss:  2.315524760860961 .\n",
      "Completed iter 7700 / 100000 . Training loss:  2.2142198255991783 .\n",
      "Completed iter 7800 / 100000 . Training loss:  2.2289277625919155 .\n",
      "Completed iter 7900 / 100000 . Training loss:  2.3636988020196235 .\n",
      "Completed iter 8000 / 100000 . Training loss:  2.1857209996538227 .\n",
      "Completed iter 8100 / 100000 . Training loss:  2.1898441087705063 .\n",
      "Completed iter 8200 / 100000 . Training loss:  2.239330584266943 .\n",
      "Completed iter 8300 / 100000 . Training loss:  2.2513189016985447 .\n",
      "Completed iter 8400 / 100000 . Training loss:  2.1857849722332023 .\n",
      "Completed iter 8500 / 100000 . Training loss:  2.2025248706015725 .\n",
      "Completed iter 8600 / 100000 . Training loss:  2.150597626795693 .\n",
      "Completed iter 8700 / 100000 . Training loss:  2.1614087001602575 .\n",
      "Completed iter 8800 / 100000 . Training loss:  2.261960821867992 .\n",
      "Completed iter 8900 / 100000 . Training loss:  2.181938256914796 .\n",
      "Completed iter 9000 / 100000 . Training loss:  2.2459513613127298 .\n",
      "Completed iter 9100 / 100000 . Training loss:  2.1965805489940005 .\n",
      "Completed iter 9200 / 100000 . Training loss:  2.1215847584824026 .\n",
      "Completed iter 9300 / 100000 . Training loss:  2.168304901845021 .\n",
      "Completed iter 9400 / 100000 . Training loss:  2.1296055048264737 .\n",
      "Completed iter 9500 / 100000 . Training loss:  2.1515783137016147 .\n",
      "Completed iter 9600 / 100000 . Training loss:  2.1977175940032634 .\n",
      "Completed iter 9700 / 100000 . Training loss:  2.256752789070214 .\n",
      "Completed iter 9800 / 100000 . Training loss:  2.206606001070604 .\n",
      "Completed iter 9900 / 100000 . Training loss:  2.2008143971613174 .\n",
      "Completed iter 10000 / 100000 . Training loss:  2.2954367659414188 .\n",
      "Completed iter 10100 / 100000 . Training loss:  2.169200557184258 .\n",
      "Completed iter 10200 / 100000 . Training loss:  2.096147815067015 .\n",
      "Completed iter 10300 / 100000 . Training loss:  2.02455653912525 .\n",
      "Completed iter 10400 / 100000 . Training loss:  2.078936590271398 .\n",
      "Completed iter 10500 / 100000 . Training loss:  2.164607387620328 .\n",
      "Completed iter 10600 / 100000 . Training loss:  2.178636072024917 .\n",
      "Completed iter 10700 / 100000 . Training loss:  2.2047637796330135 .\n",
      "Completed iter 10800 / 100000 . Training loss:  2.21706632485569 .\n",
      "Completed iter 10900 / 100000 . Training loss:  2.098196778828536 .\n",
      "Completed iter 11000 / 100000 . Training loss:  2.2677024387819333 .\n",
      "Completed iter 11100 / 100000 . Training loss:  2.061147532518529 .\n",
      "Completed iter 11200 / 100000 . Training loss:  2.1211303377395856 .\n",
      "Completed iter 11300 / 100000 . Training loss:  2.1829770189443973 .\n",
      "Completed iter 11400 / 100000 . Training loss:  2.1560617269960116 .\n",
      "Completed iter 11500 / 100000 . Training loss:  2.215324119904001 .\n",
      "Completed iter 11600 / 100000 . Training loss:  2.0250598044613004 .\n",
      "Completed iter 11700 / 100000 . Training loss:  2.1232047404843923 .\n",
      "Completed iter 11800 / 100000 . Training loss:  2.130396216814688 .\n",
      "Completed iter 11900 / 100000 . Training loss:  2.178402566752629 .\n",
      "Completed iter 12000 / 100000 . Training loss:  1.9701489316037368 .\n",
      "Completed iter 12100 / 100000 . Training loss:  2.100847594543497 .\n",
      "Completed iter 12200 / 100000 . Training loss:  2.086390186312738 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 12300 / 100000 . Training loss:  2.083155718124655 .\n",
      "Completed iter 12400 / 100000 . Training loss:  2.0872015672401316 .\n",
      "Completed iter 12500 / 100000 . Training loss:  2.1128734973041605 .\n",
      "Completed iter 12600 / 100000 . Training loss:  2.2960628605883713 .\n",
      "Completed iter 12700 / 100000 . Training loss:  2.186617183687071 .\n",
      "Completed iter 12800 / 100000 . Training loss:  1.9915244434734336 .\n",
      "Completed iter 12900 / 100000 . Training loss:  2.1941469476371 .\n",
      "Completed iter 13000 / 100000 . Training loss:  2.2362468554138495 .\n",
      "Completed iter 13100 / 100000 . Training loss:  2.0867032144024193 .\n",
      "Completed iter 13200 / 100000 . Training loss:  2.2423025893184745 .\n",
      "Completed iter 13300 / 100000 . Training loss:  2.1009282628351573 .\n",
      "Completed iter 13400 / 100000 . Training loss:  2.101060090352491 .\n",
      "Completed iter 13500 / 100000 . Training loss:  2.090668818287221 .\n",
      "Completed iter 13600 / 100000 . Training loss:  2.0841526836408493 .\n",
      "Completed iter 13700 / 100000 . Training loss:  2.1596017545835986 .\n",
      "Completed iter 13800 / 100000 . Training loss:  2.063790925799191 .\n",
      "Completed iter 13900 / 100000 . Training loss:  2.122885430324563 .\n",
      "Completed iter 14000 / 100000 . Training loss:  2.1043185977843004 .\n",
      "Completed iter 14100 / 100000 . Training loss:  1.9999187398020657 .\n",
      "Completed iter 14200 / 100000 . Training loss:  2.123331262826679 .\n",
      "Completed iter 14300 / 100000 . Training loss:  2.0446665698450084 .\n",
      "Completed iter 14400 / 100000 . Training loss:  2.0044289083731837 .\n",
      "Completed iter 14500 / 100000 . Training loss:  2.1149435804480516 .\n",
      "Completed iter 14600 / 100000 . Training loss:  2.0785121242613847 .\n",
      "Completed iter 14700 / 100000 . Training loss:  2.027010126302987 .\n",
      "Completed iter 14800 / 100000 . Training loss:  2.1323845988430046 .\n",
      "Completed iter 14900 / 100000 . Training loss:  2.0592433632041045 .\n",
      "Completed iter 15000 / 100000 . Training loss:  2.190314452042218 .\n",
      "Completed iter 15100 / 100000 . Training loss:  1.9995538649640563 .\n",
      "Completed iter 15200 / 100000 . Training loss:  2.051987176467631 .\n",
      "Completed iter 15300 / 100000 . Training loss:  2.0326922854495177 .\n",
      "Completed iter 15400 / 100000 . Training loss:  2.080224972818882 .\n",
      "Completed iter 15500 / 100000 . Training loss:  2.118324777403475 .\n",
      "Completed iter 15600 / 100000 . Training loss:  2.016119660251956 .\n",
      "Completed iter 15700 / 100000 . Training loss:  1.9397198342994648 .\n",
      "Completed iter 15800 / 100000 . Training loss:  2.0324273030102473 .\n",
      "Completed iter 15900 / 100000 . Training loss:  2.023935725234372 .\n",
      "Completed iter 16000 / 100000 . Training loss:  1.9325469646993993 .\n",
      "Completed iter 16100 / 100000 . Training loss:  1.9410027157982706 .\n",
      "Completed iter 16200 / 100000 . Training loss:  1.968590664046589 .\n",
      "Completed iter 16300 / 100000 . Training loss:  1.9699381182972158 .\n",
      "Completed iter 16400 / 100000 . Training loss:  2.1399625664902233 .\n",
      "Completed iter 16500 / 100000 . Training loss:  2.1483087776006853 .\n",
      "Completed iter 16600 / 100000 . Training loss:  2.096663050040053 .\n",
      "Completed iter 16700 / 100000 . Training loss:  2.0109509899809894 .\n",
      "Completed iter 16800 / 100000 . Training loss:  2.0651372519311146 .\n",
      "Completed iter 16900 / 100000 . Training loss:  2.153338151413339 .\n",
      "Completed iter 17000 / 100000 . Training loss:  2.033419216678646 .\n",
      "Completed iter 17100 / 100000 . Training loss:  1.9820213598944498 .\n",
      "Completed iter 17200 / 100000 . Training loss:  2.098646146627213 .\n",
      "Completed iter 17300 / 100000 . Training loss:  1.981014442623139 .\n",
      "Completed iter 17400 / 100000 . Training loss:  2.0324511341581033 .\n",
      "Completed iter 17500 / 100000 . Training loss:  2.0094132879647 .\n",
      "Completed iter 17600 / 100000 . Training loss:  2.0415688472759275 .\n",
      "Completed iter 17700 / 100000 . Training loss:  2.043830321460432 .\n",
      "Completed iter 17800 / 100000 . Training loss:  2.03219421511832 .\n",
      "Completed iter 17900 / 100000 . Training loss:  1.942055289123553 .\n",
      "Completed iter 18000 / 100000 . Training loss:  2.0811261128133296 .\n",
      "Completed iter 18100 / 100000 . Training loss:  1.9755237625566233 .\n",
      "Completed iter 18200 / 100000 . Training loss:  1.9221359533067777 .\n",
      "Completed iter 18300 / 100000 . Training loss:  1.940006205815029 .\n",
      "Completed iter 18400 / 100000 . Training loss:  2.039672760425965 .\n",
      "Completed iter 18500 / 100000 . Training loss:  1.92981426549151 .\n",
      "Completed iter 18600 / 100000 . Training loss:  2.1355089401151135 .\n",
      "Completed iter 18700 / 100000 . Training loss:  2.0560230862645676 .\n",
      "Completed iter 18800 / 100000 . Training loss:  1.9450486056767602 .\n",
      "Completed iter 18900 / 100000 . Training loss:  2.0390377106268547 .\n",
      "Completed iter 19000 / 100000 . Training loss:  1.8875245471178537 .\n",
      "Completed iter 19100 / 100000 . Training loss:  1.8836620538643989 .\n",
      "Completed iter 19200 / 100000 . Training loss:  2.022092186782715 .\n",
      "Completed iter 19300 / 100000 . Training loss:  2.0779972431690097 .\n",
      "Completed iter 19400 / 100000 . Training loss:  1.9294600116621679 .\n",
      "Completed iter 19500 / 100000 . Training loss:  1.9467316864365911 .\n",
      "Completed iter 19600 / 100000 . Training loss:  2.1017740623884698 .\n",
      "Completed iter 19700 / 100000 . Training loss:  1.8815943034699654 .\n",
      "Completed iter 19800 / 100000 . Training loss:  1.9951057118519415 .\n",
      "Completed iter 19900 / 100000 . Training loss:  2.050074958762089 .\n",
      "Completed iter 20000 / 100000 . Training loss:  2.0287208939741403 .\n",
      "Completed iter 20100 / 100000 . Training loss:  1.913407549591561 .\n",
      "Completed iter 20200 / 100000 . Training loss:  1.9120742682929481 .\n",
      "Completed iter 20300 / 100000 . Training loss:  2.0943066295860566 .\n",
      "Completed iter 20400 / 100000 . Training loss:  2.0377656212634636 .\n",
      "Completed iter 20500 / 100000 . Training loss:  1.906635434869655 .\n",
      "Completed iter 20600 / 100000 . Training loss:  1.9639726285881638 .\n",
      "Completed iter 20700 / 100000 . Training loss:  2.064105853629348 .\n",
      "Completed iter 20800 / 100000 . Training loss:  1.9926716271356666 .\n",
      "Completed iter 20900 / 100000 . Training loss:  1.9968808783041414 .\n",
      "Completed iter 21000 / 100000 . Training loss:  1.9574674463939004 .\n",
      "Completed iter 21100 / 100000 . Training loss:  2.072290714022004 .\n",
      "Completed iter 21200 / 100000 . Training loss:  1.9967448426804335 .\n",
      "Completed iter 21300 / 100000 . Training loss:  1.9891788157779648 .\n",
      "Completed iter 21400 / 100000 . Training loss:  1.9942421524421523 .\n",
      "Completed iter 21500 / 100000 . Training loss:  1.986372413617442 .\n",
      "Completed iter 21600 / 100000 . Training loss:  1.8633282446176809 .\n",
      "Completed iter 21700 / 100000 . Training loss:  1.8928527087597995 .\n",
      "Completed iter 21800 / 100000 . Training loss:  2.112753432285344 .\n",
      "Completed iter 21900 / 100000 . Training loss:  1.8856291916727193 .\n",
      "Completed iter 22000 / 100000 . Training loss:  1.8305595074105105 .\n",
      "Completed iter 22100 / 100000 . Training loss:  2.0225420635578035 .\n",
      "Completed iter 22200 / 100000 . Training loss:  1.9827449893410303 .\n",
      "Completed iter 22300 / 100000 . Training loss:  1.897646894880852 .\n",
      "Completed iter 22400 / 100000 . Training loss:  1.8576740840644257 .\n",
      "Completed iter 22500 / 100000 . Training loss:  2.0301163353048235 .\n",
      "Completed iter 22600 / 100000 . Training loss:  1.9358424308785638 .\n",
      "Completed iter 22700 / 100000 . Training loss:  1.9663983935624754 .\n",
      "Completed iter 22800 / 100000 . Training loss:  1.826258935587452 .\n",
      "Completed iter 22900 / 100000 . Training loss:  1.8623416206589822 .\n",
      "Completed iter 23000 / 100000 . Training loss:  1.9186457241544383 .\n",
      "Completed iter 23100 / 100000 . Training loss:  1.9653411564554688 .\n",
      "Completed iter 23200 / 100000 . Training loss:  2.038203540111914 .\n",
      "Completed iter 23300 / 100000 . Training loss:  1.9357543744777064 .\n",
      "Completed iter 23400 / 100000 . Training loss:  1.981075515605736 .\n",
      "Completed iter 23500 / 100000 . Training loss:  2.058872690374522 .\n",
      "Completed iter 23600 / 100000 . Training loss:  1.905602868340611 .\n",
      "Completed iter 23700 / 100000 . Training loss:  1.9351603508410424 .\n",
      "Completed iter 23800 / 100000 . Training loss:  1.8144012016901636 .\n",
      "Completed iter 23900 / 100000 . Training loss:  1.9739566684933894 .\n",
      "Completed iter 24000 / 100000 . Training loss:  2.0751269653256106 .\n",
      "Completed iter 24100 / 100000 . Training loss:  1.9148674082722321 .\n",
      "Completed iter 24200 / 100000 . Training loss:  1.9581733756401356 .\n",
      "Completed iter 24300 / 100000 . Training loss:  2.091658492763705 .\n",
      "Completed iter 24400 / 100000 . Training loss:  1.9981991037266198 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 24500 / 100000 . Training loss:  2.015933965841375 .\n",
      "Completed iter 24600 / 100000 . Training loss:  1.9052218923222743 .\n",
      "Completed iter 24700 / 100000 . Training loss:  2.0132732494333108 .\n",
      "Completed iter 24800 / 100000 . Training loss:  2.0790508640296856 .\n",
      "Completed iter 24900 / 100000 . Training loss:  1.8788876240758272 .\n",
      "Completed iter 25000 / 100000 . Training loss:  1.9212613907342009 .\n",
      "Completed iter 25100 / 100000 . Training loss:  1.8848985671017984 .\n",
      "Completed iter 25200 / 100000 . Training loss:  1.9495857206267015 .\n",
      "Completed iter 25300 / 100000 . Training loss:  2.097414257474546 .\n",
      "Completed iter 25400 / 100000 . Training loss:  1.9167221012138973 .\n",
      "Completed iter 25500 / 100000 . Training loss:  1.9937920727707104 .\n",
      "Completed iter 25600 / 100000 . Training loss:  1.8950396926928041 .\n",
      "Completed iter 25700 / 100000 . Training loss:  1.9438023751959248 .\n",
      "Completed iter 25800 / 100000 . Training loss:  1.8379127291062993 .\n",
      "Completed iter 25900 / 100000 . Training loss:  1.9793445255973743 .\n",
      "Completed iter 26000 / 100000 . Training loss:  1.9731488032560258 .\n",
      "Completed iter 26100 / 100000 . Training loss:  1.9172759574801563 .\n",
      "Completed iter 26200 / 100000 . Training loss:  1.9570879251698479 .\n",
      "Completed iter 26300 / 100000 . Training loss:  1.9215293436235057 .\n",
      "Completed iter 26400 / 100000 . Training loss:  1.9409926656135918 .\n",
      "Completed iter 26500 / 100000 . Training loss:  1.956451947111383 .\n",
      "Completed iter 26600 / 100000 . Training loss:  1.8383551273072303 .\n",
      "Completed iter 26700 / 100000 . Training loss:  1.8597541429341438 .\n",
      "Completed iter 26800 / 100000 . Training loss:  2.0218655878735707 .\n",
      "Completed iter 26900 / 100000 . Training loss:  1.926069803572843 .\n",
      "Completed iter 27000 / 100000 . Training loss:  1.8267642714041046 .\n",
      "Completed iter 27100 / 100000 . Training loss:  1.934590407635299 .\n",
      "Completed iter 27200 / 100000 . Training loss:  2.034834696283777 .\n",
      "Completed iter 27300 / 100000 . Training loss:  1.9609966490833846 .\n",
      "Completed iter 27400 / 100000 . Training loss:  1.8107624111406189 .\n",
      "Completed iter 27500 / 100000 . Training loss:  1.8951511785034372 .\n",
      "Completed iter 27600 / 100000 . Training loss:  2.087715173691436 .\n",
      "Completed iter 27700 / 100000 . Training loss:  1.9556473931621696 .\n",
      "Completed iter 27800 / 100000 . Training loss:  1.7886140059737818 .\n",
      "Completed iter 27900 / 100000 . Training loss:  1.9356655317912486 .\n",
      "Completed iter 28000 / 100000 . Training loss:  1.8151236368022792 .\n",
      "Completed iter 28100 / 100000 . Training loss:  1.8822881200889294 .\n",
      "Completed iter 28200 / 100000 . Training loss:  1.8864754493744202 .\n",
      "Completed iter 28300 / 100000 . Training loss:  1.9155111428127036 .\n",
      "Completed iter 28400 / 100000 . Training loss:  2.002770885871345 .\n",
      "Completed iter 28500 / 100000 . Training loss:  1.829280027029305 .\n",
      "Completed iter 28600 / 100000 . Training loss:  1.8937433181389793 .\n",
      "Completed iter 28700 / 100000 . Training loss:  1.9989774666835336 .\n",
      "Completed iter 28800 / 100000 . Training loss:  1.970405238771307 .\n",
      "Completed iter 28900 / 100000 . Training loss:  1.8307589203358614 .\n",
      "Completed iter 29000 / 100000 . Training loss:  1.9116640439887609 .\n",
      "Completed iter 29100 / 100000 . Training loss:  1.8558966423780459 .\n",
      "Completed iter 29200 / 100000 . Training loss:  1.9640658527556742 .\n",
      "Completed iter 29300 / 100000 . Training loss:  1.8607545058908144 .\n",
      "Completed iter 29400 / 100000 . Training loss:  1.878322082383737 .\n",
      "Completed iter 29500 / 100000 . Training loss:  1.9914651961226015 .\n",
      "Completed iter 29600 / 100000 . Training loss:  1.876001922649803 .\n",
      "Completed iter 29700 / 100000 . Training loss:  1.821894877234298 .\n",
      "Completed iter 29800 / 100000 . Training loss:  1.8395149299255702 .\n",
      "Completed iter 29900 / 100000 . Training loss:  1.9570451694649502 .\n",
      "Completed iter 30000 / 100000 . Training loss:  1.8944289354252792 .\n",
      "Completed iter 30100 / 100000 . Training loss:  1.944608456509575 .\n",
      "Completed iter 30200 / 100000 . Training loss:  1.9627058435197058 .\n",
      "Completed iter 30300 / 100000 . Training loss:  1.986422639275697 .\n",
      "Completed iter 30400 / 100000 . Training loss:  1.91636338112581 .\n",
      "Completed iter 30500 / 100000 . Training loss:  1.898953210769545 .\n",
      "Completed iter 30600 / 100000 . Training loss:  1.8565143086513711 .\n",
      "Completed iter 30700 / 100000 . Training loss:  1.8641896428982305 .\n",
      "Completed iter 30800 / 100000 . Training loss:  1.8705777781157906 .\n",
      "Completed iter 30900 / 100000 . Training loss:  1.9894191815715743 .\n",
      "Completed iter 31000 / 100000 . Training loss:  1.9604631204214658 .\n",
      "Completed iter 31100 / 100000 . Training loss:  2.0255917606178055 .\n",
      "Completed iter 31200 / 100000 . Training loss:  1.8258589802698517 .\n",
      "Completed iter 31300 / 100000 . Training loss:  1.961460109603836 .\n",
      "Completed iter 31400 / 100000 . Training loss:  1.9238298611336397 .\n",
      "Completed iter 31500 / 100000 . Training loss:  1.9229857931907213 .\n",
      "Completed iter 31600 / 100000 . Training loss:  1.8626165129997998 .\n",
      "Completed iter 31700 / 100000 . Training loss:  1.9522327355041176 .\n",
      "Completed iter 31800 / 100000 . Training loss:  1.9491991490822491 .\n",
      "Completed iter 31900 / 100000 . Training loss:  1.9817993859253975 .\n",
      "Completed iter 32000 / 100000 . Training loss:  1.8140523295914732 .\n",
      "Completed iter 32100 / 100000 . Training loss:  1.861600297087703 .\n",
      "Completed iter 32200 / 100000 . Training loss:  1.839798621796858 .\n",
      "Completed iter 32300 / 100000 . Training loss:  1.8416451445719408 .\n",
      "Completed iter 32400 / 100000 . Training loss:  2.102460357893792 .\n",
      "Completed iter 32500 / 100000 . Training loss:  1.932781580641287 .\n",
      "Completed iter 32600 / 100000 . Training loss:  1.9501128366317948 .\n",
      "Completed iter 32700 / 100000 . Training loss:  1.9417895564096002 .\n",
      "Completed iter 32800 / 100000 . Training loss:  1.8901416942070761 .\n",
      "Completed iter 32900 / 100000 . Training loss:  1.9761045774744015 .\n",
      "Completed iter 33000 / 100000 . Training loss:  1.8239534524612129 .\n",
      "Completed iter 33100 / 100000 . Training loss:  2.023749390861368 .\n",
      "Completed iter 33200 / 100000 . Training loss:  1.9159893667043502 .\n",
      "Completed iter 33300 / 100000 . Training loss:  2.040125376082075 .\n",
      "Completed iter 33400 / 100000 . Training loss:  1.8870247667114552 .\n",
      "Completed iter 33500 / 100000 . Training loss:  1.9320588115023345 .\n",
      "Completed iter 33600 / 100000 . Training loss:  1.942970064667629 .\n",
      "Completed iter 33700 / 100000 . Training loss:  1.833164047022896 .\n",
      "Completed iter 33800 / 100000 . Training loss:  1.8290426279028298 .\n",
      "Completed iter 33900 / 100000 . Training loss:  1.9407398471183126 .\n",
      "Completed iter 34000 / 100000 . Training loss:  1.9264682364264574 .\n",
      "Completed iter 34100 / 100000 . Training loss:  1.9767662529181074 .\n",
      "Completed iter 34200 / 100000 . Training loss:  2.040253149753872 .\n",
      "Completed iter 34300 / 100000 . Training loss:  1.896313173825761 .\n",
      "Completed iter 34400 / 100000 . Training loss:  1.8495972560225158 .\n",
      "Completed iter 34500 / 100000 . Training loss:  1.8162243825907256 .\n",
      "Completed iter 34600 / 100000 . Training loss:  1.7521347227972426 .\n",
      "Completed iter 34700 / 100000 . Training loss:  1.9335565971848097 .\n",
      "Completed iter 34800 / 100000 . Training loss:  1.934406019658408 .\n",
      "Completed iter 34900 / 100000 . Training loss:  1.9810838444928185 .\n",
      "Completed iter 35000 / 100000 . Training loss:  1.8868303735134013 .\n",
      "Completed iter 35100 / 100000 . Training loss:  1.89212223785021 .\n",
      "Completed iter 35200 / 100000 . Training loss:  1.9678721584756573 .\n",
      "Completed iter 35300 / 100000 . Training loss:  1.8321982194259685 .\n",
      "Completed iter 35400 / 100000 . Training loss:  1.8050417149615514 .\n",
      "Completed iter 35500 / 100000 . Training loss:  1.9177362216279907 .\n",
      "Completed iter 35600 / 100000 . Training loss:  1.929538265741415 .\n",
      "Completed iter 35700 / 100000 . Training loss:  1.899700283661917 .\n",
      "Completed iter 35800 / 100000 . Training loss:  1.9883081357355719 .\n",
      "Completed iter 35900 / 100000 . Training loss:  2.0383754349949257 .\n",
      "Completed iter 36000 / 100000 . Training loss:  1.9949701012104577 .\n",
      "Completed iter 36100 / 100000 . Training loss:  1.8817905776037442 .\n",
      "Completed iter 36200 / 100000 . Training loss:  1.8388621612339853 .\n",
      "Completed iter 36300 / 100000 . Training loss:  1.93782100278373 .\n",
      "Completed iter 36400 / 100000 . Training loss:  1.8596550657407647 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 36500 / 100000 . Training loss:  1.7071952999034934 .\n",
      "Completed iter 36600 / 100000 . Training loss:  1.8314608777038812 .\n",
      "Completed iter 36700 / 100000 . Training loss:  2.0661820056322218 .\n",
      "Completed iter 36800 / 100000 . Training loss:  1.910747734591559 .\n",
      "Completed iter 36900 / 100000 . Training loss:  1.969951498772691 .\n",
      "Completed iter 37000 / 100000 . Training loss:  1.7945211930035114 .\n",
      "Completed iter 37100 / 100000 . Training loss:  1.9346928264950807 .\n",
      "Completed iter 37200 / 100000 . Training loss:  1.960993152299326 .\n",
      "Completed iter 37300 / 100000 . Training loss:  1.972027323217511 .\n",
      "Completed iter 37400 / 100000 . Training loss:  1.7933226174542014 .\n",
      "Completed iter 37500 / 100000 . Training loss:  1.937575203620705 .\n",
      "Completed iter 37600 / 100000 . Training loss:  1.8516917077688986 .\n",
      "Completed iter 37700 / 100000 . Training loss:  1.995257157556645 .\n",
      "Completed iter 37800 / 100000 . Training loss:  1.8484898553426579 .\n",
      "Completed iter 37900 / 100000 . Training loss:  1.8248288236779318 .\n",
      "Completed iter 38000 / 100000 . Training loss:  1.8313332800812612 .\n",
      "Completed iter 38100 / 100000 . Training loss:  1.8827025723880488 .\n",
      "Completed iter 38200 / 100000 . Training loss:  1.7873635204145697 .\n",
      "Completed iter 38300 / 100000 . Training loss:  1.8556854632764974 .\n",
      "Completed iter 38400 / 100000 . Training loss:  1.906773044720836 .\n",
      "Completed iter 38500 / 100000 . Training loss:  1.845657107633367 .\n",
      "Completed iter 38600 / 100000 . Training loss:  1.8363921491250603 .\n",
      "Completed iter 38700 / 100000 . Training loss:  2.023837783111614 .\n",
      "Completed iter 38800 / 100000 . Training loss:  1.9438737939227946 .\n",
      "Completed iter 38900 / 100000 . Training loss:  1.804036481982235 .\n",
      "Completed iter 39000 / 100000 . Training loss:  1.9517714782063007 .\n",
      "Completed iter 39100 / 100000 . Training loss:  1.9542079245174846 .\n",
      "Completed iter 39200 / 100000 . Training loss:  2.127452996348296 .\n",
      "Completed iter 39300 / 100000 . Training loss:  1.940513713787448 .\n",
      "Completed iter 39400 / 100000 . Training loss:  1.8971556964129606 .\n",
      "Completed iter 39500 / 100000 . Training loss:  1.8192391185500587 .\n",
      "Completed iter 39600 / 100000 . Training loss:  1.7536054205866214 .\n",
      "Completed iter 39700 / 100000 . Training loss:  1.9296080926803854 .\n",
      "Completed iter 39800 / 100000 . Training loss:  1.7951095854998007 .\n",
      "Completed iter 39900 / 100000 . Training loss:  1.8655837439460996 .\n",
      "Completed iter 40000 / 100000 . Training loss:  2.016482985707695 .\n",
      "Completed iter 40100 / 100000 . Training loss:  1.8006149361763093 .\n",
      "Completed iter 40200 / 100000 . Training loss:  1.907329178283883 .\n",
      "Completed iter 40300 / 100000 . Training loss:  1.8961694009274574 .\n",
      "Completed iter 40400 / 100000 . Training loss:  1.894705809203676 .\n",
      "Completed iter 40500 / 100000 . Training loss:  1.937287158794062 .\n",
      "Completed iter 40600 / 100000 . Training loss:  1.8987410449925206 .\n",
      "Completed iter 40700 / 100000 . Training loss:  1.8485523194675868 .\n",
      "Completed iter 40800 / 100000 . Training loss:  1.9797904106775643 .\n",
      "Completed iter 40900 / 100000 . Training loss:  1.8692268052502532 .\n",
      "Completed iter 41000 / 100000 . Training loss:  1.801812900586459 .\n",
      "Completed iter 41100 / 100000 . Training loss:  1.8830906369102212 .\n",
      "Completed iter 41200 / 100000 . Training loss:  1.979797411610168 .\n",
      "Completed iter 41300 / 100000 . Training loss:  1.8617248793069854 .\n",
      "Completed iter 41400 / 100000 . Training loss:  1.937622479694039 .\n",
      "Completed iter 41500 / 100000 . Training loss:  1.804654556542491 .\n",
      "Completed iter 41600 / 100000 . Training loss:  1.9410419489164736 .\n",
      "Completed iter 41700 / 100000 . Training loss:  1.9384803715489576 .\n",
      "Completed iter 41800 / 100000 . Training loss:  1.9791643049382994 .\n",
      "Completed iter 41900 / 100000 . Training loss:  1.8095371362905324 .\n",
      "Completed iter 42000 / 100000 . Training loss:  1.8334566453325587 .\n",
      "Completed iter 42100 / 100000 . Training loss:  1.9227515437181615 .\n",
      "Completed iter 42200 / 100000 . Training loss:  1.8531579366999607 .\n",
      "Completed iter 42300 / 100000 . Training loss:  1.9879953836956221 .\n",
      "Completed iter 42400 / 100000 . Training loss:  2.077352786649012 .\n",
      "Completed iter 42500 / 100000 . Training loss:  1.7709365150886343 .\n",
      "Completed iter 42600 / 100000 . Training loss:  1.8811994841798334 .\n",
      "Completed iter 42700 / 100000 . Training loss:  1.8336284148020185 .\n",
      "Completed iter 42800 / 100000 . Training loss:  1.7651036962178044 .\n",
      "Completed iter 42900 / 100000 . Training loss:  1.9629139623098386 .\n",
      "Completed iter 43000 / 100000 . Training loss:  1.9363918636124444 .\n",
      "Completed iter 43100 / 100000 . Training loss:  1.9185583655003404 .\n",
      "Completed iter 43200 / 100000 . Training loss:  1.9055601840216103 .\n",
      "Completed iter 43300 / 100000 . Training loss:  1.7906668595124426 .\n",
      "Completed iter 43400 / 100000 . Training loss:  1.8144858097694911 .\n",
      "Completed iter 43500 / 100000 . Training loss:  1.8310205143143703 .\n",
      "Completed iter 43600 / 100000 . Training loss:  1.9133641381306523 .\n",
      "Completed iter 43700 / 100000 . Training loss:  1.8708736938443442 .\n",
      "Completed iter 43800 / 100000 . Training loss:  1.8709587737365412 .\n",
      "Completed iter 43900 / 100000 . Training loss:  1.935262969575808 .\n",
      "Completed iter 44000 / 100000 . Training loss:  1.8559793398488444 .\n",
      "Completed iter 44100 / 100000 . Training loss:  1.9509745432819312 .\n",
      "Completed iter 44200 / 100000 . Training loss:  1.9045930408510083 .\n",
      "Completed iter 44300 / 100000 . Training loss:  1.961471296912189 .\n",
      "Completed iter 44400 / 100000 . Training loss:  1.964273324305827 .\n",
      "Completed iter 44500 / 100000 . Training loss:  1.8611845022716684 .\n",
      "Completed iter 44600 / 100000 . Training loss:  1.8817824104207608 .\n",
      "Completed iter 44700 / 100000 . Training loss:  1.9662381862223932 .\n",
      "Completed iter 44800 / 100000 . Training loss:  2.004726437439479 .\n",
      "Completed iter 44900 / 100000 . Training loss:  1.937788764495508 .\n",
      "Completed iter 45000 / 100000 . Training loss:  1.8940242370160174 .\n",
      "Completed iter 45100 / 100000 . Training loss:  1.8979214231853694 .\n",
      "Completed iter 45200 / 100000 . Training loss:  1.8202526111569097 .\n",
      "Completed iter 45300 / 100000 . Training loss:  1.9261887699511087 .\n",
      "Completed iter 45400 / 100000 . Training loss:  1.9051794756469076 .\n",
      "Completed iter 45500 / 100000 . Training loss:  1.8665729643344218 .\n",
      "Completed iter 45600 / 100000 . Training loss:  1.9641345020539946 .\n",
      "Completed iter 45700 / 100000 . Training loss:  1.8006686981667634 .\n",
      "Completed iter 45800 / 100000 . Training loss:  1.8650286533098004 .\n",
      "Completed iter 45900 / 100000 . Training loss:  1.865147037697859 .\n",
      "Completed iter 46000 / 100000 . Training loss:  1.909811980490948 .\n",
      "Completed iter 46100 / 100000 . Training loss:  1.8541457226884674 .\n",
      "Completed iter 46200 / 100000 . Training loss:  1.9534840453302538 .\n",
      "Completed iter 46300 / 100000 . Training loss:  1.9943058498467778 .\n",
      "Completed iter 46400 / 100000 . Training loss:  1.872670534125437 .\n",
      "Completed iter 46500 / 100000 . Training loss:  1.9173544322931797 .\n",
      "Completed iter 46600 / 100000 . Training loss:  1.967994325749601 .\n",
      "Completed iter 46700 / 100000 . Training loss:  1.9283384699441086 .\n",
      "Completed iter 46800 / 100000 . Training loss:  1.9266038841278712 .\n",
      "Completed iter 46900 / 100000 . Training loss:  1.9088046388588622 .\n",
      "Completed iter 47000 / 100000 . Training loss:  1.9592993279860365 .\n",
      "Completed iter 47100 / 100000 . Training loss:  1.9759107801637803 .\n",
      "Completed iter 47200 / 100000 . Training loss:  1.9264844233876126 .\n",
      "Completed iter 47300 / 100000 . Training loss:  1.862809006515452 .\n",
      "Completed iter 47400 / 100000 . Training loss:  1.9054335907349573 .\n",
      "Completed iter 47500 / 100000 . Training loss:  1.9140366674740599 .\n",
      "Completed iter 47600 / 100000 . Training loss:  1.8751378261025944 .\n",
      "Completed iter 47700 / 100000 . Training loss:  1.888973128718203 .\n",
      "Completed iter 47800 / 100000 . Training loss:  1.9796778918392173 .\n",
      "Completed iter 47900 / 100000 . Training loss:  1.9401595085028993 .\n",
      "Completed iter 48000 / 100000 . Training loss:  1.9856759405875162 .\n",
      "Completed iter 48100 / 100000 . Training loss:  1.923257893103143 .\n",
      "Completed iter 48200 / 100000 . Training loss:  1.9484753663000096 .\n",
      "Completed iter 48300 / 100000 . Training loss:  1.8670678153933473 .\n",
      "Completed iter 48400 / 100000 . Training loss:  1.972080372267266 .\n",
      "Completed iter 48500 / 100000 . Training loss:  1.941653432406047 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 48600 / 100000 . Training loss:  1.6399763099835116 .\n",
      "Completed iter 48700 / 100000 . Training loss:  2.0349749347904265 .\n",
      "Completed iter 48800 / 100000 . Training loss:  1.9057426245243239 .\n",
      "Completed iter 48900 / 100000 . Training loss:  2.0555736035716112 .\n",
      "Completed iter 49000 / 100000 . Training loss:  1.872622724122031 .\n",
      "Completed iter 49100 / 100000 . Training loss:  1.9101989191937894 .\n",
      "Completed iter 49200 / 100000 . Training loss:  1.8200212179676545 .\n",
      "Completed iter 49300 / 100000 . Training loss:  1.888079106919969 .\n",
      "Completed iter 49400 / 100000 . Training loss:  1.9048068485007925 .\n",
      "Completed iter 49500 / 100000 . Training loss:  1.9003040369233246 .\n",
      "Completed iter 49600 / 100000 . Training loss:  1.9268644372913224 .\n",
      "Completed iter 49700 / 100000 . Training loss:  1.9172653062704135 .\n",
      "Completed iter 49800 / 100000 . Training loss:  1.8874350914379694 .\n",
      "Completed iter 49900 / 100000 . Training loss:  1.8817067882336214 .\n",
      "Completed iter 50000 / 100000 . Training loss:  1.9090313522547722 .\n",
      "Completed iter 50100 / 100000 . Training loss:  1.8678960116119092 .\n",
      "Completed iter 50200 / 100000 . Training loss:  1.9864044428950172 .\n",
      "Completed iter 50300 / 100000 . Training loss:  1.8438060997289938 .\n",
      "Completed iter 50400 / 100000 . Training loss:  1.9334667246748878 .\n",
      "Completed iter 50500 / 100000 . Training loss:  1.850343817938789 .\n",
      "Completed iter 50600 / 100000 . Training loss:  1.871157328386186 .\n",
      "Completed iter 50700 / 100000 . Training loss:  1.960038740923622 .\n",
      "Completed iter 50800 / 100000 . Training loss:  1.9396416684492193 .\n",
      "Completed iter 50900 / 100000 . Training loss:  1.866235607544772 .\n",
      "Completed iter 51000 / 100000 . Training loss:  1.9022189885765834 .\n",
      "Completed iter 51100 / 100000 . Training loss:  1.8827055981507665 .\n",
      "Completed iter 51200 / 100000 . Training loss:  1.9275708307091408 .\n",
      "Completed iter 51300 / 100000 . Training loss:  1.9678819760856354 .\n",
      "Completed iter 51400 / 100000 . Training loss:  1.9439964929475342 .\n",
      "Completed iter 51500 / 100000 . Training loss:  1.9184273260683424 .\n",
      "Completed iter 51600 / 100000 . Training loss:  1.875946079160989 .\n",
      "Completed iter 51700 / 100000 . Training loss:  1.866179343271472 .\n",
      "Completed iter 51800 / 100000 . Training loss:  1.8249513107945616 .\n",
      "Completed iter 51900 / 100000 . Training loss:  1.782888756910983 .\n",
      "Completed iter 52000 / 100000 . Training loss:  1.9295020777117182 .\n",
      "Completed iter 52100 / 100000 . Training loss:  1.8807574465563033 .\n",
      "Completed iter 52200 / 100000 . Training loss:  1.9897809271760816 .\n",
      "Completed iter 52300 / 100000 . Training loss:  2.0304820711584584 .\n",
      "Completed iter 52400 / 100000 . Training loss:  1.9234054566573586 .\n",
      "Completed iter 52500 / 100000 . Training loss:  2.009627790350786 .\n",
      "Completed iter 52600 / 100000 . Training loss:  1.974603923445252 .\n",
      "Completed iter 52700 / 100000 . Training loss:  1.8603862642278597 .\n",
      "Completed iter 52800 / 100000 . Training loss:  1.8576456163039945 .\n",
      "Completed iter 52900 / 100000 . Training loss:  1.869265394160278 .\n",
      "Completed iter 53000 / 100000 . Training loss:  1.888392400496452 .\n",
      "Completed iter 53100 / 100000 . Training loss:  1.900396644978938 .\n",
      "Completed iter 53200 / 100000 . Training loss:  1.8703193462875998 .\n",
      "Completed iter 53300 / 100000 . Training loss:  1.9404756065947817 .\n",
      "Completed iter 53400 / 100000 . Training loss:  1.971214690570466 .\n",
      "Completed iter 53500 / 100000 . Training loss:  1.9849796134128652 .\n",
      "Completed iter 53600 / 100000 . Training loss:  1.8870859909863371 .\n",
      "Completed iter 53700 / 100000 . Training loss:  1.944676787480079 .\n",
      "Completed iter 53800 / 100000 . Training loss:  2.0074112351908577 .\n",
      "Completed iter 53900 / 100000 . Training loss:  1.9283639757264173 .\n",
      "Completed iter 54000 / 100000 . Training loss:  1.9108950805002636 .\n",
      "Completed iter 54100 / 100000 . Training loss:  2.0779228940442516 .\n",
      "Completed iter 54200 / 100000 . Training loss:  2.0355972593485068 .\n",
      "Completed iter 54300 / 100000 . Training loss:  2.0001126430502665 .\n",
      "Completed iter 54400 / 100000 . Training loss:  1.9245500895681436 .\n",
      "Completed iter 54500 / 100000 . Training loss:  1.7992517377213968 .\n",
      "Completed iter 54600 / 100000 . Training loss:  1.8808104927729576 .\n",
      "Completed iter 54700 / 100000 . Training loss:  1.831664199508079 .\n",
      "Completed iter 54800 / 100000 . Training loss:  2.039811701011889 .\n",
      "Completed iter 54900 / 100000 . Training loss:  2.033624265796574 .\n",
      "Completed iter 55000 / 100000 . Training loss:  1.888201151354045 .\n",
      "Completed iter 55100 / 100000 . Training loss:  1.7993363437131304 .\n",
      "Completed iter 55200 / 100000 . Training loss:  1.8778641141942236 .\n",
      "Completed iter 55300 / 100000 . Training loss:  1.9306875771266976 .\n",
      "Completed iter 55400 / 100000 . Training loss:  1.827908121496855 .\n",
      "Completed iter 55500 / 100000 . Training loss:  1.9517090639179464 .\n",
      "Completed iter 55600 / 100000 . Training loss:  1.9507032376306437 .\n",
      "Completed iter 55700 / 100000 . Training loss:  1.9868009814825736 .\n",
      "Completed iter 55800 / 100000 . Training loss:  1.9985477619526193 .\n",
      "Completed iter 55900 / 100000 . Training loss:  1.8874343253246713 .\n",
      "Completed iter 56000 / 100000 . Training loss:  1.887385650485275 .\n",
      "Completed iter 56100 / 100000 . Training loss:  1.822695266128841 .\n",
      "Completed iter 56200 / 100000 . Training loss:  1.9950065679953677 .\n",
      "Completed iter 56300 / 100000 . Training loss:  1.9107538675083626 .\n",
      "Completed iter 56400 / 100000 . Training loss:  1.9243968771071605 .\n",
      "Completed iter 56500 / 100000 . Training loss:  2.030072625056786 .\n",
      "Completed iter 56600 / 100000 . Training loss:  1.8008436073243845 .\n",
      "Completed iter 56700 / 100000 . Training loss:  1.8958335349754472 .\n",
      "Completed iter 56800 / 100000 . Training loss:  1.8936874542962192 .\n",
      "Completed iter 56900 / 100000 . Training loss:  1.8667438343044813 .\n",
      "Completed iter 57000 / 100000 . Training loss:  1.913391107310294 .\n",
      "Completed iter 57100 / 100000 . Training loss:  1.801522700742398 .\n",
      "Completed iter 57200 / 100000 . Training loss:  1.918797337713383 .\n",
      "Completed iter 57300 / 100000 . Training loss:  1.898174487884371 .\n",
      "Completed iter 57400 / 100000 . Training loss:  1.864330962000694 .\n",
      "Completed iter 57500 / 100000 . Training loss:  1.9953235448431712 .\n",
      "Completed iter 57600 / 100000 . Training loss:  1.8637834278530163 .\n",
      "Completed iter 57700 / 100000 . Training loss:  1.9021258400222099 .\n",
      "Completed iter 57800 / 100000 . Training loss:  1.9644887338915373 .\n",
      "Completed iter 57900 / 100000 . Training loss:  1.8229100054514884 .\n",
      "Completed iter 58000 / 100000 . Training loss:  1.9520365470274026 .\n",
      "Completed iter 58100 / 100000 . Training loss:  1.9632617087168607 .\n",
      "Completed iter 58200 / 100000 . Training loss:  1.8505371247588593 .\n",
      "Completed iter 58300 / 100000 . Training loss:  1.9896220428911726 .\n",
      "Completed iter 58400 / 100000 . Training loss:  2.000364841442515 .\n",
      "Completed iter 58500 / 100000 . Training loss:  1.973493102950934 .\n",
      "Completed iter 58600 / 100000 . Training loss:  1.9283922399681084 .\n",
      "Completed iter 58700 / 100000 . Training loss:  1.9176674657497672 .\n",
      "Completed iter 58800 / 100000 . Training loss:  1.9391497900256873 .\n",
      "Completed iter 58900 / 100000 . Training loss:  1.9211787380407792 .\n",
      "Completed iter 59000 / 100000 . Training loss:  1.921283676106008 .\n",
      "Completed iter 59100 / 100000 . Training loss:  1.8734135286699445 .\n",
      "Completed iter 59200 / 100000 . Training loss:  1.8850313545479533 .\n",
      "Completed iter 59300 / 100000 . Training loss:  1.9365362520342062 .\n",
      "Completed iter 59400 / 100000 . Training loss:  1.9677064107646385 .\n",
      "Completed iter 59500 / 100000 . Training loss:  1.9334440169576452 .\n",
      "Completed iter 59600 / 100000 . Training loss:  1.7382511110737782 .\n",
      "Completed iter 59700 / 100000 . Training loss:  1.9543565268754781 .\n",
      "Completed iter 59800 / 100000 . Training loss:  1.8990511935119083 .\n",
      "Completed iter 59900 / 100000 . Training loss:  1.824646146494054 .\n",
      "Completed iter 60000 / 100000 . Training loss:  2.0141417255856027 .\n",
      "Completed iter 60100 / 100000 . Training loss:  1.829598283972246 .\n",
      "Completed iter 60200 / 100000 . Training loss:  1.8848355976518716 .\n",
      "Completed iter 60300 / 100000 . Training loss:  1.8225107855987994 .\n",
      "Completed iter 60400 / 100000 . Training loss:  1.855454861264584 .\n",
      "Completed iter 60500 / 100000 . Training loss:  1.8633271571827779 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 60600 / 100000 . Training loss:  1.8673071360055657 .\n",
      "Completed iter 60700 / 100000 . Training loss:  1.9154949515005957 .\n",
      "Completed iter 60800 / 100000 . Training loss:  1.9347729653182597 .\n",
      "Completed iter 60900 / 100000 . Training loss:  1.9144984493105104 .\n",
      "Completed iter 61000 / 100000 . Training loss:  1.89061016243638 .\n",
      "Completed iter 61100 / 100000 . Training loss:  2.099612351131682 .\n",
      "Completed iter 61200 / 100000 . Training loss:  1.9326099405748556 .\n",
      "Completed iter 61300 / 100000 . Training loss:  2.011524825702955 .\n",
      "Completed iter 61400 / 100000 . Training loss:  1.961119182796612 .\n",
      "Completed iter 61500 / 100000 . Training loss:  1.8841844652904063 .\n",
      "Completed iter 61600 / 100000 . Training loss:  1.7764597486872609 .\n",
      "Completed iter 61700 / 100000 . Training loss:  1.9619307767309293 .\n",
      "Completed iter 61800 / 100000 . Training loss:  1.7741151729389464 .\n",
      "Completed iter 61900 / 100000 . Training loss:  1.782438006206373 .\n",
      "Completed iter 62000 / 100000 . Training loss:  1.8844203320069237 .\n",
      "Completed iter 62100 / 100000 . Training loss:  1.8701890356432578 .\n",
      "Completed iter 62200 / 100000 . Training loss:  1.9780887572606602 .\n",
      "Completed iter 62300 / 100000 . Training loss:  1.800703624659468 .\n",
      "Completed iter 62400 / 100000 . Training loss:  1.8979906700271847 .\n",
      "Completed iter 62500 / 100000 . Training loss:  1.9769692645081522 .\n",
      "Completed iter 62600 / 100000 . Training loss:  1.9556258759695433 .\n",
      "Completed iter 62700 / 100000 . Training loss:  1.8660863345151948 .\n",
      "Completed iter 62800 / 100000 . Training loss:  2.0257645928498746 .\n",
      "Completed iter 62900 / 100000 . Training loss:  1.985459787334548 .\n",
      "Completed iter 63000 / 100000 . Training loss:  1.9705962533093087 .\n",
      "Completed iter 63100 / 100000 . Training loss:  1.84399019815099 .\n",
      "Completed iter 63200 / 100000 . Training loss:  1.9429034156680474 .\n",
      "Completed iter 63300 / 100000 . Training loss:  1.9158612700685351 .\n",
      "Completed iter 63400 / 100000 . Training loss:  1.9489067500587685 .\n",
      "Completed iter 63500 / 100000 . Training loss:  1.8168658949067666 .\n",
      "Completed iter 63600 / 100000 . Training loss:  1.8868767122768897 .\n",
      "Completed iter 63700 / 100000 . Training loss:  1.9846494156803416 .\n",
      "Completed iter 63800 / 100000 . Training loss:  1.7685977617301303 .\n",
      "Completed iter 63900 / 100000 . Training loss:  1.885285104850337 .\n",
      "Completed iter 64000 / 100000 . Training loss:  1.8961792951876593 .\n",
      "Completed iter 64100 / 100000 . Training loss:  1.8577813217093482 .\n",
      "Completed iter 64200 / 100000 . Training loss:  2.020641108623434 .\n",
      "Completed iter 64300 / 100000 . Training loss:  1.8220612071128175 .\n",
      "Completed iter 64400 / 100000 . Training loss:  1.8550863932926303 .\n",
      "Completed iter 64500 / 100000 . Training loss:  1.9586986418689385 .\n",
      "Completed iter 64600 / 100000 . Training loss:  1.9057988301604212 .\n",
      "Completed iter 64700 / 100000 . Training loss:  1.88973484963484 .\n",
      "Completed iter 64800 / 100000 . Training loss:  1.9174007082303144 .\n",
      "Completed iter 64900 / 100000 . Training loss:  1.902142132799084 .\n",
      "Completed iter 65000 / 100000 . Training loss:  1.9303047947388141 .\n",
      "Completed iter 65100 / 100000 . Training loss:  1.8497158285803572 .\n",
      "Completed iter 65200 / 100000 . Training loss:  1.985719501996225 .\n",
      "Completed iter 65300 / 100000 . Training loss:  1.8265286705606527 .\n",
      "Completed iter 65400 / 100000 . Training loss:  1.9061239249895903 .\n",
      "Completed iter 65500 / 100000 . Training loss:  1.9817627304065657 .\n",
      "Completed iter 65600 / 100000 . Training loss:  1.8818045623880937 .\n",
      "Completed iter 65700 / 100000 . Training loss:  1.881645427962468 .\n",
      "Completed iter 65800 / 100000 . Training loss:  1.8668810933754454 .\n",
      "Completed iter 65900 / 100000 . Training loss:  1.8576062563406155 .\n",
      "Completed iter 66000 / 100000 . Training loss:  1.825584043721685 .\n",
      "Completed iter 66100 / 100000 . Training loss:  1.9087039784040156 .\n",
      "Completed iter 66200 / 100000 . Training loss:  1.922475371544519 .\n",
      "Completed iter 66300 / 100000 . Training loss:  1.841204063275055 .\n",
      "Completed iter 66400 / 100000 . Training loss:  1.9042109231745095 .\n",
      "Completed iter 66500 / 100000 . Training loss:  1.8247805027661688 .\n",
      "Completed iter 66600 / 100000 . Training loss:  1.9200271094158237 .\n",
      "Completed iter 66700 / 100000 . Training loss:  1.9017816359295827 .\n",
      "Completed iter 66800 / 100000 . Training loss:  1.9108260532808259 .\n",
      "Completed iter 66900 / 100000 . Training loss:  1.8677379560143312 .\n",
      "Completed iter 67000 / 100000 . Training loss:  1.8177848190653563 .\n",
      "Completed iter 67100 / 100000 . Training loss:  1.7865966767167958 .\n",
      "Completed iter 67200 / 100000 . Training loss:  1.8295262730356443 .\n",
      "Completed iter 67300 / 100000 . Training loss:  1.8751370100588163 .\n",
      "Completed iter 67400 / 100000 . Training loss:  1.9563139539534329 .\n",
      "Completed iter 67500 / 100000 . Training loss:  1.8441540013952251 .\n",
      "Completed iter 67600 / 100000 . Training loss:  1.8855703236733026 .\n",
      "Completed iter 67700 / 100000 . Training loss:  1.87950341856794 .\n",
      "Completed iter 67800 / 100000 . Training loss:  1.8609224467598502 .\n",
      "Completed iter 67900 / 100000 . Training loss:  1.9224224512413663 .\n",
      "Completed iter 68000 / 100000 . Training loss:  1.8597253102020972 .\n",
      "Completed iter 68100 / 100000 . Training loss:  1.9011683364338552 .\n",
      "Completed iter 68200 / 100000 . Training loss:  1.928028267445828 .\n",
      "Completed iter 68300 / 100000 . Training loss:  1.9519915637594576 .\n",
      "Completed iter 68400 / 100000 . Training loss:  1.9053353632442152 .\n",
      "Completed iter 68500 / 100000 . Training loss:  1.8812645929342011 .\n",
      "Completed iter 68600 / 100000 . Training loss:  1.8847224500296638 .\n",
      "Completed iter 68700 / 100000 . Training loss:  1.769293349640586 .\n",
      "Completed iter 68800 / 100000 . Training loss:  1.9531818695757055 .\n",
      "Completed iter 68900 / 100000 . Training loss:  1.8917253826958662 .\n",
      "Completed iter 69000 / 100000 . Training loss:  1.8709079013866177 .\n",
      "Completed iter 69100 / 100000 . Training loss:  1.8038342391481486 .\n",
      "Completed iter 69200 / 100000 . Training loss:  1.975624431763915 .\n",
      "Completed iter 69300 / 100000 . Training loss:  1.9916503974814053 .\n",
      "Completed iter 69400 / 100000 . Training loss:  1.8741951510018409 .\n",
      "Completed iter 69500 / 100000 . Training loss:  2.037351264629419 .\n",
      "Completed iter 69600 / 100000 . Training loss:  1.8331638268089536 .\n",
      "Completed iter 69700 / 100000 . Training loss:  1.9184420499550392 .\n",
      "Completed iter 69800 / 100000 . Training loss:  1.8200400405356618 .\n",
      "Completed iter 69900 / 100000 . Training loss:  2.0065217016534316 .\n",
      "Completed iter 70000 / 100000 . Training loss:  1.9374666936907226 .\n",
      "Completed iter 70100 / 100000 . Training loss:  1.8851809639531214 .\n",
      "Completed iter 70200 / 100000 . Training loss:  1.9026166814709304 .\n",
      "Completed iter 70300 / 100000 . Training loss:  1.9090604213819549 .\n",
      "Completed iter 70400 / 100000 . Training loss:  1.696948052958895 .\n",
      "Completed iter 70500 / 100000 . Training loss:  1.8735193447434155 .\n",
      "Completed iter 70600 / 100000 . Training loss:  2.058364191076991 .\n",
      "Completed iter 70700 / 100000 . Training loss:  1.847577940368143 .\n",
      "Completed iter 70800 / 100000 . Training loss:  1.964298919760389 .\n",
      "Completed iter 70900 / 100000 . Training loss:  1.9268338726463 .\n",
      "Completed iter 71000 / 100000 . Training loss:  1.8327667006944166 .\n",
      "Completed iter 71100 / 100000 . Training loss:  1.939493297999996 .\n",
      "Completed iter 71200 / 100000 . Training loss:  1.866554016764383 .\n",
      "Completed iter 71300 / 100000 . Training loss:  1.8807239790394332 .\n",
      "Completed iter 71400 / 100000 . Training loss:  1.8928992139242422 .\n",
      "Completed iter 71500 / 100000 . Training loss:  1.9532426442978938 .\n",
      "Completed iter 71600 / 100000 . Training loss:  1.8858228278704634 .\n",
      "Completed iter 71700 / 100000 . Training loss:  1.8871302048817475 .\n",
      "Completed iter 71800 / 100000 . Training loss:  1.8236805053060683 .\n",
      "Completed iter 71900 / 100000 . Training loss:  1.944028572860201 .\n",
      "Completed iter 72000 / 100000 . Training loss:  1.7549389873296048 .\n",
      "Completed iter 72100 / 100000 . Training loss:  2.006062574175581 .\n",
      "Completed iter 72200 / 100000 . Training loss:  1.8216671568869067 .\n",
      "Completed iter 72300 / 100000 . Training loss:  1.8134829481556063 .\n",
      "Completed iter 72400 / 100000 . Training loss:  1.9373351077652037 .\n",
      "Completed iter 72500 / 100000 . Training loss:  1.9180842541353862 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 72600 / 100000 . Training loss:  1.8896934943924186 .\n",
      "Completed iter 72700 / 100000 . Training loss:  1.9765127626264754 .\n",
      "Completed iter 72800 / 100000 . Training loss:  1.894834542128913 .\n",
      "Completed iter 72900 / 100000 . Training loss:  1.8599578132219412 .\n",
      "Completed iter 73000 / 100000 . Training loss:  1.8834033038969857 .\n",
      "Completed iter 73100 / 100000 . Training loss:  1.847598943385513 .\n",
      "Completed iter 73200 / 100000 . Training loss:  1.9063249393595485 .\n",
      "Completed iter 73300 / 100000 . Training loss:  1.9097040980179276 .\n",
      "Completed iter 73400 / 100000 . Training loss:  1.8200888854659043 .\n",
      "Completed iter 73500 / 100000 . Training loss:  1.7906630501388063 .\n",
      "Completed iter 73600 / 100000 . Training loss:  1.8995973061533404 .\n",
      "Completed iter 73700 / 100000 . Training loss:  1.9255910388618525 .\n",
      "Completed iter 73800 / 100000 . Training loss:  1.8578136375815708 .\n",
      "Completed iter 73900 / 100000 . Training loss:  1.856394378026975 .\n",
      "Completed iter 74000 / 100000 . Training loss:  1.9442002519537673 .\n",
      "Completed iter 74100 / 100000 . Training loss:  1.9289336506784371 .\n",
      "Completed iter 74200 / 100000 . Training loss:  1.7716476459275847 .\n",
      "Completed iter 74300 / 100000 . Training loss:  1.980721973857561 .\n",
      "Completed iter 74400 / 100000 . Training loss:  1.7733313308878165 .\n",
      "Completed iter 74500 / 100000 . Training loss:  1.847799646926866 .\n",
      "Completed iter 74600 / 100000 . Training loss:  2.000167823995301 .\n",
      "Completed iter 74700 / 100000 . Training loss:  1.7175572642731507 .\n",
      "Completed iter 74800 / 100000 . Training loss:  1.8840499655282585 .\n",
      "Completed iter 74900 / 100000 . Training loss:  1.8433214005567338 .\n",
      "Completed iter 75000 / 100000 . Training loss:  1.8243589677019125 .\n",
      "Completed iter 75100 / 100000 . Training loss:  1.8008378890297216 .\n",
      "Completed iter 75200 / 100000 . Training loss:  1.9457988607743801 .\n",
      "Completed iter 75300 / 100000 . Training loss:  1.889898743233876 .\n",
      "Completed iter 75400 / 100000 . Training loss:  1.931662182180831 .\n",
      "Completed iter 75500 / 100000 . Training loss:  1.892101591119294 .\n",
      "Completed iter 75600 / 100000 . Training loss:  1.7170964054795903 .\n",
      "Completed iter 75700 / 100000 . Training loss:  1.8680898793746907 .\n",
      "Completed iter 75800 / 100000 . Training loss:  2.0516887707989717 .\n",
      "Completed iter 75900 / 100000 . Training loss:  1.9048515377853936 .\n",
      "Completed iter 76000 / 100000 . Training loss:  1.8805783264723277 .\n",
      "Completed iter 76100 / 100000 . Training loss:  1.8637900821490008 .\n",
      "Completed iter 76200 / 100000 . Training loss:  1.8489043087537171 .\n",
      "Completed iter 76300 / 100000 . Training loss:  1.9244660609895035 .\n",
      "Completed iter 76400 / 100000 . Training loss:  1.9454192148472744 .\n",
      "Completed iter 76500 / 100000 . Training loss:  1.9280427711673787 .\n",
      "Completed iter 76600 / 100000 . Training loss:  1.876196822696629 .\n",
      "Completed iter 76700 / 100000 . Training loss:  1.9164925499595795 .\n",
      "Completed iter 76800 / 100000 . Training loss:  1.8358983290540825 .\n",
      "Completed iter 76900 / 100000 . Training loss:  1.9000753932795236 .\n",
      "Completed iter 77000 / 100000 . Training loss:  1.8214184147432566 .\n",
      "Completed iter 77100 / 100000 . Training loss:  1.9757672708262577 .\n",
      "Completed iter 77200 / 100000 . Training loss:  1.950222181292379 .\n",
      "Completed iter 77300 / 100000 . Training loss:  1.8662746843705444 .\n",
      "Completed iter 77400 / 100000 . Training loss:  1.9154667619113819 .\n",
      "Completed iter 77500 / 100000 . Training loss:  1.915849411539062 .\n",
      "Completed iter 77600 / 100000 . Training loss:  1.8990131330375672 .\n",
      "Completed iter 77700 / 100000 . Training loss:  1.8946822004351498 .\n",
      "Completed iter 77800 / 100000 . Training loss:  1.8906566929664628 .\n",
      "Completed iter 77900 / 100000 . Training loss:  1.9544519968416316 .\n",
      "Completed iter 78000 / 100000 . Training loss:  1.8579147861867005 .\n",
      "Completed iter 78100 / 100000 . Training loss:  1.8424058005614998 .\n",
      "Completed iter 78200 / 100000 . Training loss:  1.8857934164180203 .\n",
      "Completed iter 78300 / 100000 . Training loss:  1.993616720151822 .\n",
      "Completed iter 78400 / 100000 . Training loss:  1.818547165393777 .\n",
      "Completed iter 78500 / 100000 . Training loss:  1.8735142030533707 .\n",
      "Completed iter 78600 / 100000 . Training loss:  1.8860751931918052 .\n",
      "Completed iter 78700 / 100000 . Training loss:  1.9343420374823201 .\n",
      "Completed iter 78800 / 100000 . Training loss:  1.8797560873177601 .\n",
      "Completed iter 78900 / 100000 . Training loss:  1.8905659768088983 .\n",
      "Completed iter 79000 / 100000 . Training loss:  1.8198951682500246 .\n",
      "Completed iter 79100 / 100000 . Training loss:  1.9831132979167863 .\n",
      "Completed iter 79200 / 100000 . Training loss:  1.9090791798106352 .\n",
      "Completed iter 79300 / 100000 . Training loss:  1.8328144533239712 .\n",
      "Completed iter 79400 / 100000 . Training loss:  1.8465453974425667 .\n",
      "Completed iter 79500 / 100000 . Training loss:  1.8423642291911717 .\n",
      "Completed iter 79600 / 100000 . Training loss:  2.0276582136589183 .\n",
      "Completed iter 79700 / 100000 . Training loss:  1.7181930021608216 .\n",
      "Completed iter 79800 / 100000 . Training loss:  2.0247496479557943 .\n",
      "Completed iter 79900 / 100000 . Training loss:  1.8457374119199785 .\n",
      "Completed iter 80000 / 100000 . Training loss:  1.7190864830221502 .\n",
      "Completed iter 80100 / 100000 . Training loss:  1.8025870486118927 .\n",
      "Completed iter 80200 / 100000 . Training loss:  1.9512789397892634 .\n",
      "Completed iter 80300 / 100000 . Training loss:  2.0077842190654955 .\n",
      "Completed iter 80400 / 100000 . Training loss:  1.9031351587555534 .\n",
      "Completed iter 80500 / 100000 . Training loss:  1.8252169343190041 .\n",
      "Completed iter 80600 / 100000 . Training loss:  1.8305682436764161 .\n",
      "Completed iter 80700 / 100000 . Training loss:  1.890182065385046 .\n",
      "Completed iter 80800 / 100000 . Training loss:  1.8592882466326164 .\n",
      "Completed iter 80900 / 100000 . Training loss:  1.8905061710508515 .\n",
      "Completed iter 81000 / 100000 . Training loss:  1.9090687747745119 .\n",
      "Completed iter 81100 / 100000 . Training loss:  1.9028727565071517 .\n",
      "Completed iter 81200 / 100000 . Training loss:  1.905973850993723 .\n",
      "Completed iter 81300 / 100000 . Training loss:  1.907199190427867 .\n",
      "Completed iter 81400 / 100000 . Training loss:  1.9290051300047903 .\n",
      "Completed iter 81500 / 100000 . Training loss:  1.9089089070684928 .\n",
      "Completed iter 81600 / 100000 . Training loss:  1.9197625026550948 .\n",
      "Completed iter 81700 / 100000 . Training loss:  1.8297240736593965 .\n",
      "Completed iter 81800 / 100000 . Training loss:  1.8231327890814168 .\n",
      "Completed iter 81900 / 100000 . Training loss:  1.8551371546873672 .\n",
      "Completed iter 82000 / 100000 . Training loss:  1.923317059097718 .\n",
      "Completed iter 82100 / 100000 . Training loss:  1.8690472228877726 .\n",
      "Completed iter 82200 / 100000 . Training loss:  2.008810269755483 .\n",
      "Completed iter 82300 / 100000 . Training loss:  1.7932352620668484 .\n",
      "Completed iter 82400 / 100000 . Training loss:  1.8739983807009217 .\n",
      "Completed iter 82500 / 100000 . Training loss:  1.8067473503542575 .\n",
      "Completed iter 82600 / 100000 . Training loss:  1.8236736819149975 .\n",
      "Completed iter 82700 / 100000 . Training loss:  1.9727408497134191 .\n",
      "Completed iter 82800 / 100000 . Training loss:  1.8520009614442963 .\n",
      "Completed iter 82900 / 100000 . Training loss:  1.8940403043729046 .\n",
      "Completed iter 83000 / 100000 . Training loss:  1.8985371943423797 .\n",
      "Completed iter 83100 / 100000 . Training loss:  1.8254274562863027 .\n",
      "Completed iter 83200 / 100000 . Training loss:  1.9718533331523436 .\n",
      "Completed iter 83300 / 100000 . Training loss:  1.959339648166101 .\n",
      "Completed iter 83400 / 100000 . Training loss:  1.8488162342106098 .\n",
      "Completed iter 83500 / 100000 . Training loss:  1.886847688226816 .\n",
      "Completed iter 83600 / 100000 . Training loss:  1.7830286579225425 .\n",
      "Completed iter 83700 / 100000 . Training loss:  1.7886782172028324 .\n",
      "Completed iter 83800 / 100000 . Training loss:  1.7923838719821155 .\n",
      "Completed iter 83900 / 100000 . Training loss:  1.9683322700155235 .\n",
      "Completed iter 84000 / 100000 . Training loss:  1.9176961461661526 .\n",
      "Completed iter 84100 / 100000 . Training loss:  1.9112401521490165 .\n",
      "Completed iter 84200 / 100000 . Training loss:  1.7897441735205295 .\n",
      "Completed iter 84300 / 100000 . Training loss:  1.9560381036797194 .\n",
      "Completed iter 84400 / 100000 . Training loss:  1.9383553481552096 .\n",
      "Completed iter 84500 / 100000 . Training loss:  1.8045765661047621 .\n",
      "Completed iter 84600 / 100000 . Training loss:  1.8435110116650528 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 84700 / 100000 . Training loss:  1.8358344978486683 .\n",
      "Completed iter 84800 / 100000 . Training loss:  1.866983519244568 .\n",
      "Completed iter 84900 / 100000 . Training loss:  1.9272761827911258 .\n",
      "Completed iter 85000 / 100000 . Training loss:  1.8872321289755543 .\n",
      "Completed iter 85100 / 100000 . Training loss:  1.7841189065629608 .\n",
      "Completed iter 85200 / 100000 . Training loss:  1.7635211739138443 .\n",
      "Completed iter 85300 / 100000 . Training loss:  1.8337587755344598 .\n",
      "Completed iter 85400 / 100000 . Training loss:  1.8877503319894144 .\n",
      "Completed iter 85500 / 100000 . Training loss:  1.9383032538781297 .\n",
      "Completed iter 85600 / 100000 . Training loss:  2.088033180458727 .\n",
      "Completed iter 85700 / 100000 . Training loss:  1.8409840450124328 .\n",
      "Completed iter 85800 / 100000 . Training loss:  1.8567274665874525 .\n",
      "Completed iter 85900 / 100000 . Training loss:  1.7530138916965485 .\n",
      "Completed iter 86000 / 100000 . Training loss:  1.8544018251073933 .\n",
      "Completed iter 86100 / 100000 . Training loss:  1.9461137178820138 .\n",
      "Completed iter 86200 / 100000 . Training loss:  2.1054031519490835 .\n",
      "Completed iter 86300 / 100000 . Training loss:  1.8151271212798 .\n",
      "Completed iter 86400 / 100000 . Training loss:  1.9335096810054075 .\n",
      "Completed iter 86500 / 100000 . Training loss:  1.9594084999244448 .\n",
      "Completed iter 86600 / 100000 . Training loss:  1.9015266647396776 .\n",
      "Completed iter 86700 / 100000 . Training loss:  1.9129783679984673 .\n",
      "Completed iter 86800 / 100000 . Training loss:  1.8297454811084095 .\n",
      "Completed iter 86900 / 100000 . Training loss:  1.907453542965184 .\n",
      "Completed iter 87000 / 100000 . Training loss:  1.8454849815223329 .\n",
      "Completed iter 87100 / 100000 . Training loss:  1.9492982416564142 .\n",
      "Completed iter 87200 / 100000 . Training loss:  1.97580530210001 .\n",
      "Completed iter 87300 / 100000 . Training loss:  2.0397360427677147 .\n",
      "Completed iter 87400 / 100000 . Training loss:  1.941462614859504 .\n",
      "Completed iter 87500 / 100000 . Training loss:  1.9256915902733756 .\n",
      "Completed iter 87600 / 100000 . Training loss:  1.8662020160736648 .\n",
      "Completed iter 87700 / 100000 . Training loss:  1.920203306955269 .\n",
      "Completed iter 87800 / 100000 . Training loss:  1.8616072210555974 .\n",
      "Completed iter 87900 / 100000 . Training loss:  1.8364098212445088 .\n",
      "Completed iter 88000 / 100000 . Training loss:  1.8637557459817882 .\n",
      "Completed iter 88100 / 100000 . Training loss:  1.9861729110425013 .\n",
      "Completed iter 88200 / 100000 . Training loss:  1.898310609435086 .\n",
      "Completed iter 88300 / 100000 . Training loss:  1.9869699271513372 .\n",
      "Completed iter 88400 / 100000 . Training loss:  1.896589709301622 .\n",
      "Completed iter 88500 / 100000 . Training loss:  1.7969144706776712 .\n",
      "Completed iter 88600 / 100000 . Training loss:  1.8399961814894328 .\n",
      "Completed iter 88700 / 100000 . Training loss:  1.8874267185970928 .\n",
      "Completed iter 88800 / 100000 . Training loss:  1.8403460582961184 .\n",
      "Completed iter 88900 / 100000 . Training loss:  1.9423267581180923 .\n",
      "Completed iter 89000 / 100000 . Training loss:  1.9892072035256576 .\n",
      "Completed iter 89100 / 100000 . Training loss:  1.9651207768290995 .\n",
      "Completed iter 89200 / 100000 . Training loss:  1.9160583670567195 .\n",
      "Completed iter 89300 / 100000 . Training loss:  1.7773421018028284 .\n",
      "Completed iter 89400 / 100000 . Training loss:  1.9326039737543521 .\n",
      "Completed iter 89500 / 100000 . Training loss:  1.905060848451238 .\n",
      "Completed iter 89600 / 100000 . Training loss:  1.9450064528461246 .\n",
      "Completed iter 89700 / 100000 . Training loss:  1.8856832074645933 .\n",
      "Completed iter 89800 / 100000 . Training loss:  1.7488956498217674 .\n",
      "Completed iter 89900 / 100000 . Training loss:  2.0201383443224525 .\n",
      "Completed iter 90000 / 100000 . Training loss:  1.9423154356953933 .\n",
      "Completed iter 90100 / 100000 . Training loss:  1.8742731600917386 .\n",
      "Completed iter 90200 / 100000 . Training loss:  1.9453875454411507 .\n",
      "Completed iter 90300 / 100000 . Training loss:  1.8873504003677106 .\n",
      "Completed iter 90400 / 100000 . Training loss:  1.9000128320351382 .\n",
      "Completed iter 90500 / 100000 . Training loss:  1.8046047836204797 .\n",
      "Completed iter 90600 / 100000 . Training loss:  1.9575758304233262 .\n",
      "Completed iter 90700 / 100000 . Training loss:  1.8620030715434481 .\n",
      "Completed iter 90800 / 100000 . Training loss:  1.9701064823847028 .\n",
      "Completed iter 90900 / 100000 . Training loss:  1.880287975049143 .\n",
      "Completed iter 91000 / 100000 . Training loss:  1.880631441559807 .\n",
      "Completed iter 91100 / 100000 . Training loss:  1.7355530513714263 .\n",
      "Completed iter 91200 / 100000 . Training loss:  1.83416038934956 .\n",
      "Completed iter 91300 / 100000 . Training loss:  1.9703853427234175 .\n",
      "Completed iter 91400 / 100000 . Training loss:  1.9349540194206278 .\n",
      "Completed iter 91500 / 100000 . Training loss:  1.86659206419825 .\n",
      "Completed iter 91600 / 100000 . Training loss:  1.8858345589201262 .\n",
      "Completed iter 91700 / 100000 . Training loss:  1.9947552239234396 .\n",
      "Completed iter 91800 / 100000 . Training loss:  1.8478590195216393 .\n",
      "Completed iter 91900 / 100000 . Training loss:  2.0381088296183836 .\n",
      "Completed iter 92000 / 100000 . Training loss:  1.8944686441340077 .\n",
      "Completed iter 92100 / 100000 . Training loss:  1.8419863308988171 .\n",
      "Completed iter 92200 / 100000 . Training loss:  1.9165526296049373 .\n",
      "Completed iter 92300 / 100000 . Training loss:  1.9329672867549434 .\n",
      "Completed iter 92400 / 100000 . Training loss:  1.8627756903910755 .\n",
      "Completed iter 92500 / 100000 . Training loss:  1.9465042213040615 .\n",
      "Completed iter 92600 / 100000 . Training loss:  1.977014909655298 .\n",
      "Completed iter 92700 / 100000 . Training loss:  1.9388884311988899 .\n",
      "Completed iter 92800 / 100000 . Training loss:  1.9221220803367953 .\n",
      "Completed iter 92900 / 100000 . Training loss:  1.9726959196835812 .\n",
      "Completed iter 93000 / 100000 . Training loss:  1.8831372595020177 .\n",
      "Completed iter 93100 / 100000 . Training loss:  1.8365780992140908 .\n",
      "Completed iter 93200 / 100000 . Training loss:  1.8702092091255236 .\n",
      "Completed iter 93300 / 100000 . Training loss:  1.9177399633863332 .\n",
      "Completed iter 93400 / 100000 . Training loss:  1.9649857964072412 .\n",
      "Completed iter 93500 / 100000 . Training loss:  1.7874822315444292 .\n",
      "Completed iter 93600 / 100000 . Training loss:  1.9231865609221923 .\n",
      "Completed iter 93700 / 100000 . Training loss:  1.8779854469032835 .\n",
      "Completed iter 93800 / 100000 . Training loss:  2.003455155815254 .\n",
      "Completed iter 93900 / 100000 . Training loss:  1.8757677170283518 .\n",
      "Completed iter 94000 / 100000 . Training loss:  1.8487591996743566 .\n",
      "Completed iter 94100 / 100000 . Training loss:  1.8620994887393523 .\n",
      "Completed iter 94200 / 100000 . Training loss:  1.9726925087299312 .\n",
      "Completed iter 94300 / 100000 . Training loss:  1.8183429595091565 .\n",
      "Completed iter 94400 / 100000 . Training loss:  1.799807721919483 .\n",
      "Completed iter 94500 / 100000 . Training loss:  1.7927032098415427 .\n",
      "Completed iter 94600 / 100000 . Training loss:  1.9130637952779375 .\n",
      "Completed iter 94700 / 100000 . Training loss:  1.940809667368052 .\n",
      "Completed iter 94800 / 100000 . Training loss:  1.8464595931995302 .\n",
      "Completed iter 94900 / 100000 . Training loss:  2.022951657041622 .\n",
      "Completed iter 95000 / 100000 . Training loss:  1.8572973297824973 .\n",
      "Completed iter 95100 / 100000 . Training loss:  2.028450856225851 .\n",
      "Completed iter 95200 / 100000 . Training loss:  1.772002866743531 .\n",
      "Completed iter 95300 / 100000 . Training loss:  1.7688331115724107 .\n",
      "Completed iter 95400 / 100000 . Training loss:  1.9639585474356813 .\n",
      "Completed iter 95500 / 100000 . Training loss:  1.8861776582248062 .\n",
      "Completed iter 95600 / 100000 . Training loss:  1.9389146980292815 .\n",
      "Completed iter 95700 / 100000 . Training loss:  1.8080439140415556 .\n",
      "Completed iter 95800 / 100000 . Training loss:  1.843697429414746 .\n",
      "Completed iter 95900 / 100000 . Training loss:  1.8290588061153905 .\n",
      "Completed iter 96000 / 100000 . Training loss:  1.8979020910923312 .\n",
      "Completed iter 96100 / 100000 . Training loss:  1.863565134125354 .\n",
      "Completed iter 96200 / 100000 . Training loss:  2.012702462590155 .\n",
      "Completed iter 96300 / 100000 . Training loss:  1.8152365379726292 .\n",
      "Completed iter 96400 / 100000 . Training loss:  1.8720245213477993 .\n",
      "Completed iter 96500 / 100000 . Training loss:  1.8348980519374831 .\n",
      "Completed iter 96600 / 100000 . Training loss:  1.9742516750464305 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iter 96700 / 100000 . Training loss:  1.7821290885783307 .\n",
      "Completed iter 96800 / 100000 . Training loss:  1.8952916496400378 .\n",
      "Completed iter 96900 / 100000 . Training loss:  1.907343717465465 .\n",
      "Completed iter 97000 / 100000 . Training loss:  1.7559915682415095 .\n",
      "Completed iter 97100 / 100000 . Training loss:  1.9255240172456405 .\n",
      "Completed iter 97200 / 100000 . Training loss:  1.9062823125603081 .\n",
      "Completed iter 97300 / 100000 . Training loss:  1.9566386421282471 .\n",
      "Completed iter 97400 / 100000 . Training loss:  1.8864252414782068 .\n",
      "Completed iter 97500 / 100000 . Training loss:  1.8345881039741017 .\n",
      "Completed iter 97600 / 100000 . Training loss:  1.8486836935792441 .\n",
      "Completed iter 97700 / 100000 . Training loss:  1.89782151758764 .\n",
      "Completed iter 97800 / 100000 . Training loss:  1.8676572035244288 .\n",
      "Completed iter 97900 / 100000 . Training loss:  1.8759555981233516 .\n",
      "Completed iter 98000 / 100000 . Training loss:  2.0191499934911565 .\n",
      "Completed iter 98100 / 100000 . Training loss:  1.882350409780943 .\n",
      "Completed iter 98200 / 100000 . Training loss:  1.9644518767329748 .\n",
      "Completed iter 98300 / 100000 . Training loss:  1.8643710376699947 .\n",
      "Completed iter 98400 / 100000 . Training loss:  1.8528075735702574 .\n",
      "Completed iter 98500 / 100000 . Training loss:  2.0041923197977707 .\n",
      "Completed iter 98600 / 100000 . Training loss:  1.8178791497564308 .\n",
      "Completed iter 98700 / 100000 . Training loss:  1.8770251652460856 .\n",
      "Completed iter 98800 / 100000 . Training loss:  1.9889670706711156 .\n",
      "Completed iter 98900 / 100000 . Training loss:  1.9171537637065645 .\n",
      "Completed iter 99000 / 100000 . Training loss:  1.6956933319137621 .\n",
      "Completed iter 99100 / 100000 . Training loss:  1.834535608121408 .\n",
      "Completed iter 99200 / 100000 . Training loss:  1.911917512022802 .\n",
      "Completed iter 99300 / 100000 . Training loss:  1.812871045567253 .\n",
      "Completed iter 99400 / 100000 . Training loss:  1.9041313538212365 .\n",
      "Completed iter 99500 / 100000 . Training loss:  1.925718020788836 .\n",
      "Completed iter 99600 / 100000 . Training loss:  1.8115078615811968 .\n",
      "Completed iter 99700 / 100000 . Training loss:  1.82736143284409 .\n",
      "Completed iter 99800 / 100000 . Training loss:  1.8392572005620484 .\n",
      "Completed iter 99900 / 100000 . Training loss:  1.958534025077304 .\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "bestNet = SoftmaxLayer(10)\n",
    "loss_history = bestNet.fit(x_test, y_test, n_epochs=10000, lr=0.00011, mini_batch_sz=50, reg=0.5)\n",
    "y_predict = bestNet.predict(x_test)\n",
    "accuracy = bestNet.accuracy(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g. Visualize learned weights\n",
    "\n",
    "Run the following code that plots the network weights going to each output neuron. If all goes well, you should see something really cool! Include the plot in your submitted project to show me what you got!\n",
    "\n",
    "**Note:** the quality of your visualizations will depend on:\n",
    "- The quality of the hyperparameters that you got via grid search.\n",
    "- How many epochs that you trained the network before plotting the weights\n",
    "\n",
    "One extension idea: is to find the combination of the above that result in the best visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the class names\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)\n",
    "\n",
    "# We don't care about the bias wt\n",
    "wts = bestNet.wts\n",
    "# Reshape the wt vectors into spatial 'image' configurations to visualization\n",
    "wts = wts.reshape(32, 32, 3, 10)\n",
    "\n",
    "# Make a large new empty figure/plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Loop through each output neuron\n",
    "for i in range(10):\n",
    "  # Make a 2x5 grid of images\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  currImg = 255.0 * (wts[:, :, :, i].squeeze() - np.min(wts)) / (np.max(wts) - np.min(wts))\n",
    "  \n",
    "  plt.imshow(currImg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
